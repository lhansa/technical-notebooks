{
  "hash": "b37d870570fe225a04acaa831ba7a13b",
  "result": {
    "markdown": "---\ntitle: \"Cómo procesas datos que no te caben en RAM\"\ndescription: \"Procesamiento de datos con más gigas que memoria RAM\"\ndescription-meta: \"Procesamiento de datos con más gigas que memoria RAM\"\nauthor: \"Leonardo Hansa\"\ndate: \"2024-04-21\"\ncategories: [datos]\nexecute: \n  echo: true\n  message: false\n  warning: false\nfreeze: true\n---\n\n\nTengo unos 16GB de datos en ficheros parquet en una carpeta. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\npath_to_folder <- \"../../data/taxiny/\"\n\n# https://stackoverflow.com/a/68145683/7569570\ndir_size <- function(path, recursive = TRUE) {\n  stopifnot(is.character(path))\n  files <- list.files(path, full.names = T, recursive = recursive)\n  vect_size <- sapply(files, function(x) file.size(x))\n  size_files <- sum(vect_size)\n  size_files\n}\n\ncat(dir_size(path_to_folder)/10**9, \"GB\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n16.74646 GB\n```\n:::\n:::\n\n\nCada fichero tiene datos de [trayectos de taxi](# https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) en la ciudad de Nueva York. \n\nQuiero calcular cuántos trayectos con tarifa de aeropuerto ha habido en cada día, desde enero de 2021 hasta febrero de 2024. Es una operación con una estructura sencilla: si te cupiera en un Excel lo podrías plantear en una tabla dinámica sin muchas especificaciones. \n\nO con SQL como lingua franca, si tuviera todos los datos en una tabla, sería algo así: \n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT day, count(*)\nFROM Viajes\nWHERE airport_fee > 0\nGROUP BY day\n```\n:::\n\n\nPero no los tengo todos en una tabla, sino en 38 ficheros. En cualquier programa orientado a dato es fácil juntar varias en tablas en una, con algún _concat_ o variantes. **Pero tengo 16GB de datos para un portátil de 8GB de RAM:** no voy a poder.\n\nAsí que necesito calcular eso sin cargar todo en memoria. ¿Cómo procedo?\n\n## Versión directa\n\nSi lo que conoces es R (aplica también a pandas y Python), podrías plantear un bucle que itere sobre cada fichero. \n\nLos ficheros están en parquet, así que usaré la librería **arrow** para cargar sus datos en un data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(arrow)\n```\n:::\n\n\nLa operación con un solo fichero sería algo así:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read_parquet(\n  \"../../data/taxiny/fhvhv_tripdata_2023-01.parquet\", \n  col_select = c(\"pickup_datetime\", \"airport_fee\"),\n  as_data_frame = TRUE\n)\n\ndf |> \n  filter(airport_fee > 0) |> \n  mutate(day = as.Date(pickup_datetime)) |>\n  summarise(count = n(), .by = day) |> \n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   day        count\n   <date>     <int>\n 1 2023-01-01 41384\n 2 2023-01-02 50682\n 3 2023-01-03 50072\n 4 2023-01-04 43015\n 5 2023-01-05 42661\n 6 2023-01-06 41856\n 7 2023-01-07 36971\n 8 2023-01-08 50466\n 9 2023-01-09 46022\n10 2023-01-10 39091\n```\n:::\n:::\n\n\n\n\nLuego vemos lo de `as_data_frame = TRUE`. Por ahora metemos eso en un bucle.\n\n## Método 1. Bucle\n\nLo más intuitivo para mí siempre ha sido un bucle. Que lo plantees directamente con un `for()`, con un `lapply()` más `rbind` y `do.call()` o con otro conjunto de herramientas es cosa tuya. \n\nMe resulta directo `map_dfr()` de **purrr** porque lo que necesito al final es un data frame de todos los data frames intermedios apilados por filas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaths_a_ficheros <- list.files(path_to_folder,\n                               pattern = \"parquet$\", \n                               full.names = TRUE)\n\nsystem.time(\n  df_all <- purrr::map_dfr(paths_a_ficheros, function(.path) {\n    df <- read_parquet(\n      .path,\n      col_select = c(\"pickup_datetime\", \"airport_fee\"),\n      as_data_frame = TRUE\n    )\n    \n    df <- df |> \n      filter(airport_fee > 0) |> \n      mutate(day = as.Date(pickup_datetime)) |>\n      summarise(count = n(), .by = day)\n    \n    # gc()\n    \n    return(df)\n  }) \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 92.233  41.167 148.677 \n```\n:::\n:::\n\n\nHa tardado como un minuto, pero lo tengo. \n\n::: {.cell}\n\n```{.r .cell-code}\nhead(df_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  day        count\n  <date>     <int>\n1 2021-04-05  6548\n2 2021-04-06  5664\n3 2021-04-07  5343\n4 2021-04-08  6054\n5 2021-04-09  6069\n6 2021-04-10  5157\n```\n:::\n:::\n\n\nNo he cargado en ningún momento todos los datos: he cargado solo lo que necesitaba\nde cada fichero y he operado con cada fichero por separado. \n\n- `col_select` te permite no leer todas las columnas sino solo las que incluyas en la selección. Los ficheros son grandes, así que ahorras tiempo y memoria si eliges previamente.\n- `as_data_frame = TRUE` está diciendo a la función que cargue todo el data frame en memoria (o por lo menos la selección que hemos hecho). Si lo marcas como `FALSE` el resultado de la lectura será una tabla de arrow, y el cómputo que viene después no lo hará dplyr sino arrow, aunque luego tendrás que recuperar los datos con un `collect()`. \n\n### Alternativa en el bucle. Sin selección de columnas\n\nSiempre que sepas que te sobran columnas, elimínalas. Para que te hagas una idea, el siguiente código solo se diferencia del anterior en la selección de columnas... pues este no me tira. Se peta todo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n  df_all <- purrr::map_dfr(paths_a_ficheros, function(.path) {\n    df <- read_parquet(\n      .path,\n      as_data_frame = TRUE\n    )\n    \n    df <- df |> \n      filter(airport_fee > 0) |> \n      mutate(day = as.Date(pickup_datetime)) |>\n      summarise(count = n(), .by = day)\n    \n    # gc()\n    \n    return(df)\n  }) \n)\n```\n:::\n\n\n\n### Alternativa en el bucle. Tabla arrow\n\nUna opción es fijar `as_data_frame` a `FALSE` para trabajar con las tablas de arrow en lugar de data frame. Esto te será útil si cada fichero es a su vez muy grande. Pero tendrás de todos modos que usar `collect()` en cada iteración para que `map_dfr` pueda apilar los data frames. Esto hará que cada data frame (resultante de cada iteración) se vaya volcando en memoria.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n  df_all <- purrr::map_dfr(paths_a_ficheros, function(.path) {\n    df <- read_parquet(\n      .path,\n      col_select = c(\"pickup_datetime\", \"airport_fee\"),\n      as_data_frame = FALSE\n    )\n    \n    df <- df |> \n      filter(airport_fee > 0) |> \n      mutate(day = as.Date(pickup_datetime)) |>\n      summarise(count = n(), .by = day)\n    \n    # gc()\n    \n    return(collect(df))\n  }) \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 70.561  16.871  82.183 \n```\n:::\n:::\n\n\n## Método 2. `arrow`\n\nEn lugar del bucle, si te vas a meter en **arrow**, le sacarás más partido si aprovechas todas sus herramientas. \n\nSi tienes unos ficheros parquet en una carpeta, en lugar de leerlos uno a uno desde una mentalidad R, puedes dejar que `arrow` se encargue de la gestión en memoria con particiones de todos los ficheros en conjunto. \n\n> Hay formas más recomendables de guardar los ficheros parquet para facilitar esta tarea a arrow, pero los dejo como están para este ejemplo porque me sirven así.\n\nCon `arrow::open_dataset()` cargo la información de los ficheros. Por ejemplo, debajo puedes ver los nombres y tipos de columnas de los ficheros:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(path_to_folder)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 38 Parquet files\nhvfhs_license_num: string\ndispatching_base_num: string\noriginating_base_num: string\nrequest_datetime: timestamp[us]\non_scene_datetime: timestamp[us]\npickup_datetime: timestamp[us]\ndropoff_datetime: timestamp[us]\nPULocationID: int64\nDOLocationID: int64\ntrip_miles: double\ntrip_time: int64\nbase_passenger_fare: double\ntolls: double\nbcf: double\nsales_tax: double\ncongestion_surcharge: double\nairport_fee: double\ntips: double\ndriver_pay: double\nshared_request_flag: string\nshared_match_flag: string\naccess_a_ride_flag: string\nwav_request_flag: string\nwav_match_flag: string\n\nSee $metadata for additional Schema metadata\n```\n:::\n:::\n\n\nY puedes trabajar ese resultado como si fuera un data frame de dplyr. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(path_to_folder) |> \n  select(airport_fee, pickup_datetime) |> \n  filter(airport_fee > 0) |> \n  mutate(day = as.Date(pickup_datetime)) |>\n  summarise(count = n(), .by = day)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset (query)\nday: date32[day]\ncount: int64\n\nSee $.data for the source Arrow object\n```\n:::\n:::\n\n\nCon ese código no has ejecutado ninguna operación, por eso no tarda nada en ejercutarse. Lo que has hecho es preparar una operación con una sintaxis. Ahora falta materializarla, decirle a R que mande la orden y arrow la ejecutará. Para eso, necesitas `collect()`, pero la sintaxis de dplyr no cambia.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n  df_all <- open_dataset(path_to_folder) |> \n    select(airport_fee, pickup_datetime) |> \n    filter(airport_fee > 0) |> \n    mutate(day = as.Date(pickup_datetime)) |>\n    summarise(count = n(), .by = day) |> \n    collect()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 64.199  13.922  38.288 \n```\n:::\n:::\n\n\narrow se ha encargado de paralelizar la operación aprovechando los recursos y por eso tarda tan poco. \n\nEn tu caso no sé, pero si hubiera intentando yo pararelizar la operación con ``mcLapply` o cosas así seguramente habría petado el ordenador... porque habría intentado cargar en memoria más de lo que el ordenador puede.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(df_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  day        count\n  <date>     <int>\n1 2021-06-01 26302\n2 2021-06-02 23917\n3 2021-06-03 26256\n4 2021-06-04 27644\n5 2021-06-05 22020\n6 2021-06-06 28681\n```\n:::\n:::\n\n\n## Método 3. duckdb\n\nOtra herramienta en la que te puedes apoyar sin que tu sintaxis tidyverse se vea afectada en **DuckDB**, en colaboración con **dbplyr**, el backend de dplyr para bases de datos. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckdb)\nlibrary(dplyr)\n# hive_partitioning = true\ncon <- dbConnect(duckdb())\nsystem.time(\n  df_all <- tbl(con, \"read_parquet('../../data/taxiny/*.parquet')\") |> \n    select(airport_fee, pickup_datetime) |> \n    filter(airport_fee > 0) |> \n    mutate(day = as.Date(pickup_datetime)) |>\n    summarise(count = n(), .by = day) |> \n    collect()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 57.329   1.487  20.903 \n```\n:::\n:::\n\n\nSe ha portado bien. En benchmarks más serios, con datos medianos funciona muy bien.\n\nDuckDB además podrías trabajarla con SQL, lo que puede facilitar el trabajo en equipos grandes, en los que pocos miembros sabrán dplyr.\n\n## Comentarios finales\n\n- Si gestionas bien las particiones de ficheros por variables, obtendrás incluso mejores resultados. \n- Recientemente ha salido [duckplyr](https://github.com/duckdblabs/duckplyr), para que el propio dplyr te contruya la infraestructura duckdb directamente. Pero según entiendo esto lo hace con data frames que ya tienes cargados en memoria. Recuerda que en este caso no tienes suficiente memoria.\n- Si te cabe en memoria, el clásico `data.table` de R sigue estando vigente, aunque muchos ojos se estén yendo a _polars_ ([benchmark](https://h2oai.github.io/db-benchmark/))\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}