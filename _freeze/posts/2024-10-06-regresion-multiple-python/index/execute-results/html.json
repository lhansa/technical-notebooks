{
  "hash": "0151c0885d49a9de225e6a6fa08eaa66",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Ejemplo de regresión múltiple con Python\"\ndescription: \"Pequeña práctica de regresión múltiple con Python, siguiendo ISLP\"\ndescription-meta: \"Pequeña práctica de regresión múltiple con Python, siguiendo ISLP\"\nauthor: \"Leonardo Hansa\"\ndate: \"2024-10-06\"\ncategories: [exploraciones]\nexecute: \n  echo: true\n  message: false\n  warning: false\nfreeze: true\n---\n\n\nAquí sigo un ejercicio de regresión lineal con Python del libro _Introduction to Statistical Learning with Python._\n\n## Regresión simple\n\n### Código del ajuste del modelo\n\n::: {#libs .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\n\nimport statsmodels.api as sm\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS, summarize, poly)\n```\n:::\n\n\n::: {#cell-load-data .cell execution_count=2}\n``` {.python .cell-code}\nboston = load_data('Boston')\nboston.head()\n```\n\n::: {#load-data .cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>crim</th>\n      <th>zn</th>\n      <th>indus</th>\n      <th>chas</th>\n      <th>nox</th>\n      <th>rm</th>\n      <th>age</th>\n      <th>dis</th>\n      <th>rad</th>\n      <th>tax</th>\n      <th>ptratio</th>\n      <th>lstat</th>\n      <th>medv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296</td>\n      <td>15.3</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEstos datos son un clásico, que incluso han tenido ya _haters_ por no sé qué variable que hace al conjunto racista. Pero yo voy a seguir con el ejemplo de ISLP porque no me quiero complicar ahora. \n\nEl libro ajusta una regresión lineal de `'medv'` frente a `'lstat'`. Por supuesto, ni idea de qué es cada una. Pero solo quier practicar código Python; el resultado nos da igual. \n\n::: {#cell-datos .cell execution_count=3}\n``` {.python .cell-code}\nX = pd.DataFrame({'intercept': np.ones(boston.shape[0]),\n                  'lstat': boston['lstat']})\n\nX.head()                  \n```\n\n::: {#datos .cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>intercept</th>\n      <th>lstat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>4.98</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>9.14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>4.03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>2.94</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>5.33</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEn **statsmodels** el _intercept_ no se incluye por defecto y es el usuario quien tiene que incluirlo.\n\n::: {#cell-ols1 .cell execution_count=4}\n``` {.python .cell-code}\ny = boston['medv']\nmodel = sm.OLS(y, X)\nresults = model.fit()\nresults.summary()\n```\n\n::: {#ols1 .cell-output .cell-output-display execution_count=4}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.544</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.543</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   601.6</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 14 Oct 2024</td> <th>  Prob (F-statistic):</th> <td>5.08e-88</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>11:25:22</td>     <th>  Log-Likelihood:    </th> <td> -1641.5</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3287.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   504</td>      <th>  BIC:               </th> <td>   3295.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>intercept</th> <td>   34.5538</td> <td>    0.563</td> <td>   61.415</td> <td> 0.000</td> <td>   33.448</td> <td>   35.659</td>\n</tr>\n<tr>\n  <th>lstat</th>     <td>   -0.9500</td> <td>    0.039</td> <td>  -24.528</td> <td> 0.000</td> <td>   -1.026</td> <td>   -0.874</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>137.043</td> <th>  Durbin-Watson:     </th> <td>   0.892</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 291.373</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 1.453</td>  <th>  Prob(JB):          </th> <td>5.36e-64</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.319</td>  <th>  Cond. No.          </th> <td>    29.7</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nUna forma más generalizable de preparar los datos es con la función `MS()`, que funciona al estilo `sklearn`. Pero es algo propio del libro así que no sé si le haré mucho caso.\n\n::: {#cell-fit-tranform .cell execution_count=5}\n``` {.python .cell-code}\ndesign = MS(['lstat'])\ndesign = design.fit(boston)\nX = design.transform(boston)\nX.head()\n```\n\n::: {#fit-tranform .cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>intercept</th>\n      <th>lstat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>4.98</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>9.14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>4.03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>2.94</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>5.33</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nTambién se puede acortar con `fit_transform()`.\n\nA partir del modelo, guardado en `results`, y un conjunto de datos, puedes obtener predicciones.\n\n::: {#21a25317 .cell execution_count=6}\n``` {.python .cell-code}\nnew_df = pd.DataFrame({'lstat': [5, 10, 15]})\nnewX = design.transform(new_df)\nnewX\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>intercept</th>\n      <th>lstat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nY ahora predices:\n\n::: {#cell-predicciones .cell execution_count=7}\n``` {.python .cell-code}\nnew_predictions = results.get_prediction(newX)\nnew_predictions.conf_int(alpha=0.95)\n```\n\n::: {#predicciones .cell-output .cell-output-display execution_count=7}\n```\narray([[29.7781697 , 29.82901852],\n       [25.03485131, 25.07184337],\n       [20.28485052, 20.32135063]])\n```\n:::\n:::\n\n\n### Gráfico\n\nEl primer gráfico muestra la regresión calculada.\n\n::: {#cell-plot-regression .cell execution_count=8}\n``` {.python .cell-code}\ndef abline(ax, b, m, *args, **kwargs):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim, *args, **kwargs)\n\nax = boston.plot.scatter('lstat', 'medv')\nabline(ax, \n       results.params[0],\n       results.params[1], \n       'r--', \n       linewidth=3)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/plot-regression-output-1.png){#plot-regression width=585 height=429}\n:::\n:::\n\n\nEl segundo gráfico compara los residuos frente a los valores ajustados. \n\nParece que los valores ajustados más pequeños están asociados a residuos negativos y altos.\nEn valores más altos de ajuste, hay residuos tanto positivos como negativos, pero se concentran muchos más en casos negativos.\n\n::: {#cell-residuals .cell execution_count=9}\n``` {.python .cell-code}\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls= '--')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/residuals-output-1.png){#residuals width=670 height=651}\n:::\n:::\n\n\nEl _leverage_ es una métrica que indica cuánto se aleja un punto del centroide de toda la nube de puntos. Puntos muy alejados tendrán una mayor influencia en el cálculo del coeficiente que puntos cercanos. \n\nEl gráfico muestra el _leverage_ contra el índice del punto, lo que ayuda a identificar puntos más influyentes (en el caso de una serie temporal, se podría ver como una evolución).\n\n::: {#plot-levarage .cell execution_count=10}\n``` {.python .cell-code}\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag)\n```\n\n::: {#plot-levarage-1 .cell-output .cell-output-display execution_count=10}\n```\n374\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/plot-levarage-output-2.png){#plot-levarage-2 width=680 height=651}\n:::\n:::\n\n\n## Regresión múltiple\n\nAhora enfrentan `'medv'` a `'lstat'` y `'age'`.\n\n::: {#cell-multiple .cell execution_count=11}\n``` {.python .cell-code}\nX = MS(['lstat', 'age']).fit_transform(boston)\nmodel1 = sm.OLS(y, X)\nresults1 = model1.fit()\nresults1.summary()\n```\n\n::: {#multiple .cell-output .cell-output-display execution_count=11}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.551</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.549</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   309.0</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 14 Oct 2024</td> <th>  Prob (F-statistic):</th> <td>2.98e-88</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>11:25:23</td>     <th>  Log-Likelihood:    </th> <td> -1637.5</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3281.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   503</td>      <th>  BIC:               </th> <td>   3294.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>intercept</th> <td>   33.2228</td> <td>    0.731</td> <td>   45.458</td> <td> 0.000</td> <td>   31.787</td> <td>   34.659</td>\n</tr>\n<tr>\n  <th>lstat</th>     <td>   -1.0321</td> <td>    0.048</td> <td>  -21.416</td> <td> 0.000</td> <td>   -1.127</td> <td>   -0.937</td>\n</tr>\n<tr>\n  <th>age</th>       <td>    0.0345</td> <td>    0.012</td> <td>    2.826</td> <td> 0.005</td> <td>    0.011</td> <td>    0.059</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>124.288</td> <th>  Durbin-Watson:     </th> <td>   0.945</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 244.026</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 1.362</td>  <th>  Prob(JB):          </th> <td>1.02e-53</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.038</td>  <th>  Cond. No.          </th> <td>    201.</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}