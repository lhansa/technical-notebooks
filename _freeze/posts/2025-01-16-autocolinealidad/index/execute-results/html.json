{
  "hash": "4ff319ecdd9a54cef7395a3a821594ea",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Experimento con multicolinealidad\"\ndescription: \"Experimentos con multicolinealidad para ver si siempre es tan mala como se dice en la regresión lineal\"\ndescription-meta: \"Experimentos con multicolinealidad para ver si siempre es tan mala como se dice en la regresión lineal\"\nauthor: \"Leonardo Hansa\"\ndate: \"2025-01-16\"\ncategories: [datos]\nexecute: \n  echo: true\n  message: false\n  warning: false\nfreeze: true\n---\n\n::: {#libs .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.regression.linear_model import OLS\n```\n:::\n\n\n## Multicolinealidad en la regresión lineal con muestras grandes\n\nTienes una variable $y$ que depende de dos variables $x_1$ y $x_2$. Pero $x_2$ es una combinación lineal de $x_1$. \n\n::: {#datos .cell execution_count=2}\n``` {.python .cell-code}\nnp.random.seed(123)\n\nn = 10000\nx1 = np.random.normal(50, 10, n)\nx2 = 0.5 * x1 + np.random.normal(0, 5, n)\ny = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n```\n:::\n\n\nPor si hay alguna duda:\n\n::: {#cell-correlacion .cell execution_count=3}\n``` {.python .cell-code}\nnp.corrcoef(x1, x2)[0, 1]\n```\n\n::: {#correlacion .cell-output .cell-output-display execution_count=3}\n```\n0.7074774568946038\n```\n:::\n:::\n\n\n¿Qué pasa si usas $x_1$ y $x_2$ en una regresión lineal?\n\n::: {#regresion .cell execution_count=4}\n``` {.python .cell-code}\nX = np.column_stack((x1, x2))\nX = sm.add_constant(X)\n\nmodelo = OLS(y, X).fit()\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.967\nModel:                            OLS   Adj. R-squared:                  0.967\nMethod:                 Least Squares   F-statistic:                 1.465e+05\nDate:                Sun, 26 Jan 2025   Prob (F-statistic):               0.00\nTime:                        21:41:39   Log-Likelihood:                -37142.\nNo. Observations:               10000   AIC:                         7.429e+04\nDf Residuals:                    9997   BIC:                         7.431e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.2011      0.508      4.332      0.000       1.205       3.197\nx1             3.0220      0.014    214.712      0.000       2.994       3.050\nx2             3.9480      0.020    199.329      0.000       3.909       3.987\n==============================================================================\nOmnibus:                        0.071   Durbin-Watson:                   1.994\nProb(Omnibus):                  0.965   Jarque-Bera (JB):                0.085\nSkew:                           0.005   Prob(JB):                        0.958\nKurtosis:                       2.989   Cond. No.                         293.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nPues aparentemente nada. Los coeficientes son los esperados y las $t$-stats son altas.\n\n## Multicolinealidad en la regresión lineal con muestras pequeñas\n\nAntes tenías una muestra de tamaño 10000. Vamos a cambiar eso.\n\n::: {#datos-pequenos .cell execution_count=5}\n``` {.python .cell-code}\nn = 100\nx1 = np.random.normal(50, 10, n)\nx2 = 0.5 * x1 + np.random.normal(0, 5, n)\ny = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n```\n:::\n\n\nCon estos datos, aparentemente los mismos pero con menor muestra, ajusto la regresión. \n\n::: {#regresion-pequena .cell execution_count=6}\n``` {.python .cell-code}\nX = np.column_stack((x1, x2))\nX = sm.add_constant(X)\n\nmodelo = OLS(y, X).fit()\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.974\nModel:                            OLS   Adj. R-squared:                  0.973\nMethod:                 Least Squares   F-statistic:                     1810.\nDate:                Sun, 26 Jan 2025   Prob (F-statistic):           1.60e-77\nTime:                        21:41:39   Log-Likelihood:                -367.78\nNo. Observations:                 100   AIC:                             741.6\nDf Residuals:                      97   BIC:                             749.4\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.7233      4.880     -0.353      0.725     -11.410       7.963\nx1             3.0745      0.140     21.956      0.000       2.797       3.352\nx2             3.9304      0.190     20.682      0.000       3.553       4.308\n==============================================================================\nOmnibus:                        0.041   Durbin-Watson:                   2.129\nProb(Omnibus):                  0.980   Jarque-Bera (JB):                0.191\nSkew:                           0.014   Prob(JB):                        0.909\nKurtosis:                       2.788   Cond. No.                         295.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nLos coeficientes de $x_1$ y $x_2$ no son horribles, pero el intercept ya no tiene sentido. \n\n## Incertidumbre\n\nLos resultados de la regresión pueden mejorar, no solo por tener más datos, sino simplemente por una muestra que encaje mejor. \n\nAsí que vamos a generar muchas muestras, de distintos tamaños, y vemos en cada una cómo cambian los coeficientes.\n\n::: {#simulacion .cell execution_count=7}\n``` {.python .cell-code}\nn_sizes = np.array([100, 1000, 2500, 5000, 7500, 10000])\n\nruns_per_size = 50\n\ncoeficientes = np.zeros((len(n_sizes), runs_per_size, 3))\n\nfor i, n in enumerate(n_sizes):\n    for j in range(runs_per_size):\n        x1 = np.random.normal(50, 10, n)\n        x2 = 0.5 * x1 + np.random.normal(0, 5, n)\n        y = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n        \n        X = np.column_stack((x1, x2))\n        X = sm.add_constant(X)\n\n        modelo = OLS(y, X).fit()\n        coeficientes[i, j] = modelo.params\n```\n:::\n\n\nVisualizo los coeficientes como puntos, incluido el intercept. En el eje $x$ tienes el tamaño de la muestra y en el eje $y$ el valor del coeficiente. La línea horizontal es el valor real del coeficiente.\n\nFíjate cómo, cuanto menor es el tamaño muestral, menor es la precisión de la estimación. No creas que por que alguna vez salga bien con una muestra, tu estimación va a ser buena. La incertidumbre es alta, te guste o no.\n\n::: {#cell-grafico .cell execution_count=8}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 3, figsize=(18, 4))\n\nfor i in range(runs_per_size):\n    ax[0].scatter(n_sizes, coeficientes[:, i, 0], color='blue', alpha=0.1)\n    ax[1].scatter(n_sizes, coeficientes[:, i, 1], color='blue', alpha=0.1)\n    ax[2].scatter(n_sizes, coeficientes[:, i, 2], color='blue', alpha=0.1)\n\nax[0].axhline(2, color='red', linestyle='--')\nax[1].axhline(3, color='red', linestyle='--')\nax[2].axhline(4, color='red', linestyle='--')\n\n\nax[0].set_title('Intercept')\nax[1].set_title('Coeficiente de x1')\nax[2].set_title('Coeficiente de x2')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/grafico-output-1.png){#grafico width=1399 height=357}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}