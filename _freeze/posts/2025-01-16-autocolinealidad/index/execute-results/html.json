{
  "hash": "ebd0b4f7d96aef43fa15049123f9c603",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Experimento con autocolinealidad\"\ndescription: \"Experimentos con autocolinealidad para ver si siempre es tan mala como se dice en la regresión lineal\"\ndescription-meta: \"Experimentos con autocolinealidad para ver si siempre es tan mala como se dice en la regresión lineal\"\nauthor: \"Leonardo Hansa\"\ndate: \"2025-01-16\"\ncategories: [datos]\nexecute: \n  echo: true\n  message: false\n  warning: false\nfreeze: true\n---\n\n\n## Autocolinealidad en la regresión lineal con muestras grandes\n\nTienes una variable $y$ que depende de dos variables $x_1$ y $x_2$. Pero $x_2$ es una combinación lineal de $x_1$. \n\n::: {#libs .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.regression.linear_model import OLS\n```\n:::\n\n\n::: {#datos .cell execution_count=2}\n``` {.python .cell-code}\nn = 10000\nx1 = np.random.normal(50, 10, n)\nx2 = 0.5 * x1 + np.random.normal(0, 5, n)\ny = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n```\n:::\n\n\n¿Qué pasa si usas $x_1$ y $x_2$ en una regresión lineal?\n\n::: {#regresion .cell execution_count=3}\n``` {.python .cell-code}\nX = np.column_stack((x1, x2))\nX = sm.add_constant(X)\n\nmodelo = OLS(y, X).fit()\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.967\nModel:                            OLS   Adj. R-squared:                  0.967\nMethod:                 Least Squares   F-statistic:                 1.452e+05\nDate:                Thu, 16 Jan 2025   Prob (F-statistic):               0.00\nTime:                        22:53:56   Log-Likelihood:                -37256.\nNo. Observations:               10000   AIC:                         7.452e+04\nDf Residuals:                    9997   BIC:                         7.454e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.9362      0.513      3.778      0.000       0.931       2.941\nx1             2.9751      0.014    208.083      0.000       2.947       3.003\nx2             4.0514      0.020    201.997      0.000       4.012       4.091\n==============================================================================\nOmnibus:                        0.939   Durbin-Watson:                   1.984\nProb(Omnibus):                  0.625   Jarque-Bera (JB):                0.963\nSkew:                          -0.006   Prob(JB):                        0.618\nKurtosis:                       2.954   Cond. No.                         291.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nPues aparentemente nada. Los coeficientes son los esperados y las $t$-stats son altas.\n\n## Autocolinealidad en la regresión lineal con muestras pequeñas\n\nAntes tenías una muestra de tamaño `python n`. Vamos a cambiar eso.\n\n::: {#datos-pequenos .cell execution_count=4}\n``` {.python .cell-code}\nn = 100\nx1 = np.random.normal(50, 10, n)\nx2 = 0.5 * x1 + np.random.normal(0, 5, n)\ny = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n```\n:::\n\n\nCon estos datos, aparentemente los mismos pero con menor muestra, ajusto la regresión. \n\n::: {#regresion-pequena .cell execution_count=5}\n``` {.python .cell-code}\nX = np.column_stack((x1, x2))\nX = sm.add_constant(X)\n\nmodelo = OLS(y, X).fit()\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.971\nModel:                            OLS   Adj. R-squared:                  0.970\nMethod:                 Least Squares   F-statistic:                     1608.\nDate:                Thu, 16 Jan 2025   Prob (F-statistic):           4.26e-75\nTime:                        22:53:56   Log-Likelihood:                -367.71\nNo. Observations:                 100   AIC:                             741.4\nDf Residuals:                      97   BIC:                             749.2\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.9188      5.129     -0.569      0.571     -13.098       7.260\nx1             3.1849      0.141     22.585      0.000       2.905       3.465\nx2             3.7938      0.188     20.143      0.000       3.420       4.168\n==============================================================================\nOmnibus:                        3.931   Durbin-Watson:                   2.372\nProb(Omnibus):                  0.140   Jarque-Bera (JB):                3.710\nSkew:                          -0.253   Prob(JB):                        0.156\nKurtosis:                       3.796   Cond. No.                         309.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nLos coeficientes de $x_1$ y $x_2$ no son horribles, pero el intercept ya no tiene sentido. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}