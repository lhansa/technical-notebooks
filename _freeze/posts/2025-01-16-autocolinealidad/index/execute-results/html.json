{
  "hash": "32e0cdaf4919e542c9854812543d8af3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Experimento con multicolinealidad\"\ndescription: \"Experimentos con multicolinealidad para ver si siempre es tan mala como se dice en la regresión lineal\"\ndescription-meta: \"Experimentos con multicolinealidad para ver si siempre es tan mala como se dice en la regresión lineal\"\nauthor: \"Leonardo Hansa\"\ndate: \"2025-01-16\"\ncategories: [datos]\nexecute: \n  echo: true\n  message: false\n  warning: false\nfreeze: true\n---\n\n::: {#libs .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.regression.linear_model import OLS\n```\n:::\n\n\n## Multicolinealidad en la regresión lineal con muestras grandes\n\nTienes una variable $y$ que depende de dos variables $x_1$ y $x_2$. Pero $x_2$ es una combinación lineal de $x_1$. \n\n::: {#datos .cell execution_count=2}\n``` {.python .cell-code}\nn = 10000\nx1 = np.random.normal(50, 10, n)\nx2 = 0.5 * x1 + np.random.normal(0, 5, n)\ny = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n```\n:::\n\n\nPor si hay alguna duda:\n\n::: {#cell-correlacion .cell execution_count=3}\n``` {.python .cell-code}\nnp.corrcoef(x1, x2)[0, 1]\n```\n\n::: {#correlacion .cell-output .cell-output-display execution_count=9}\n```\n0.697235733025426\n```\n:::\n:::\n\n\n¿Qué pasa si usas $x_1$ y $x_2$ en una regresión lineal?\n\n::: {#regresion .cell execution_count=4}\n``` {.python .cell-code}\nX = np.column_stack((x1, x2))\nX = sm.add_constant(X)\n\nmodelo = OLS(y, X).fit()\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.965\nModel:                            OLS   Adj. R-squared:                  0.965\nMethod:                 Least Squares   F-statistic:                 1.398e+05\nDate:                Sun, 19 Jan 2025   Prob (F-statistic):               0.00\nTime:                        21:06:06   Log-Likelihood:                -37199.\nNo. Observations:               10000   AIC:                         7.440e+04\nDf Residuals:                    9997   BIC:                         7.443e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.6701      0.518      5.159      0.000       1.656       3.685\nx1             2.9780      0.014    210.271      0.000       2.950       3.006\nx2             4.0143      0.020    201.193      0.000       3.975       4.053\n==============================================================================\nOmnibus:                        3.669   Durbin-Watson:                   1.977\nProb(Omnibus):                  0.160   Jarque-Bera (JB):                3.463\nSkew:                          -0.002   Prob(JB):                        0.177\nKurtosis:                       2.909   Cond. No.                         296.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nPues aparentemente nada. Los coeficientes son los esperados y las $t$-stats son altas.\n\n## Multicolinealidad en la regresión lineal con muestras pequeñas\n\nAntes tenías una muestra de tamaño `py n`. Vamos a cambiar eso.\n\n::: {#datos-pequenos .cell execution_count=5}\n``` {.python .cell-code}\nn = 100\nx1 = np.random.normal(50, 10, n)\nx2 = 0.5 * x1 + np.random.normal(0, 5, n)\ny = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n```\n:::\n\n\nCon estos datos, aparentemente los mismos pero con menor muestra, ajusto la regresión. \n\n::: {#regresion-pequena .cell execution_count=6}\n``` {.python .cell-code}\nX = np.column_stack((x1, x2))\nX = sm.add_constant(X)\n\nmodelo = OLS(y, X).fit()\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.964\nModel:                            OLS   Adj. R-squared:                  0.964\nMethod:                 Least Squares   F-statistic:                     1313.\nDate:                Sun, 19 Jan 2025   Prob (F-statistic):           5.67e-71\nTime:                        21:06:06   Log-Likelihood:                -367.44\nNo. Observations:                 100   AIC:                             740.9\nDf Residuals:                      97   BIC:                             748.7\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         11.9141      5.083      2.344      0.021       1.826      22.003\nx1             2.8967      0.135     21.462      0.000       2.629       3.165\nx2             3.8677      0.193     19.992      0.000       3.484       4.252\n==============================================================================\nOmnibus:                        1.625   Durbin-Watson:                   1.845\nProb(Omnibus):                  0.444   Jarque-Bera (JB):                1.263\nSkew:                          -0.271   Prob(JB):                        0.532\nKurtosis:                       3.097   Cond. No.                         297.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nLos coeficientes de $x_1$ y $x_2$ no son horribles, pero el intercept ya no tiene sentido. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}