---
title: "Ejemplo de regresión múltiple con Python"
description: "Pequeña práctica de regresión múltiple con Python, siguiendo ISLP"
description-meta: "Pequeña práctica de regresión múltiple con Python, siguiendo ISLP"
author: "Leonardo Hansa"
date: "2024-10-06"
categories: [exploraciones]
execute: 
  echo: true
  message: false
  warning: false
freeze: true
---

Aquí sigo un ejercicio de regresión lineal con Python del libro _Introduction to Statistical Learning with Python._

## Regresión simple

### Código del ajuste del modelo

```{python}
#| label: libs
import numpy as np
import pandas as pd
from matplotlib.pyplot import subplots

import statsmodels.api as sm

from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF
from statsmodels.stats.anova import anova_lm

from ISLP import load_data
from ISLP.models import (ModelSpec as MS, summarize, poly)
```

```{python}
#| label: load-data
boston = load_data('Boston')
boston.head()
```

Estos datos son un clásico, que incluso han tenido ya _haters_ por no sé qué variable que hace al conjunto racista. Pero yo voy a seguir con el ejemplo de ISLP porque no me quiero complicar ahora. 

El libro ajusta una regresión lineal de `'medv'` frente a `'lstat'`. Por supuesto, ni idea de qué es cada una. Pero solo quier practicar código Python; el resultado nos da igual. 

```{python}
#| label: datos
X = pd.DataFrame({'intercept': np.ones(boston.shape[0]),
                  'lstat': boston['lstat']})

X.head()                  
```

En **statsmodels** el _intercept_ no se incluye por defecto y es el usuario quien tiene que incluirlo.

```{python}
#| label: ols1
y = boston['medv']
model = sm.OLS(y, X)
results = model.fit()
results.summary()
```

Una forma más generalizable de preparar los datos es con la función `MS()`, que funciona al estilo `sklearn`. Pero es algo propio del libro así que no sé si le haré mucho caso.

```{python}
#| label: fit-tranform
design = MS(['lstat'])
design = design.fit(boston)
X = design.transform(boston)
X.head()
```

También se puede acortar con `fit_transform()`.

A partir del modelo, guardado en `results`, y un conjunto de datos, puedes obtener predicciones.

```{python}
new_df = pd.DataFrame({'lstat': [5, 10, 15]})
newX = design.transform(new_df)
newX
```

Y ahora predices:

```{python}
#| label: predicciones
new_predictions = results.get_prediction(newX)
new_predictions.conf_int(alpha=0.95)
```

### Gráfico

El primer gráfico muestra la regresión calculada.

```{python}
#| label: plot-regression
def abline(ax, b, m, *args, **kwargs):
    "Add a line with slope m and intercept b to ax"
    xlim = ax.get_xlim()
    ylim = [m * xlim[0] + b, m * xlim[1] + b]
    ax.plot(xlim, ylim, *args, **kwargs)

ax = boston.plot.scatter('lstat', 'medv')
abline(ax, 
       results.params[0],
       results.params[1], 
       'r--', 
       linewidth=3)
```

El segundo gráfico compara los residuos frente a los valores ajustados. 

Parece que los valores ajustados más pequeños están asociados a residuos negativos y altos.
En valores más altos de ajuste, hay residuos tanto positivos como negativos, pero se concentran muchos más en casos negativos.

```{python}
#| label: residuals
ax = subplots(figsize=(8,8))[1]
ax.scatter(results.fittedvalues, results.resid)
ax.set_xlabel('Fitted value')
ax.set_ylabel('Residual')
ax.axhline(0, c='k', ls= '--')
```

El _leverage_ es una métrica que indica cuánto se aleja un punto del centroide de toda la nube de puntos. Puntos muy alejados tendrán una mayor influencia en el cálculo del coeficiente que puntos cercanos. 

El gráfico muestra el _leverage_ contra el índice del punto, lo que ayuda a identificar puntos más influyentes (en el caso de una serie temporal, se podría ver como una evolución).

```{python}
#| label: plot-levarage
infl = results.get_influence()
ax = subplots(figsize=(8,8))[1]
ax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)
ax.set_xlabel('Index')
ax.set_ylabel('Leverage')
np.argmax(infl.hat_matrix_diag)
```

## Regresión múltiple

Ahora enfrentan `'medv'` a `'lstat'` y `'age'`.


```{python}
#| label: multiple
X = MS(['lstat', 'age']).fit_transform(boston)
model1 = sm.OLS(y, X)
results1 = model1.fit()
results1.summary()
```