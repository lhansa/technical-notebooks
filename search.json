[
  {
    "objectID": "posts/2024-09-08-rust/index.html",
    "href": "posts/2024-09-08-rust/index.html",
    "title": "Introducción práctica a Rust",
    "section": "",
    "text": "No tengo ni idea de Rust, pero he oído que es muy rápido y gestiona la RAM de manera más automática que C++. C++ es fácil de aprovechar con R para utilizar su velocidad pero me tienta probar cosas nuevas.\nPara usar R y Rust he leído sobre el paquete rextendr.\nlibrary(rextendr)\n\n# create a Rust function\nrust_function(\"fn add(a:f64, b:f64) -&gt; f64 { a + b }\")\n\n# call it from R\nadd(2.5, 4.7)\n\n[1] 7.2\nEse código básico funciona. Se ha creado un objeto nuevo add que es de tipo function."
  },
  {
    "objectID": "posts/2024-09-08-rust/index.html#cálculo-de-r2-con-rust",
    "href": "posts/2024-09-08-rust/index.html#cálculo-de-r2-con-rust",
    "title": "Introducción práctica a Rust",
    "section": "Cálculo de R2 con Rust",
    "text": "Cálculo de R2 con Rust\nAhora creo una función que, dados dos vectores, calcule el R2 de ambos, como si uno fuese la predicción que un modelo hace para el otro.\nObservación. Yo no sé Rust pero ChatGPT y otras IIAA sí saben.\n\ncode &lt;- r\"(\n    #[extendr]\n    fn calcula_r2_rust(actual: &[f64], predicted: &[f64]) -&gt; f64 {\n     \n        let n = actual.len() as f64;\n        let mean_actual = actual.iter().sum::&lt;f64&gt;() / n;\n\n        let ssr = actual.iter().enumerate().map(|(i, x)| (x - predicted[i]).powi(2)).sum::&lt;f64&gt;();\n        let sst = actual.iter().map(|(x)| (x - mean_actual).powi(2)).sum::&lt;f64&gt;();\n        1.0 - ssr / sst\n    \n    }\n)\"\nrust_source(\n    code = code, \n)\n\nAhora me invento unos datos y aplico la función.\n\nnsize &lt;- 1e6\nvalor_real &lt;- rnorm(nsize, 10, 1)\nvalor_pred &lt;- valor_real + rnorm(nsize, 0, 0.5)\n\ncalcula_r2_rust(valor_real, valor_pred)\n\n[1] 0.7492446"
  },
  {
    "objectID": "posts/2024-09-08-rust/index.html#comparativa-de-tiempos-de-ejecución",
    "href": "posts/2024-09-08-rust/index.html#comparativa-de-tiempos-de-ejecución",
    "title": "Introducción práctica a Rust",
    "section": "Comparativa de tiempos de ejecución",
    "text": "Comparativa de tiempos de ejecución\nCreo una función para calcular eso mismo con R.\n\ncalcula_r2_r &lt;- function(actual, pred) {\n    numerador &lt;- sum((actual - pred) ^ 2)\n    mean_actual &lt;- mean(actual)\n    denominador &lt;- sum((actual - mean_actual) ^ 2)\n\n    1 - numerador / denominador\n}\n\ncalcula_r2_r(valor_real, valor_pred)\n\n[1] 0.7492446\n\n\nY ahora lo divertido, comparo tiempos. Que, por supuesto, no tienen ningún sentido.\n\nmicrobenchmark::microbenchmark(\n    rust = calcula_r2_rust(valor_real, valor_pred), \n    r = calcula_r2_r(valor_real, valor_pred)\n)\n\nUnit: milliseconds\n expr      min       lq     mean   median       uq      max neval\n rust 60.86050 62.41938 67.36861 64.71865 72.43868 86.67186   100\n    r 16.77276 17.47662 24.79657 23.71143 30.52387 50.23770   100\n\n\nYa, otro día comparo con Rcpp y lo llevo a Python."
  },
  {
    "objectID": "posts/2025-01-24-mas-relaciones-espurias/index.html",
    "href": "posts/2025-01-24-mas-relaciones-espurias/index.html",
    "title": "Otro experimento con relaciones espurias",
    "section": "",
    "text": "Tienes una variable \\(x_{real}\\) que genera dos variables: \\(y\\) y \\(x_{spur}\\).\n\nlibrary(rethinking)\nlibrary(ggplot2)\n\nn_data &lt;- 100\nx_real &lt;- rnorm(n_data)\nx_spur &lt;- rnorm(n_data, x_real)\ny  &lt;- rnorm(n_data, x_real)\n\ndf &lt;- data.frame(y, x_real, x_spur)\n\nggplot(df) + \n  geom_histogram(aes(x_spur), fill = \"#800080\")\n\n\n\n\n\n\n\n\nComo la relación entre \\(x_{real}\\) y \\(x_{spur}\\) es alta, \\(x_{spur}\\) parece tener una relación con \\(y\\).\n\nggplot(df) + \n  geom_point(aes(x_spur, y)) + \n  geom_smooth(aes(x_spur, y), method = \"lm\")\n\n\n\n\n\n\n\n\nPero \\(y\\) por definición depende únicamente de \\(x_{real}\\), por lo que la relación entre \\(x_{spur}\\) y \\(y\\) es espuria."
  },
  {
    "objectID": "posts/2025-01-24-mas-relaciones-espurias/index.html#datos-y-planteamiento",
    "href": "posts/2025-01-24-mas-relaciones-espurias/index.html#datos-y-planteamiento",
    "title": "Otro experimento con relaciones espurias",
    "section": "",
    "text": "Tienes una variable \\(x_{real}\\) que genera dos variables: \\(y\\) y \\(x_{spur}\\).\n\nlibrary(rethinking)\nlibrary(ggplot2)\n\nn_data &lt;- 100\nx_real &lt;- rnorm(n_data)\nx_spur &lt;- rnorm(n_data, x_real)\ny  &lt;- rnorm(n_data, x_real)\n\ndf &lt;- data.frame(y, x_real, x_spur)\n\nggplot(df) + \n  geom_histogram(aes(x_spur), fill = \"#800080\")\n\n\n\n\n\n\n\n\nComo la relación entre \\(x_{real}\\) y \\(x_{spur}\\) es alta, \\(x_{spur}\\) parece tener una relación con \\(y\\).\n\nggplot(df) + \n  geom_point(aes(x_spur, y)) + \n  geom_smooth(aes(x_spur, y), method = \"lm\")\n\n\n\n\n\n\n\n\nPero \\(y\\) por definición depende únicamente de \\(x_{real}\\), por lo que la relación entre \\(x_{spur}\\) y \\(y\\) es espuria."
  },
  {
    "objectID": "posts/2025-01-24-mas-relaciones-espurias/index.html#regresiones",
    "href": "posts/2025-01-24-mas-relaciones-espurias/index.html#regresiones",
    "title": "Otro experimento con relaciones espurias",
    "section": "Regresiones",
    "text": "Regresiones\nLa regresión que se ajusta según la definición sería:\n\nfit_real &lt;- quap(\n  alist(\n    y ~ dnorm(mu, sigma), \n    mu &lt;- a + b * x_real, \n    a ~ dnorm(0, 0.2), \n    b ~ dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ), \n  data = df\n)\n\nprecis(fit_real)\n\n           mean         sd        5.5%     94.5%\na     0.1200136 0.08733753 -0.01956863 0.2595958\nb     1.0571869 0.09017250  0.91307384 1.2013000\nsigma 0.9596754 0.06750045  0.85179669 1.0675542\n\n\nSi intentas ajustar una regresión que incluya a la variable espuria, el modelo te la rechazará.\n\nfit &lt;- quap(\n    alist(\n        y ~ dnorm(mu, sigma), \n        mu &lt;- a + b_spur * x_spur + b_real * x_real, \n        a ~ dnorm(0, 0.2),\n        b_spur ~ dnorm(0, 0.5),\n        b_real ~ dnorm(0, 0.5),\n        sigma ~ dexp(1)\n    ), \n    data = df\n)\n\nprecis(fit)\n\n              mean         sd        5.5%     94.5%\na       0.11982176 0.08719034 -0.01952525 0.2591688\nb_spur -0.03306708 0.09448089 -0.18406579 0.1179316\nb_real  1.09219751 0.13449093  0.87725503 1.3071400\nsigma   0.95766044 0.06749719  0.84978689 1.0655340\n\n\nSi no quieres hacerlo bayesiano:\n\nfit_lm &lt;- lm(y ~ x_spur + x_real, data = df)\nsummary(fit_lm)\n\n\nCall:\nlm(formula = y ~ x_spur + x_real, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.26639 -0.58538 -0.03526  0.66746  2.29240 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.14138    0.09862   1.434    0.155    \nx_spur      -0.07976    0.09989  -0.798    0.427    \nx_real       1.17575    0.14336   8.201    1e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9749 on 97 degrees of freedom\nMultiple R-squared:  0.5856,    Adjusted R-squared:  0.577 \nF-statistic: 68.53 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nOtra opción sería que sí usaras la variable espuria pero no la real:\n\nfit_spur &lt;- quap(\n  alist(\n    y ~ dnorm(mu, sigma), \n    mu &lt;- a + b * x_spur, \n    a ~ dnorm(0, 0.2),\n    b ~ dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ), \n  data = df\n)\n\nprecis(fit_spur)\n\n           mean         sd        5.5%     94.5%\na     0.1670335 0.10619925 -0.00269343 0.3367604\nb     0.5325380 0.08162221  0.40208999 0.6629861\nsigma 1.2435069 0.08730279  1.10398016 1.3830336"
  },
  {
    "objectID": "posts/2023-12-31-conjugate-prior/index.html",
    "href": "posts/2023-12-31-conjugate-prior/index.html",
    "title": "¿Cómo se usan las prioris conjugadas?",
    "section": "",
    "text": "En estadística bayesiana, cuando buscas estimar un parámetro, lo que vas a obtener es una distribución de este parámetro. O sea, no obtienes un valor preciso, sino unas probabilidades asociadas a varios valores, de forma que algunos serán más probables que otros.\nUn método intuitivo de conseguir esta distribución es usar el método del mallado ( grid approximation ). Con este método, relacionas todos los casos posibles, calculas una verosimilitud para cada caso, y lo multiplicas por una probabilidad a priori que decidas. Ese producto es la distribución que buscas para tu parámetro, la posteriori.\nEn algunas situaciones, usar una priori en concreto te puede simplificar luego el cálculo, ya que no te hará falta aplicar los cálculos del mallado: puedes, pero el resultado será el mismo."
  },
  {
    "objectID": "posts/2023-12-31-conjugate-prior/index.html#prioris-conjugados",
    "href": "posts/2023-12-31-conjugate-prior/index.html#prioris-conjugados",
    "title": "¿Cómo se usan las prioris conjugadas?",
    "section": "Prioris conjugados",
    "text": "Prioris conjugados\nEn el caso de lanzar una moneda, estás trabajando con una binomial: tienes ciertos éxitos ante un número posible de casos.\nSi en este caso, usas como priori la distribución beta, entonces surgirá la magia.\nLa distribución es\n\nmonedas &lt;- 100\ncaras &lt;- seq(0, monedas, 1)\np_cara &lt;- seq(0, 1, 0.01)\nsample_size &lt;- 1000\n\nbeta_alpha &lt;- 2\nbeta_beta &lt;- 10\nhist(rbeta(sample_size, beta_alpha, beta_beta), col = \"#800080\")\n\n\n\n\n\n\n\ndf &lt;- as.data.frame(expand.grid(caras, p_cara))\nnames(df) &lt;- c(\"caras\", \"p_cara\")\n\nY podemos simular la posteriori con el método del mallado:\n\ndf$prior &lt;- dbeta(df$p_cara, beta_alpha, beta_beta)\n# df$prior &lt;- dunif(df$p_cara)\n\ndf$likelihood &lt;- dbinom(df$caras, monedas, df$p_cara)\n\ndf$posteriori &lt;- df$likelihood * df$prior\ndf$std_posteriori &lt;- df$posteriori / sum(df$posteriori)\n\n\ncaras_observadas &lt;- 70\nplot(df$p_cara[df$caras == caras_observadas], \n     df$posteriori[df$caras == caras_observadas] / sum(df$posteriori[df$caras == caras_observadas]), \n     type = \"o\", xlab = \"Probabilidad de cara\", ylab = \"Probabilidad\")\nlines(df$p_cara[df$caras == caras_observadas], \n      df$prior[df$caras == caras_observadas] / sum(df$prior[df$caras == caras_observadas]), \n      col = \"red\")\n\n\n\n\n\n\n\n\nLa magia está en que, dado que la distribución a priori es una beta, no necesitamos simular la posteriori, sino que sabemos que ya seguirá una distribución en concreto (una beta también, con pequeños cambios en sus parámetros);\n\nposteriori_simulation &lt;- dbeta(df$p_cara[df$caras == caras_observadas], \n                               beta_alpha + caras_observadas, \n                               beta_beta + monedas - caras_observadas)\n\nposteriori_simulation &lt;- posteriori_simulation / sum(posteriori_simulation)\n\nplot(df$p_cara[df$caras == caras_observadas], \n     df$posteriori[df$caras == caras_observadas] / sum(df$posteriori[df$caras == caras_observadas]), \n     type = \"o\", xlab = \"Probabilidad de cara\", ylab = \"Probabilidad\")\nlines(df$p_cara[df$caras == caras_observadas], \n      df$prior[df$caras == caras_observadas] / sum(df$prior[df$caras == caras_observadas]), \n      col = \"red\")\nlines(df$p_cara[df$caras == caras_observadas], \n      posteriori_simulation, col = \"blue\")"
  },
  {
    "objectID": "posts/2025-01-16-autocolinealidad/index.html",
    "href": "posts/2025-01-16-autocolinealidad/index.html",
    "title": "Experimento con multicolinealidad",
    "section": "",
    "text": "import numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.regression.linear_model import OLS"
  },
  {
    "objectID": "posts/2025-01-16-autocolinealidad/index.html#multicolinealidad-en-la-regresión-lineal-con-muestras-grandes",
    "href": "posts/2025-01-16-autocolinealidad/index.html#multicolinealidad-en-la-regresión-lineal-con-muestras-grandes",
    "title": "Experimento con multicolinealidad",
    "section": "Multicolinealidad en la regresión lineal con muestras grandes",
    "text": "Multicolinealidad en la regresión lineal con muestras grandes\nTienes una variable \\(y\\) que depende de dos variables \\(x_1\\) y \\(x_2\\). Pero \\(x_2\\) es una combinación lineal de \\(x_1\\).\n\nnp.random.seed(123)\n\nn = 10000\nx1 = np.random.normal(50, 10, n)\nx2 = 0.5 * x1 + np.random.normal(0, 5, n)\ny = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n\nPor si hay alguna duda:\n\nnp.corrcoef(x1, x2)[0, 1]\n\n0.7074774568946038\n\n\n¿Qué pasa si usas \\(x_1\\) y \\(x_2\\) en una regresión lineal?\n\nX = np.column_stack((x1, x2))\nX = sm.add_constant(X)\n\nmodelo = OLS(y, X).fit()\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.967\nModel:                            OLS   Adj. R-squared:                  0.967\nMethod:                 Least Squares   F-statistic:                 1.465e+05\nDate:                Wed, 12 Mar 2025   Prob (F-statistic):               0.00\nTime:                        15:34:28   Log-Likelihood:                -37142.\nNo. Observations:               10000   AIC:                         7.429e+04\nDf Residuals:                    9997   BIC:                         7.431e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.2011      0.508      4.332      0.000       1.205       3.197\nx1             3.0220      0.014    214.712      0.000       2.994       3.050\nx2             3.9480      0.020    199.329      0.000       3.909       3.987\n==============================================================================\nOmnibus:                        0.071   Durbin-Watson:                   1.994\nProb(Omnibus):                  0.965   Jarque-Bera (JB):                0.085\nSkew:                           0.005   Prob(JB):                        0.958\nKurtosis:                       2.989   Cond. No.                         293.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nPues aparentemente nada. Los coeficientes son los esperados y las \\(t\\)-stats son altas."
  },
  {
    "objectID": "posts/2025-01-16-autocolinealidad/index.html#multicolinealidad-en-la-regresión-lineal-con-muestras-pequeñas",
    "href": "posts/2025-01-16-autocolinealidad/index.html#multicolinealidad-en-la-regresión-lineal-con-muestras-pequeñas",
    "title": "Experimento con multicolinealidad",
    "section": "Multicolinealidad en la regresión lineal con muestras pequeñas",
    "text": "Multicolinealidad en la regresión lineal con muestras pequeñas\nAntes tenías una muestra de tamaño 10000. Vamos a cambiar eso.\n\nn = 100\nx1 = np.random.normal(50, 10, n)\nx2 = 0.5 * x1 + np.random.normal(0, 5, n)\ny = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n\nCon estos datos, aparentemente los mismos pero con menor muestra, ajusto la regresión.\n\nX = np.column_stack((x1, x2))\nX = sm.add_constant(X)\n\nmodelo = OLS(y, X).fit()\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.974\nModel:                            OLS   Adj. R-squared:                  0.973\nMethod:                 Least Squares   F-statistic:                     1810.\nDate:                Wed, 12 Mar 2025   Prob (F-statistic):           1.60e-77\nTime:                        15:34:28   Log-Likelihood:                -367.78\nNo. Observations:                 100   AIC:                             741.6\nDf Residuals:                      97   BIC:                             749.4\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.7233      4.880     -0.353      0.725     -11.410       7.963\nx1             3.0745      0.140     21.956      0.000       2.797       3.352\nx2             3.9304      0.190     20.682      0.000       3.553       4.308\n==============================================================================\nOmnibus:                        0.041   Durbin-Watson:                   2.129\nProb(Omnibus):                  0.980   Jarque-Bera (JB):                0.191\nSkew:                           0.014   Prob(JB):                        0.909\nKurtosis:                       2.788   Cond. No.                         295.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nLos coeficientes de \\(x_1\\) y \\(x_2\\) no son horribles, pero el intercept ya no tiene sentido."
  },
  {
    "objectID": "posts/2025-01-16-autocolinealidad/index.html#incertidumbre",
    "href": "posts/2025-01-16-autocolinealidad/index.html#incertidumbre",
    "title": "Experimento con multicolinealidad",
    "section": "Incertidumbre",
    "text": "Incertidumbre\nLos resultados de la regresión pueden mejorar, no solo por tener más datos, sino simplemente por una muestra que encaje mejor.\nAsí que vamos a generar muchas muestras, de distintos tamaños, y vemos en cada una cómo cambian los coeficientes.\n\nn_sizes = np.array([100, 1000, 2500, 5000, 7500, 10000])\n\nruns_per_size = 50\n\ncoeficientes = np.zeros((len(n_sizes), runs_per_size, 3))\n\nfor i, n in enumerate(n_sizes):\n    for j in range(runs_per_size):\n        x1 = np.random.normal(50, 10, n)\n        x2 = 0.5 * x1 + np.random.normal(0, 5, n)\n        y = 2 + 3 * x1 + 4 * x2 + np.random.normal(0, 10, n)\n        \n        X = np.column_stack((x1, x2))\n        X = sm.add_constant(X)\n\n        modelo = OLS(y, X).fit()\n        coeficientes[i, j] = modelo.params\n\nVisualizo los coeficientes como puntos, incluido el intercept. En el eje \\(x\\) tienes el tamaño de la muestra y en el eje \\(y\\) el valor del coeficiente. La línea horizontal es el valor real del coeficiente.\nFíjate cómo, cuanto menor es el tamaño muestral, menor es la precisión de la estimación. No creas que por que alguna vez salga bien con una muestra, tu estimación va a ser buena. La incertidumbre es alta, te guste o no.\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 3, figsize=(18, 4))\n\nfor i in range(runs_per_size):\n    ax[0].scatter(n_sizes, coeficientes[:, i, 0], color='blue', alpha=0.1)\n    ax[1].scatter(n_sizes, coeficientes[:, i, 1], color='blue', alpha=0.1)\n    ax[2].scatter(n_sizes, coeficientes[:, i, 2], color='blue', alpha=0.1)\n\nax[0].axhline(2, color='red', linestyle='--')\nax[1].axhline(3, color='red', linestyle='--')\nax[2].axhline(4, color='red', linestyle='--')\n\n\nax[0].set_title('Intercept')\nax[1].set_title('Coeficiente de x1')\nax[2].set_title('Coeficiente de x2')\n\nplt.show()"
  },
  {
    "objectID": "posts/2024-10-06-regresion-multiple-python/index.html",
    "href": "posts/2024-10-06-regresion-multiple-python/index.html",
    "title": "Ejemplo de regresión múltiple con Python",
    "section": "",
    "text": "Aquí sigo un ejercicio de regresión lineal con Python del libro Introduction to Statistical Learning with Python."
  },
  {
    "objectID": "posts/2024-10-06-regresion-multiple-python/index.html#regresión-simple",
    "href": "posts/2024-10-06-regresion-multiple-python/index.html#regresión-simple",
    "title": "Ejemplo de regresión múltiple con Python",
    "section": "Regresión simple",
    "text": "Regresión simple\n\nCódigo del ajuste del modelo\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\n\nimport statsmodels.api as sm\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS, summarize, poly)\n\n\nboston = load_data('Boston')\nboston.head()\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n\n\n\n\n\nEstos datos son un clásico, que incluso han tenido ya haters por no sé qué variable que hace al conjunto racista. Pero yo voy a seguir con el ejemplo de ISLP porque no me quiero complicar ahora.\nEl libro ajusta una regresión lineal de 'medv' frente a 'lstat'. Por supuesto, ni idea de qué es cada una. Pero solo quier practicar código Python; el resultado nos da igual.\n\nX = pd.DataFrame({'intercept': np.ones(boston.shape[0]),\n                  'lstat': boston['lstat']})\n\nX.head()                  \n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n4\n1.0\n5.33\n\n\n\n\n\n\n\nEn statsmodels el intercept no se incluye por defecto y es el usuario quien tiene que incluirlo.\n\ny = boston['medv']\nmodel = sm.OLS(y, X)\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.544\n\n\nModel:\nOLS\nAdj. R-squared:\n0.543\n\n\nMethod:\nLeast Squares\nF-statistic:\n601.6\n\n\nDate:\nMon, 14 Oct 2024\nProb (F-statistic):\n5.08e-88\n\n\nTime:\n11:25:22\nLog-Likelihood:\n-1641.5\n\n\nNo. Observations:\n506\nAIC:\n3287.\n\n\nDf Residuals:\n504\nBIC:\n3295.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n34.5538\n0.563\n61.415\n0.000\n33.448\n35.659\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.000\n-1.026\n-0.874\n\n\n\n\n\n\n\n\nOmnibus:\n137.043\nDurbin-Watson:\n0.892\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n291.373\n\n\nSkew:\n1.453\nProb(JB):\n5.36e-64\n\n\nKurtosis:\n5.319\nCond. No.\n29.7\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nUna forma más generalizable de preparar los datos es con la función MS(), que funciona al estilo sklearn. Pero es algo propio del libro así que no sé si le haré mucho caso.\n\ndesign = MS(['lstat'])\ndesign = design.fit(boston)\nX = design.transform(boston)\nX.head()\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n4\n1.0\n5.33\n\n\n\n\n\n\n\nTambién se puede acortar con fit_transform().\nA partir del modelo, guardado en results, y un conjunto de datos, puedes obtener predicciones.\n\nnew_df = pd.DataFrame({'lstat': [5, 10, 15]})\nnewX = design.transform(new_df)\nnewX\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n5\n\n\n1\n1.0\n10\n\n\n2\n1.0\n15\n\n\n\n\n\n\n\nY ahora predices:\n\nnew_predictions = results.get_prediction(newX)\nnew_predictions.conf_int(alpha=0.95)\n\narray([[29.7781697 , 29.82901852],\n       [25.03485131, 25.07184337],\n       [20.28485052, 20.32135063]])\n\n\n\n\nGráfico\nEl primer gráfico muestra la regresión calculada.\n\ndef abline(ax, b, m, *args, **kwargs):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim, *args, **kwargs)\n\nax = boston.plot.scatter('lstat', 'medv')\nabline(ax, \n       results.params[0],\n       results.params[1], \n       'r--', \n       linewidth=3)\n\n\n\n\n\n\n\n\nEl segundo gráfico compara los residuos frente a los valores ajustados.\nParece que los valores ajustados más pequeños están asociados a residuos negativos y altos. En valores más altos de ajuste, hay residuos tanto positivos como negativos, pero se concentran muchos más en casos negativos.\n\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls= '--')\n\n\n\n\n\n\n\n\nEl leverage es una métrica que indica cuánto se aleja un punto del centroide de toda la nube de puntos. Puntos muy alejados tendrán una mayor influencia en el cálculo del coeficiente que puntos cercanos.\nEl gráfico muestra el leverage contra el índice del punto, lo que ayuda a identificar puntos más influyentes (en el caso de una serie temporal, se podría ver como una evolución).\n\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag)\n\n374"
  },
  {
    "objectID": "posts/2024-10-06-regresion-multiple-python/index.html#regresión-múltiple",
    "href": "posts/2024-10-06-regresion-multiple-python/index.html#regresión-múltiple",
    "title": "Ejemplo de regresión múltiple con Python",
    "section": "Regresión múltiple",
    "text": "Regresión múltiple\nAhora enfrentan 'medv' a 'lstat' y 'age'.\n\nX = MS(['lstat', 'age']).fit_transform(boston)\nmodel1 = sm.OLS(y, X)\nresults1 = model1.fit()\nresults1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.551\n\n\nModel:\nOLS\nAdj. R-squared:\n0.549\n\n\nMethod:\nLeast Squares\nF-statistic:\n309.0\n\n\nDate:\nMon, 14 Oct 2024\nProb (F-statistic):\n2.98e-88\n\n\nTime:\n11:25:23\nLog-Likelihood:\n-1637.5\n\n\nNo. Observations:\n506\nAIC:\n3281.\n\n\nDf Residuals:\n503\nBIC:\n3294.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n33.2228\n0.731\n45.458\n0.000\n31.787\n34.659\n\n\nlstat\n-1.0321\n0.048\n-21.416\n0.000\n-1.127\n-0.937\n\n\nage\n0.0345\n0.012\n2.826\n0.005\n0.011\n0.059\n\n\n\n\n\n\n\n\nOmnibus:\n124.288\nDurbin-Watson:\n0.945\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n244.026\n\n\nSkew:\n1.362\nProb(JB):\n1.02e-53\n\n\nKurtosis:\n5.038\nCond. No.\n201.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "posts/2023-07-21-muestreo-ii/index.html",
    "href": "posts/2023-07-21-muestreo-ii/index.html",
    "title": "Muestreo (II)",
    "section": "",
    "text": "Hace unos días cacharreaba con algunos conceptos sobre el muestro.\nHoy verás conceptos sobre distribuciones de un estadístico de una muestra."
  },
  {
    "objectID": "posts/2023-07-21-muestreo-ii/index.html#distribución-muestral-exacta",
    "href": "posts/2023-07-21-muestreo-ii/index.html#distribución-muestral-exacta",
    "title": "Muestreo (II)",
    "section": "Distribución muestral exacta",
    "text": "Distribución muestral exacta\nImagina que tienes cuatro dados y los lanzas. El primer dado puede sacar uno de seis posibles valores; el segundo, igual; y así… Es decir, el primero puede sacar 6 valores, el segundo, otros 6… Matemáticamente:\n\\[6 \\times 6 \\times 6 \\times 6 = 6 ^ 4 = 6 ^ {\\mbox{dados}}\\]\nLanzas una vez y calculas la suma de las caras que ves sobre la mesa (un juego de dados típicos es apostar a la suma de las caras tras lanzar dos dados). Lanzas otra vez y calculas la suma de nuevo. Y así…\n\n\n\n\n\n\n\n\n\nEste, con 4 dados, es un caso razonablemente pequeño y has podido calcular todos los resultados posibles. Concretamente, ese gráfico representa la distribución exacta de la suma de lanzar 4 dados.\nEn el gráfico puedes ver que el resultado con más casos favorables es 14, con 146 repeticiones.\n¿Pero qué ocurre si tienes más dados?"
  },
  {
    "objectID": "posts/2023-07-21-muestreo-ii/index.html#simulación-de-la-distribución-muestral",
    "href": "posts/2023-07-21-muestreo-ii/index.html#simulación-de-la-distribución-muestral",
    "title": "Muestreo (II)",
    "section": "Simulación de la distribución muestral",
    "text": "Simulación de la distribución muestral\nEn el curso de Datacamp de donde estoy sacando estas ideas dicen que, si tiras 100 dados, tienes tantas combinaciones posibles como átomos hay en el universo: 6.5331862^{77}\nCuando no puedes replicar la distribución exacta porque el número de casos es enorme, o porque no tienes acceso a toda la población (bien por tecnología, bien por coste, …), necesitas una aproximación a esa distribución.\nPor ejemplo, si en el caso de 4 dados no pudiéramos replicar todos los lanzamientos, podríamos buscar una aproximación mediante algunos lanzamientos. En el gráfico anterior están los 1296 casos posibles. Vamos a ver cómo queda esa distribución si lo aproximamos con 750 simulaciones.\n\n\n\n\n\n\n\n\n\nNo es igual pero la forma es parecida. Una nota importante: en este caso estamos simulando, o sea, no estamos tomando los resultados posibles reales, sino que estamos haciendo como que tiramos los dados y apuntamos la suma. Es decir, como no es el caso teórico, incluso aunque repitiéramos la simulación tantas veces como casos posibles hay, nada garantiza que vayamos a obtener la distribución exacta de antes.\n\n\n\n\n\n\n\n\n\n\nY esto, ¿para qué sirve?\nOye, Leo, pero yo ya he calculado la distribución exacta. ¿Para qué quiero esta aproximación?\nComo esa aproximación se parece a la distribución exacta, cuando no puedes acceder a la exacta, puedes usar esta:"
  },
  {
    "objectID": "posts/2023-07-21-muestreo-ii/index.html#teorema-central-del-límite",
    "href": "posts/2023-07-21-muestreo-ii/index.html#teorema-central-del-límite",
    "title": "Muestreo (II)",
    "section": "Teorema central del límite",
    "text": "Teorema central del límite\nImagina que fijas un tamaño muestral. Y para ese tamaño muestral, replicas muchas veces un experimento.\nEl experimento va a consistir en calcular la media de la profundidad de unos diamantes.\nLa media de toda la muestra es 61.7494049.\nEn el siguiente gráfico tienes la distribución de la media muestral cuando el tamaño muestral es 5, 100, 500 y 1.000 (el total de la muestra es superior a 50.000).\n\n\n\n\n\n\n\n\n\nLo interesante de este gráfico es que puedes ver cómo, a medida que aumenta el tamaño muestral, los cálculos de la media con ese tamaño son cada vez más estables. La línea vertical es la media poblacional y puedes ver que las colas son cada vez menos pesadas (más normales).\n\nPero realmente he hablado de estadístico poblacional cuando realmente estoy trabajando con una muestra (aunque tenga 50.000 diamantes, eso no es toda la población de la muestra).\nLa solución a este problema te lo cuento el próximo día."
  },
  {
    "objectID": "posts/2024-08-16-estadisticos-regresion/index.html",
    "href": "posts/2024-08-16-estadisticos-regresion/index.html",
    "title": "Estadísticos en regresión lineal por variable",
    "section": "",
    "text": "Voy a revisar aquí cómo se calcula el p-valor de una variable en una regresión lineal, porque es una cosa que siempre se me olvida. Y quiero tenerlo a mano.\nAdemás, lo voy a hacer en Python porque me gusta sufrir.\n\nimport numpy as np\nimport statsmodels.api as sm\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nMe invento unos datos que me sirvan para ajustar una regresión lineal:\n\\[\ny = \\beta_0 + \\beta_1 \\cdot x + \\varepsilon,\n\\]\ndonde \\(\\beta_0 = - 5\\), \\(\\beta_1 = 3\\) y \\(x ~ \\mbox{Poisson(12)}\\) y \\(\\varepsilon \\sim \\cal{N}(0, 10)\\).\n\nnobs = 10000\nx = np.random.poisson(12, nobs)\nnoise = np.random.normal(0, 10, nobs)\n\ny = 3 * x - 5 + noise\n\nAhora ajusto la regresión lineal con statsmodels. ¿Se puede hacer con sklearn? Pues imagino que sí, pero necesitaré luego acceder a detalles del modelo a los que no sé acceder con sklearn.\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.527\nModel:                            OLS   Adj. R-squared:                  0.527\nMethod:                 Least Squares   F-statistic:                 1.115e+04\nDate:                Mon, 19 Aug 2024   Prob (F-statistic):               0.00\nTime:                        23:37:26   Log-Likelihood:                -37190.\nNo. Observations:               10000   AIC:                         7.438e+04\nDf Residuals:                    9998   BIC:                         7.440e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -5.0025      0.357    -14.009      0.000      -5.702      -4.303\nx1             3.0140      0.029    105.585      0.000       2.958       3.070\n==============================================================================\nOmnibus:                        7.083   Durbin-Watson:                   2.023\nProb(Omnibus):                  0.029   Jarque-Bera (JB):                7.775\nSkew:                          -0.014   Prob(JB):                       0.0205\nKurtosis:                       3.134   Cond. No.                         45.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nPara calcular el p-valor de \\(x\\) en el modelo necesito el estadístico \\(t\\). Para eso necesito, aparte del valor del coeficiente, el error estándar de la variable."
  },
  {
    "objectID": "posts/2024-08-16-estadisticos-regresion/index.html#intro-y-datos",
    "href": "posts/2024-08-16-estadisticos-regresion/index.html#intro-y-datos",
    "title": "Estadísticos en regresión lineal por variable",
    "section": "",
    "text": "Voy a revisar aquí cómo se calcula el p-valor de una variable en una regresión lineal, porque es una cosa que siempre se me olvida. Y quiero tenerlo a mano.\nAdemás, lo voy a hacer en Python porque me gusta sufrir.\n\nimport numpy as np\nimport statsmodels.api as sm\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nMe invento unos datos que me sirvan para ajustar una regresión lineal:\n\\[\ny = \\beta_0 + \\beta_1 \\cdot x + \\varepsilon,\n\\]\ndonde \\(\\beta_0 = - 5\\), \\(\\beta_1 = 3\\) y \\(x ~ \\mbox{Poisson(12)}\\) y \\(\\varepsilon \\sim \\cal{N}(0, 10)\\).\n\nnobs = 10000\nx = np.random.poisson(12, nobs)\nnoise = np.random.normal(0, 10, nobs)\n\ny = 3 * x - 5 + noise\n\nAhora ajusto la regresión lineal con statsmodels. ¿Se puede hacer con sklearn? Pues imagino que sí, pero necesitaré luego acceder a detalles del modelo a los que no sé acceder con sklearn.\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.527\nModel:                            OLS   Adj. R-squared:                  0.527\nMethod:                 Least Squares   F-statistic:                 1.115e+04\nDate:                Mon, 19 Aug 2024   Prob (F-statistic):               0.00\nTime:                        23:37:26   Log-Likelihood:                -37190.\nNo. Observations:               10000   AIC:                         7.438e+04\nDf Residuals:                    9998   BIC:                         7.440e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -5.0025      0.357    -14.009      0.000      -5.702      -4.303\nx1             3.0140      0.029    105.585      0.000       2.958       3.070\n==============================================================================\nOmnibus:                        7.083   Durbin-Watson:                   2.023\nProb(Omnibus):                  0.029   Jarque-Bera (JB):                7.775\nSkew:                          -0.014   Prob(JB):                       0.0205\nKurtosis:                       3.134   Cond. No.                         45.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nPara calcular el p-valor de \\(x\\) en el modelo necesito el estadístico \\(t\\). Para eso necesito, aparte del valor del coeficiente, el error estándar de la variable."
  },
  {
    "objectID": "posts/2024-08-16-estadisticos-regresion/index.html#error-estándar",
    "href": "posts/2024-08-16-estadisticos-regresion/index.html#error-estándar",
    "title": "Estadísticos en regresión lineal por variable",
    "section": "Error estándar",
    "text": "Error estándar\nEste error estándar relaciona la varianza de la variable con la varianza de los errores:\n\nSi una variable tiene poca varianza y los errores tienen mucha, la variable está aportando poco al modelo.\nSi una variable tiene una varianza alta, estará contribuyendo más al modelo.\nSi un modelo tiene errores con varianza baja, el error estándar de la variable tenderá a ser bajo.\n\n\nCuanto mayor sea el error estándar, menor siginificatividad tendrá la variable. Por lo tanto, con un varianza del error pequeña, más fácil será que la variable aparezca significativa.\n\n\nstandard_error = np.sqrt(np.sum(model.resid ** 2) / (len(x) - 2))\nstandard_error_x = standard_error / np.sqrt(nobs)  / np.std(x)\n\nEl error estándar de los residuos es 9.975818941126349 y el de la variable es 0.028545857080981202.\n\nEl error estándar del intercept es más lío de calcular e interpretar; la idea intuitiva es que tiene en cuenta la varianza de todas las variables explicativas a la vez.\n\n\nstandard_error * np.sqrt(1 / nobs + np.mean(x) ** 2 / np.sum((x - np.mean(x)) ** 2))\n\n0.3570820876215312"
  },
  {
    "objectID": "posts/2024-08-16-estadisticos-regresion/index.html#estadístico-t",
    "href": "posts/2024-08-16-estadisticos-regresion/index.html#estadístico-t",
    "title": "Estadísticos en regresión lineal por variable",
    "section": "Estadístico \\(t\\)",
    "text": "Estadístico \\(t\\)\nEl estadístico \\(t\\) se calcula como\n\\[\nt = \\frac{\\mbox{coef}\\ x}{\\mbox{std error}\\ x}.\n\\]\n\nSi la variable tiene un error pequeño, el estadístico crece (eso es bueno para la significatividad de la variable).\nA su vez, si el coeficiente es alto (en valor absoluto), entonces también crece.\nEl estadístico decrece con coeficientes bajos o con errores altos (es decir, la variable parecerá no significativa).\n\n\n# label: t-stat\nt_stat_x = np.abs(model.params[1]) / standard_error_x\nt_stat_x\n\n105.58474083439029"
  },
  {
    "objectID": "posts/2024-08-16-estadisticos-regresion/index.html#p-valor",
    "href": "posts/2024-08-16-estadisticos-regresion/index.html#p-valor",
    "title": "Estadísticos en regresión lineal por variable",
    "section": "p-valor",
    "text": "p-valor\nAhora hay que ver qué valor es esperable para el estadístico \\(t\\).\nPon que trabajas al nivel de confianza del 90%. Hace falta calcular el \\(t\\) que deja el 5% a un lado de la distribución, y el que lo deja al otro lado. Como la distribución es simétrica, solo calculo uno:\n\npercentile = stats.t.ppf(0.95, nobs - 2)\npercentile\n\n1.6450060485564049\n\n\nAhora tienes la distribución de \\(t\\). Si \\(t\\) cae en el área sombreada (el 10% extremo) entonces la probabilidad de observar tus datos en el supuesto de que la variable no sea significativa (\\(H_0\\) o \\(\\beta_1 = 0\\)) es menor de 0,10.\n\nxx = np.linspace(-4, 4, 1000)\nyy = stats.t.pdf(xx, nobs - 2)\nplt.plot(xx, yy, color='#800080')\n# plot the t distribution and fill the 5% outside \nplt.fill_between(xx, yy, where=(xx &lt; -percentile) | (xx &gt; percentile), alpha=0.5, color='#800080')\nplt.title(\"Distribución de t\")\nplt.show()\n\n\n\n\n\n\n\n\nConcretamente, la probabilidad para el estadístico \\(t\\) que has obtenido es muy muy baja.\n\n# p value for coefficient, extracted from\n# t distribution\nstats.t.sf(t_stat_x, nobs - 2) * 2\n\n0.0"
  },
  {
    "objectID": "posts/2024-08-16-estadisticos-regresion/index.html#idea-intuitiva",
    "href": "posts/2024-08-16-estadisticos-regresion/index.html#idea-intuitiva",
    "title": "Estadísticos en regresión lineal por variable",
    "section": "Idea intuitiva",
    "text": "Idea intuitiva\nLo que estás haciendo con esto es calcular la probabilidad de obtener los datos que tienes si asumes que las dos variables no tienen relación.\nImagina que estás midiendo la relación entre el consumo de torreznos (gramos de torreznos semanales) y los niveles de colesterol (ni idea de qué unidad se usa en los análisis).\nLa forma de pensar con este análisis sería:\n\nAsumes que no hay relación\nTienes unos datos de varias personas, con su consumo de torreznos y el nivel de colesterol.\nHaces el cálculo de la regresión.\nEl p-valor es cómo de probables son tus datos.\nSi tu p-valor es pequeño, entonces los datos son poco probables en el supuesto de que no haya relación. Así que deduces que sí hay relación.\n\n¿Qué quiero decir con que tu p-valor sea pequeño?\nMe refiero a que, previamente, habrás definido un umbral. Que tu p-valor sea pequeño significa que está por debajo de ese umbral.\n¿Cuál debería ser tu umbral? Normalmente es 0,05. Eso se traduce en que:\n\nSi la probabilidad de observar tus datos en el supuesto de que no hay relación (p-valor) es menor de 0,05, entonces es que sí hay relación.\nSi la probabilidad (p-valor) es mayor, entonces es que no hay relación.\n\nY te pregunto yo. ¿Cuál debería ser para ti la probabilidad? Por ejemplo, si el p-valor es 0,20, ¿automáticamente concluyes que no hay relación?\n¿Por qué?\nSi tu p-valor es ese, estarías diciendo que la probabilidad de observar tus datos cuando no hubiera relación real es de un 20%. Quizá en un caso médico tienes que tener cuidado, como con lo del colesterol. ¿Pero qué pasa si estás midiendo la eficacia de una acción empresarial?"
  },
  {
    "objectID": "posts/2024-08-16-estadisticos-regresion/index.html#un-ejemplo-más-extremo",
    "href": "posts/2024-08-16-estadisticos-regresion/index.html#un-ejemplo-más-extremo",
    "title": "Estadísticos en regresión lineal por variable",
    "section": "Un ejemplo más extremo",
    "text": "Un ejemplo más extremo\nEn lugar de utilizar la propia variable \\(x\\) para ajustar el modelo, podemos alterar \\(x\\) introduciendo ruido en ella. Eso lo haré metiendo ruido en la variable, con media 0 y una desviación típica que variaré en varias pruebas.\nPor ahora, la desviación típica del ruido sobre \\(x\\) es 1.\n\n\n\n\n\n\n\n\n\nVoy a poner mucho más ruido. Fíjate en cuánto decrece el estadístico \\(t\\) (y en consecuencia cuánto aumenta el p-valor).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats.t.sf(0.5341, 9998)\n\n0.29664211233560395"
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "",
    "text": "Mapa y territorio. Un cambio de paradigma te puede ayudar a controlar mejor el territorio.\n\nMucha autoayuda falla porque enseñan un mapa sobre un territorio equivocado. Si estás usando un mapa que no se ajusta a tu territorio, no lo puedes aplicar.\n\nInside-out. Las victorias personales van antes que las públicas. Para mejorar las relaciones con los demás debes mejorar antes la tuya contigo mismo."
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#ideas-introductorias",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#ideas-introductorias",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "",
    "text": "Mapa y territorio. Un cambio de paradigma te puede ayudar a controlar mejor el territorio.\n\nMucha autoayuda falla porque enseñan un mapa sobre un territorio equivocado. Si estás usando un mapa que no se ajusta a tu territorio, no lo puedes aplicar.\n\nInside-out. Las victorias personales van antes que las públicas. Para mejorar las relaciones con los demás debes mejorar antes la tuya contigo mismo."
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#sé-proactivo",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#sé-proactivo",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "1. Sé proactivo",
    "text": "1. Sé proactivo\n\nEl amor es un verbo. La gente proactiva se sacrifica por otros.\nAplicación:\n\nDurante un día fíjate en tu lenguaje y el de los demás. “Cuántas veces escuchas no puedo”, “ojalá”, “tengo que”…\nIdentifica una experiencia con la que si te encuentras reaccionarías de manera reactiva. ¿Cómo la convertirás en proactiva?\nElige un problema personal o profesional que te frustre. ¿Qué puedes hacer que ayude a mejorarlo.\nDurante treinta días trabaja en tu círculo de influencia con pequeños compromisos: no juzgues, no critiques, no discutas, no culpes. Trabaja en las cosas sobre las que tienes control."
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#empieza-con-el-final-en-mente",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#empieza-con-el-final-en-mente",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "2. Empieza con el final en mente",
    "text": "2. Empieza con el final en mente\n\nEn tu funeral van a decir unas palabras sobre ti. ¿Qué te gustaría que dijeran? ¿Cómo te gustaría que te recordaran?\nEmpieza cada día con la imagen del final de tu vida, que ese sea tu criterio para evaluar tus acciones.\nTodo se crea en dos fases: una mental y otra física. Tú eres la segunda creación de tu propio diseño proactivo previo, o la segunda creación de los planes de otras personas, o circunstancias o costumbres pasadas.\nEl hábito 1 es “tú eres el creador”. El hábito 2 es la primera creación.\nSolemos necesitar más un destino y una brújula (conjunto de principios y direcciones) y menos una hoja de ruta. El camino por delante no lo conocemos y necesitarás improvisar más de lo previsto.\nSoy quien soy hoy por las decisiones que tomé ayer.\nCuando quieras ser efectivo, piensa de manera amplia. No tiene sentido que te centres en tu éxito profesional si dejas de lado tu salud.\nPiensa en tu visión a largo y misión en varios roles. Eso te dará balance"
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#pon-primero-lo-primero",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#pon-primero-lo-primero",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "3. Pon primero lo primero",
    "text": "3. Pon primero lo primero\n\nLas personas exitosas tienen el hábito de hacer lo que los fracasados no quieren hacer.\nLa gente poco efectiva dedica mucho tiempo a cosas urgentes poco importantes (en general, actividades con origen en otras personas). Luego desconectan un poco con actividades poco urgentes y poco importantes (Netflix…).\nNo te hagas planes de organización diarios sino semanales."
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#piensa-en-winwin",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#piensa-en-winwin",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "4. Piensa en win/win",
    "text": "4. Piensa en win/win\n\nBusca soluciones que beneficien a todas las partes involucradas.\nHay casos contrarios a eso:\n\nHay incentivos para pensar en win/lose. La ley está motivada por el que hay un enemigo al que hay que ganar, mediante abogados y juicios.\nEl lose/lose ocurre cuando se juntan dos personas cuyo objetivo es hacer que su enemigo pierda, aunque sea a costa de su propio bienestar.\n\nwin/win or no deal. Si no podemos llegar a un acuerdo en el que los dos salgamos beneficiados, es mejor no cerrar acuerdo.\nla actitud de un individuo tiene dos dimensiones: valor y consideración. Win/win consiste en maximizar ambos.\nMentalidad de abundancia: hay suficiente para todos."
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#busca-primero-entender-luego-ser-entendido",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#busca-primero-entender-luego-ser-entendido",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "5. Busca primero entender, luego ser entendido",
    "text": "5. Busca primero entender, luego ser entendido\n\nImagina un óptico que recomienda a un cliente las gafas que tiene él, sin diagnosticar.\nHistoria del Padre que no escucha a su hijo…\nLas necesidades cubiertas no motivan."
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#sinergiza",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#sinergiza",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "6. Sinergiza",
    "text": "6. Sinergiza\n\nTrabaja en equipo y crea sinergias para buscar equipos comunes.\nEjemplo de los fundadores de Apple"
  },
  {
    "objectID": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#afila-la-sierra",
    "href": "posts/2023-07-20-7-habitos-gente-altamente-efectiva/index.html#afila-la-sierra",
    "title": "7 hábitos de la gente altamente efectiva",
    "section": "7. Afila la sierra",
    "text": "7. Afila la sierra\n\nCuidar y mejorar continuamente los cuatro aspectos de tu vida: físico, mental, emocional y espiritual.\n\nQueremos estar saludables pero no dedicamos 30 minutos al ejercicio.\nQueremos ser inteligentes pero no pasamos 20 minutos leyendo.\nTrabajo, familia, relajación…\n\nCuatro dimensiones de control introspectivo: físico, mental, social y espiritual.\nEjercicio de fuerza. Hay que apurar hasta el final porque es como el cuerpo sabrá que hay que regenerar fibras musculares más fuertes. Igual pasa con la paciencia.\nAspecto espiritual: hay personas que encuentran en la inmersión en la música o literatura la misma satisfacción espiritual que otras personas en la meditación o en la lectura de escrituras sagradas.\nPrescripción médica: ve a un sitio que tengas buen recuerdo de niño. A las 9, escucha. A las 12, try reaching back (estudia el pasado). A las 3, explora tus motivos (si tus motivos están mal, entonces todo lo que haces está mal). A las 6, escribe tus preocupaciones en la arena.\nLa lectura nos ayuda con el hábito de primero entender, ya que nos educa en el hábito de no poner nuestra autobiografía en medio de cada idea que nos llegue.\nHans Selye: “una vida larga, sana y feliz es el resultado de hacer contribuciones”\n“Trata a una persona como es, y seguirá igual. Trata a un persona como puede ser, y llegará a ello” (Goethe)"
  },
  {
    "objectID": "posts/2023-07-16-matrimonios/index.html",
    "href": "posts/2023-07-16-matrimonios/index.html",
    "title": "Matrimonios y nacimientos",
    "section": "",
    "text": "Estoy muy preocupado por el Matrimonio: se está viniendo abajo.\nA ver, se sigue casando mucha gente.\nPero con los años la tendencia decrece.\n\n\n\n\n\nyear\nnacimientos\nmatrimonios\ndefunciones\n\n\n\n\n2021\n337380\n148588\n450744\n\n\n2020\n341315\n90670\n493776\n\n\n2019\n360617\nNA\nNA\n\n\n2018\n372777\n167613\n427721\n\n\n2017\n393181\n173626\n424523\n\n\n2016\n410583\n175343\n410611\n\n\n2015\n420290\n168910\n422568\n\n\n2014\n427595\n162554\n395830\n\n\n2013\n425715\n156446\n390419\n\n\n2012\n454648\n168556\n402950\n\n\n\n\n\n(Ni idea de por qué faltan datos en 2019)\n\n\n\n\n\n\n\n\n\nCada vez muere más gente, eso sí que sube, pero imagino que ayuda que cada vez seamos más (si cada vez nacemos menos, imagino que se debe a la inmigración, aunque eso no lo he mirado):"
  },
  {
    "objectID": "posts/2023-12-01-adventofcode-i/index.html",
    "href": "posts/2023-12-01-adventofcode-i/index.html",
    "title": "Advent of code (1)",
    "section": "",
    "text": "Nunca he jugado con el Advent of code. Hoy me ha apetecido.\nPara el ejercicio de hoy, me he empeñado en hacerlo en R base."
  },
  {
    "objectID": "posts/2023-12-01-adventofcode-i/index.html#parte-1",
    "href": "posts/2023-12-01-adventofcode-i/index.html#parte-1",
    "title": "Advent of code (1)",
    "section": "Parte 1",
    "text": "Parte 1\nEl día 1 del Advent of code 2023 nos pide que sumemos los números que aparecen en un texto.\nEl texto tiene una pinta así: \"four95qvkvveight5\", y hay 1.000 como ese.\nLa idea es:\n\ntomar el primer dígito, 9,\ny el último, 5\ny concatenarlos: 95\n\nAsí obtendrás 1000 números. La solución es la suma de esos 1.000 números.\nMi propuesta:\n\ndata &lt;- readLines(\"2023-01.txt\")\n\nget_number &lt;- function(x) {\n  str_number &lt;- gsub(\"[^0-9]\", \"\", x)\n  len &lt;- nchar(str_number)\n  str_number &lt;- paste0(substr(str_number, 1, 1), substr(str_number, len, len))\n  return(as.numeric(str_number))\n}\nnumbers &lt;- vapply(\n  data, \n  get_number,\n  numeric(1)\n)\n\nsum(numbers)\n\n[1] 55208"
  },
  {
    "objectID": "posts/2023-12-01-adventofcode-i/index.html#parte-2",
    "href": "posts/2023-12-01-adventofcode-i/index.html#parte-2",
    "title": "Advent of code (1)",
    "section": "Parte 2",
    "text": "Parte 2\nAhora hay un matiz. Fíjate de nuevo en el texto: \"four95qvkvveight5\". Realmente, el primer número es \"four\", o sea, 4. Lo que hay que hacer ahora es identificar el primer dígito y el último pero tanto si viene en número como en texto.\nLuego igual, concatenarlos y sumarlos.\nMi propuesta:\n\n# a character vector with the numbers from zero to nine in letters\nwords &lt;- c(\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\")\n\nsplit_words &lt;- strsplit(words, \"\")\nwords_inv &lt;- vapply(\n  split_words, \n  function(x) paste0(x[length(x):1], collapse = \"\"), \n  character(1)\n)\n\nwords_regex &lt;- paste0(words, collapse = \"|\")\nwords_inv_regex &lt;- paste0(words_inv, collapse = \"|\")\n\npattern &lt;- paste(\"[0-9]\", words_regex, sep = \"|\")\npattern_inv &lt;- paste(\"[0-9]\", words_inv_regex, sep = \"|\")\n\n# Change the order of a text\ninvert &lt;- function(x) {\n  if (nchar(x) == 1) {\n    return(x)\n  } else {\n    return(paste0(strsplit(x, \"\")[[1]][nchar(x):1], collapse = \"\") )\n  }\n}\n\n# Convert a text to a number, based on the words and words_inv vectors\nto_numeric &lt;- function(x, words = words, words_inv = words_inv) {\n  if (nchar(x) == 1) {\n    return(as.numeric(x))\n  } else if (x %in% words) {\n    return(which(words == x) - 1)\n  } else {\n    return(which(words_inv == x) - 1)\n  }\n}\n\nget_number2 &lt;- function(x, \n                        pattern, \n                        pattern_inv,\n                        words, \n                        words_inv) {\n  \n  first &lt;- regmatches(x, m = regexpr(pattern, x))\n  first &lt;- to_numeric(first, words = words, words_inv = words_inv)\n  \n  x_inv &lt;- invert(x)\n  last &lt;- regmatches(x_inv, m = regexpr(pattern_inv, x_inv))\n  last &lt;- invert(last)\n  last &lt;- to_numeric(last, words = words, words_inv = words_inv)\n  \n  return(first * 10 + last)\n  \n}\n\n\nnumbers2 &lt;- vapply(\n  data, \n  get_number2,\n  pattern = pattern,\n  pattern_inv = pattern_inv,\n  words = words, \n  words_inv = words_inv,\n  FUN.VALUE = numeric(1)\n)\n\nsum(numbers2)\n\n[1] 54578"
  },
  {
    "objectID": "posts/2024-04-21-dataframes-inmemory/index.html",
    "href": "posts/2024-04-21-dataframes-inmemory/index.html",
    "title": "Cómo procesas datos que no te caben en RAM",
    "section": "",
    "text": "Tengo unos 16GB de datos en ficheros parquet en una carpeta.\n# https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\npath_to_folder &lt;- \"../../data/taxiny/\"\n\n# https://stackoverflow.com/a/68145683/7569570\ndir_size &lt;- function(path, recursive = TRUE) {\n  stopifnot(is.character(path))\n  files &lt;- list.files(path, full.names = T, recursive = recursive)\n  vect_size &lt;- sapply(files, function(x) file.size(x))\n  size_files &lt;- sum(vect_size)\n  size_files\n}\n\ncat(dir_size(path_to_folder)/10**9, \"GB\")\n\n16.74646 GB\nCada fichero tiene datos de trayectos de taxi en la ciudad de Nueva York.\nQuiero calcular cuántos trayectos con tarifa de aeropuerto ha habido en cada día, desde enero de 2021 hasta febrero de 2024. Es una operación con una estructura sencilla: si te cupiera en un Excel lo podrías plantear en una tabla dinámica sin muchas especificaciones.\nO con SQL como lingua franca, si tuviera todos los datos en una tabla, sería algo así:\nSELECT day, count(*)\nFROM Viajes\nWHERE airport_fee &gt; 0\nGROUP BY day\nPero no los tengo todos en una tabla, sino en 38 ficheros. En cualquier programa orientado a dato es fácil juntar varias en tablas en una, con algún concat o variantes. Pero tengo 16GB de datos para un portátil de 8GB de RAM: no voy a poder.\nAsí que necesito calcular eso sin cargar todo en memoria. ¿Cómo procedo?"
  },
  {
    "objectID": "posts/2024-04-21-dataframes-inmemory/index.html#versión-directa",
    "href": "posts/2024-04-21-dataframes-inmemory/index.html#versión-directa",
    "title": "Cómo procesas datos que no te caben en RAM",
    "section": "Versión directa",
    "text": "Versión directa\nSi lo que conoces es R (aplica también a pandas y Python), podrías plantear un bucle que itere sobre cada fichero.\nLos ficheros están en parquet, así que usaré la librería arrow para cargar sus datos en un data frame.\n\nlibrary(dplyr)\nlibrary(arrow)\n\nLa operación con un solo fichero sería algo así:\n\ndf &lt;- read_parquet(\n  \"../../data/taxiny/fhvhv_tripdata_2023-01.parquet\", \n  col_select = c(\"pickup_datetime\", \"airport_fee\"),\n  as_data_frame = TRUE\n)\n\ndf |&gt; \n  filter(airport_fee &gt; 0) |&gt; \n  mutate(day = as.Date(pickup_datetime)) |&gt;\n  summarise(count = n(), .by = day) |&gt; \n  slice_head(n = 10)\n\n# A tibble: 10 × 2\n   day        count\n   &lt;date&gt;     &lt;int&gt;\n 1 2023-01-01 41384\n 2 2023-01-02 50682\n 3 2023-01-03 50072\n 4 2023-01-04 43015\n 5 2023-01-05 42661\n 6 2023-01-06 41856\n 7 2023-01-07 36971\n 8 2023-01-08 50466\n 9 2023-01-09 46022\n10 2023-01-10 39091\n\n\nLuego vemos lo de as_data_frame = TRUE. Por ahora metemos eso en un bucle."
  },
  {
    "objectID": "posts/2024-04-21-dataframes-inmemory/index.html#método-1.-bucle",
    "href": "posts/2024-04-21-dataframes-inmemory/index.html#método-1.-bucle",
    "title": "Cómo procesas datos que no te caben en RAM",
    "section": "Método 1. Bucle",
    "text": "Método 1. Bucle\nLo más intuitivo para mí siempre ha sido un bucle. Que lo plantees directamente con un for(), con un lapply() más rbind y do.call() o con otro conjunto de herramientas es cosa tuya.\nMe resulta directo map_dfr() de purrr porque lo que necesito al final es un data frame de todos los data frames intermedios apilados por filas.\n\npaths_a_ficheros &lt;- list.files(path_to_folder,\n                               pattern = \"parquet$\", \n                               full.names = TRUE)\n\nsystem.time(\n  df_all &lt;- purrr::map_dfr(paths_a_ficheros, function(.path) {\n    df &lt;- read_parquet(\n      .path,\n      col_select = c(\"pickup_datetime\", \"airport_fee\"),\n      as_data_frame = TRUE\n    )\n    \n    df &lt;- df |&gt; \n      filter(airport_fee &gt; 0) |&gt; \n      mutate(day = as.Date(pickup_datetime)) |&gt;\n      summarise(count = n(), .by = day)\n    \n    # gc()\n    \n    return(df)\n  }) \n)\n\n   user  system elapsed \n 92.233  41.167 148.677 \n\n\nHa tardado como un minuto, pero lo tengo.\n\nhead(df_all)\n\n# A tibble: 6 × 2\n  day        count\n  &lt;date&gt;     &lt;int&gt;\n1 2021-04-05  6548\n2 2021-04-06  5664\n3 2021-04-07  5343\n4 2021-04-08  6054\n5 2021-04-09  6069\n6 2021-04-10  5157\n\n\nNo he cargado en ningún momento todos los datos: he cargado solo lo que necesitaba de cada fichero y he operado con cada fichero por separado.\n\ncol_select te permite no leer todas las columnas sino solo las que incluyas en la selección. Los ficheros son grandes, así que ahorras tiempo y memoria si eliges previamente.\nas_data_frame = TRUE está diciendo a la función que cargue todo el data frame en memoria (o por lo menos la selección que hemos hecho). Si lo marcas como FALSE el resultado de la lectura será una tabla de arrow, y el cómputo que viene después no lo hará dplyr sino arrow, aunque luego tendrás que recuperar los datos con un collect().\n\n\nAlternativa en el bucle. Sin selección de columnas\nSiempre que sepas que te sobran columnas, elimínalas. Para que te hagas una idea, el siguiente código solo se diferencia del anterior en la selección de columnas… pues este no me tira. Se peta todo.\n\nsystem.time(\n  df_all &lt;- purrr::map_dfr(paths_a_ficheros, function(.path) {\n    df &lt;- read_parquet(\n      .path,\n      as_data_frame = TRUE\n    )\n    \n    df &lt;- df |&gt; \n      filter(airport_fee &gt; 0) |&gt; \n      mutate(day = as.Date(pickup_datetime)) |&gt;\n      summarise(count = n(), .by = day)\n    \n    # gc()\n    \n    return(df)\n  }) \n)\n\n\n\nAlternativa en el bucle. Tabla arrow\nUna opción es fijar as_data_frame a FALSE para trabajar con las tablas de arrow en lugar de data frame. Esto te será útil si cada fichero es a su vez muy grande. Pero tendrás de todos modos que usar collect() en cada iteración para que map_dfr pueda apilar los data frames. Esto hará que cada data frame (resultante de cada iteración) se vaya volcando en memoria.\n\nsystem.time(\n  df_all &lt;- purrr::map_dfr(paths_a_ficheros, function(.path) {\n    df &lt;- read_parquet(\n      .path,\n      col_select = c(\"pickup_datetime\", \"airport_fee\"),\n      as_data_frame = FALSE\n    )\n    \n    df &lt;- df |&gt; \n      filter(airport_fee &gt; 0) |&gt; \n      mutate(day = as.Date(pickup_datetime)) |&gt;\n      summarise(count = n(), .by = day)\n    \n    # gc()\n    \n    return(collect(df))\n  }) \n)\n\n   user  system elapsed \n 70.561  16.871  82.183"
  },
  {
    "objectID": "posts/2024-04-21-dataframes-inmemory/index.html#método-2.-arrow",
    "href": "posts/2024-04-21-dataframes-inmemory/index.html#método-2.-arrow",
    "title": "Cómo procesas datos que no te caben en RAM",
    "section": "Método 2. arrow",
    "text": "Método 2. arrow\nEn lugar del bucle, si te vas a meter en arrow, le sacarás más partido si aprovechas todas sus herramientas.\nSi tienes unos ficheros parquet en una carpeta, en lugar de leerlos uno a uno desde una mentalidad R, puedes dejar que arrow se encargue de la gestión en memoria con particiones de todos los ficheros en conjunto.\n\nHay formas más recomendables de guardar los ficheros parquet para facilitar esta tarea a arrow, pero los dejo como están para este ejemplo porque me sirven así.\n\nCon arrow::open_dataset() cargo la información de los ficheros. Por ejemplo, debajo puedes ver los nombres y tipos de columnas de los ficheros:\n\nopen_dataset(path_to_folder)\n\nFileSystemDataset with 38 Parquet files\nhvfhs_license_num: string\ndispatching_base_num: string\noriginating_base_num: string\nrequest_datetime: timestamp[us]\non_scene_datetime: timestamp[us]\npickup_datetime: timestamp[us]\ndropoff_datetime: timestamp[us]\nPULocationID: int64\nDOLocationID: int64\ntrip_miles: double\ntrip_time: int64\nbase_passenger_fare: double\ntolls: double\nbcf: double\nsales_tax: double\ncongestion_surcharge: double\nairport_fee: double\ntips: double\ndriver_pay: double\nshared_request_flag: string\nshared_match_flag: string\naccess_a_ride_flag: string\nwav_request_flag: string\nwav_match_flag: string\n\nSee $metadata for additional Schema metadata\n\n\nY puedes trabajar ese resultado como si fuera un data frame de dplyr.\n\nopen_dataset(path_to_folder) |&gt; \n  select(airport_fee, pickup_datetime) |&gt; \n  filter(airport_fee &gt; 0) |&gt; \n  mutate(day = as.Date(pickup_datetime)) |&gt;\n  summarise(count = n(), .by = day)\n\nFileSystemDataset (query)\nday: date32[day]\ncount: int64\n\nSee $.data for the source Arrow object\n\n\nCon ese código no has ejecutado ninguna operación, por eso no tarda nada en ejercutarse. Lo que has hecho es preparar una operación con una sintaxis. Ahora falta materializarla, decirle a R que mande la orden y arrow la ejecutará. Para eso, necesitas collect(), pero la sintaxis de dplyr no cambia.\n\nsystem.time(\n  df_all &lt;- open_dataset(path_to_folder) |&gt; \n    select(airport_fee, pickup_datetime) |&gt; \n    filter(airport_fee &gt; 0) |&gt; \n    mutate(day = as.Date(pickup_datetime)) |&gt;\n    summarise(count = n(), .by = day) |&gt; \n    collect()\n)\n\n   user  system elapsed \n 64.199  13.922  38.288 \n\n\narrow se ha encargado de paralelizar la operación aprovechando los recursos y por eso tarda tan poco.\nEn tu caso no sé, pero si hubiera intentando yo pararelizar la operación con `mcLapply o cosas así seguramente habría petado el ordenador… porque habría intentado cargar en memoria más de lo que el ordenador puede.\n\nhead(df_all)\n\n# A tibble: 6 × 2\n  day        count\n  &lt;date&gt;     &lt;int&gt;\n1 2021-06-01 26302\n2 2021-06-02 23917\n3 2021-06-03 26256\n4 2021-06-04 27644\n5 2021-06-05 22020\n6 2021-06-06 28681"
  },
  {
    "objectID": "posts/2024-04-21-dataframes-inmemory/index.html#método-3.-duckdb",
    "href": "posts/2024-04-21-dataframes-inmemory/index.html#método-3.-duckdb",
    "title": "Cómo procesas datos que no te caben en RAM",
    "section": "Método 3. duckdb",
    "text": "Método 3. duckdb\nOtra herramienta en la que te puedes apoyar sin que tu sintaxis tidyverse se vea afectada en DuckDB, en colaboración con dbplyr, el backend de dplyr para bases de datos.\n\nlibrary(duckdb)\nlibrary(dplyr)\n# hive_partitioning = true\ncon &lt;- dbConnect(duckdb())\nsystem.time(\n  df_all &lt;- tbl(con, \"read_parquet('../../data/taxiny/*.parquet')\") |&gt; \n    select(airport_fee, pickup_datetime) |&gt; \n    filter(airport_fee &gt; 0) |&gt; \n    mutate(day = as.Date(pickup_datetime)) |&gt;\n    summarise(count = n(), .by = day) |&gt; \n    collect()\n)\n\n   user  system elapsed \n 57.329   1.487  20.903 \n\n\nSe ha portado bien. En benchmarks más serios, con datos medianos funciona muy bien.\nDuckDB además podrías trabajarla con SQL, lo que puede facilitar el trabajo en equipos grandes, en los que pocos miembros sabrán dplyr."
  },
  {
    "objectID": "posts/2024-04-21-dataframes-inmemory/index.html#comentarios-finales",
    "href": "posts/2024-04-21-dataframes-inmemory/index.html#comentarios-finales",
    "title": "Cómo procesas datos que no te caben en RAM",
    "section": "Comentarios finales",
    "text": "Comentarios finales\n\nSi gestionas bien las particiones de ficheros por variables, obtendrás incluso mejores resultados.\nRecientemente ha salido duckplyr, para que el propio dplyr te contruya la infraestructura duckdb directamente. Pero según entiendo esto lo hace con data frames que ya tienes cargados en memoria. Recuerda que en este caso no tienes suficiente memoria.\nSi te cabe en memoria, el clásico data.table de R sigue estando vigente, aunque muchos ojos se estén yendo a polars (benchmark)"
  },
  {
    "objectID": "posts/2024-08-09-presupuestos-familiares/index.html",
    "href": "posts/2024-08-09-presupuestos-familiares/index.html",
    "title": "Encuesta de presupuestos familiares",
    "section": "",
    "text": "El INE tiene datos de la Encuesta de Presupuestos Familiares. He descargado un fichero con cantidades consumidas por hogar de algunos productos.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\nggplot2::theme_set(ggplot2::theme_light())\n\n\ndf_epf &lt;- read_csv2(\n  here::here(\"data/25168.csv\"),\n  locale = locale(encoding = \"latin1\", grouping_mark = \".\", decimal_mark = \",\"),\n  show_col_types = FALSE\n)\n\ndf_epf &lt;- janitor::clean_names(df_epf)\n\ndf_epf &lt;- df_epf |&gt;\n  mutate(\n    total = parse_number(\n      total,\n       locale = locale(grouping_mark = \".\", decimal_mark = \",\"), na = c(\"..\")\n    ),\n    periodo = as.integer(periodo) \n  )\n\nQuito columnas, cambio nombres, etc.\n\ndf_epf &lt;- df_epf |&gt;\n  select(-cantidades_medias_consumidas) |&gt; \n  rename(\n    producto = codigos_de_gasto_con_cantidad_fisica\n  )\n\nLos nombres de productos tienen un código al principio que me va a molestar casi siempre pero pueden llegar a servirme para hacer algún filtro de categorías.\nAsí que hago algo horrible: creo una función a la que llamaré todo el rato para limpiar los nombres de productos (en lugar de hacerlo definitivo al principio).\n\nlimpia_nombre &lt;- function(data) {\n  data |&gt;\n    mutate(\n      producto = str_remove(\n        producto, \n        \"[0-9]{2}\\\\.[0-9]\\\\.[0-9]\\\\.[0-9]\\\\sT\\\\s\"\n      )\n    )\n}"
  },
  {
    "objectID": "posts/2024-08-09-presupuestos-familiares/index.html#datos",
    "href": "posts/2024-08-09-presupuestos-familiares/index.html#datos",
    "title": "Encuesta de presupuestos familiares",
    "section": "",
    "text": "El INE tiene datos de la Encuesta de Presupuestos Familiares. He descargado un fichero con cantidades consumidas por hogar de algunos productos.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\nggplot2::theme_set(ggplot2::theme_light())\n\n\ndf_epf &lt;- read_csv2(\n  here::here(\"data/25168.csv\"),\n  locale = locale(encoding = \"latin1\", grouping_mark = \".\", decimal_mark = \",\"),\n  show_col_types = FALSE\n)\n\ndf_epf &lt;- janitor::clean_names(df_epf)\n\ndf_epf &lt;- df_epf |&gt;\n  mutate(\n    total = parse_number(\n      total,\n       locale = locale(grouping_mark = \".\", decimal_mark = \",\"), na = c(\"..\")\n    ),\n    periodo = as.integer(periodo) \n  )\n\nQuito columnas, cambio nombres, etc.\n\ndf_epf &lt;- df_epf |&gt;\n  select(-cantidades_medias_consumidas) |&gt; \n  rename(\n    producto = codigos_de_gasto_con_cantidad_fisica\n  )\n\nLos nombres de productos tienen un código al principio que me va a molestar casi siempre pero pueden llegar a servirme para hacer algún filtro de categorías.\nAsí que hago algo horrible: creo una función a la que llamaré todo el rato para limpiar los nombres de productos (en lugar de hacerlo definitivo al principio).\n\nlimpia_nombre &lt;- function(data) {\n  data |&gt;\n    mutate(\n      producto = str_remove(\n        producto, \n        \"[0-9]{2}\\\\.[0-9]\\\\.[0-9]\\\\.[0-9]\\\\sT\\\\s\"\n      )\n    )\n}"
  },
  {
    "objectID": "posts/2024-08-09-presupuestos-familiares/index.html#carnes",
    "href": "posts/2024-08-09-presupuestos-familiares/index.html#carnes",
    "title": "Encuesta de presupuestos familiares",
    "section": "Carnes",
    "text": "Carnes\n\ndibuja_evolucion &lt;- function(data, titulo, .y = \"Cantidad\") {\n  ggplot(data, aes(x = periodo, y = total, col = producto)) + \n  geom_line(linewidth = 1) +\n  geom_smooth(method = \"loess\") +\n  labs(\n    title = sprintf(\"Cantidad media consumida por hogas (%s)\", titulo), \n    x = \"\", y = .y\n  ) + \n  theme(\n    legend.position = \"bottom\", \n    legend.title = element_blank()\n  )\n}\n\n\ndf_epf |&gt;\n  limpia_nombre() |&gt; \n  filter(str_detect(producto, \"^Carne \")) |&gt; \n  dibuja_evolucion(titulo = \"Carnes\")"
  },
  {
    "objectID": "posts/2024-08-09-presupuestos-familiares/index.html#bebidas",
    "href": "posts/2024-08-09-presupuestos-familiares/index.html#bebidas",
    "title": "Encuesta de presupuestos familiares",
    "section": "Bebidas",
    "text": "Bebidas\n\nlistado_bebidas &lt;- c(\"Agua mineral\", \"Bebidas refrescantes\", \"Bebidas energéticas\", \"Bebidas isotónicas\", \"Zumos de frutas\", \"Vinos de uva\", \"Cerveza\", \"Cerveza baja en alcohol\")\n\ndf_epf |&gt;\n  limpia_nombre() |&gt; \n  filter(str_detect(producto, paste0(listado_bebidas, collapse = \"|\"))) |&gt; \n  dibuja_evolucion(titulo = \"Bebidas\")"
  },
  {
    "objectID": "posts/2023-07-21-tidytuesday-oldest-people/index.html",
    "href": "posts/2023-07-21-tidytuesday-oldest-people/index.html",
    "title": "Personas centenarias [tidytuesday]",
    "section": "",
    "text": "Exploración de datos del tidytuesday del 30 de mayo de 2023.\nSitúo por año de nacimiento a los centenarios. Habrá en los años más recientes, seguramente porque la población era mayor (en un periodo tan breve, no creo que los avances médicos supusieran tanta diferencia)."
  },
  {
    "objectID": "posts/2023-07-21-tidytuesday-oldest-people/index.html#cómo-varía-en-función-de-las-variables-que-hay",
    "href": "posts/2023-07-21-tidytuesday-oldest-people/index.html#cómo-varía-en-función-de-las-variables-que-hay",
    "title": "Personas centenarias [tidytuesday]",
    "section": "¿Cómo varía en función de las variables que hay?",
    "text": "¿Cómo varía en función de las variables que hay?\nNo tengo a mano los datos de población de los países pero me vale esto para hacerme una idea.\n\n\n\n\n\n\n\n\n\nNo hay diferencia entre chico/chica:\n\n\n\n\n\nGénero\nPersonas\n\n\n\n\nfemale\n100\n\n\nmale\n100"
  },
  {
    "objectID": "posts/2022-08-07-nate-silver-describe-el-comienzo-de-una-pandemia-que-no-fue/index.html",
    "href": "posts/2022-08-07-nate-silver-describe-el-comienzo-de-una-pandemia-que-no-fue/index.html",
    "title": "Nate Silver describe el comienzo de una pandemia que no fue",
    "section": "",
    "text": "En Fort Dix en enero de 1976 un soldado murió de una neumonía causada por la gripe porcina, la causante de la pandemia que asoló el mundo en 1918. En los EEUU esta gripe también se manifestó inicialmente en un fuerte y acabó con más de 600000 estadounidenses. El Secretario de Salud de Gerald Ford dio un millón como estimación de muertos si este nuevo brote se manifestaba tan letal como el del 18. Sabían que los equipos de investigación de vacunas necesitaban del orden de 6 meses para enfrentarse a nuevas cepas, pero el gobierno fue adelante con un plan de 200 millones de vacunaciones, el programa más grande desde la polio en el 50. El plan se aprobó con enorme mayoría en el Congreso; costaría 180 millones de dólares.\nDurante el verano aumentaron las dudas con el plan del gobierno. Si bien el verano es una época con pocos casos de gripes, en el del planeta era invierno y no se estaban reportando casos, sino de la variante A/Victoria, que era la habitual ese año. Tan solo había como casos confirmados los escasos 200 del fuerte Dix. Empezaron a llover las críticas contra el gobierno desde toda la prensa y ningún otro país había tomado medidas tan drásticas. En lugar de aceptar que habían sobrevalorado la amenaza, el gobierno de Ford apostó por una campaña de miedo en televisión.\nEn agosto, el Congreso y la Casa Blanca accedieron a idemnizar a las farmas en el caso de déficits por la manufacturación, lo que se interpretó como una falta de confianza. El programa comenzó en octubre, con menos intención de vacunación que la deseada. Al comienzo se dieron casos de fallecimientos entre gente mayor: no se le dio importancia porque, al fin y al cabo, eran mayores y había muchas causas posibles de la muerte. Pero empezaron a darse casos de Guillain-Barré, un síndrome causado por un desorden autoinmune que provoca parálisis. Suele afectar a uno por millón: entre los vacunados, la incidencia era 10 veces mayor. El programa de vacunación se canceló en diciembre. El brote de Fort Dix se quedó en eso: un brote que no salió de allí.\n(La señal y el ruido, Nate Silver)"
  },
  {
    "objectID": "posts/2024-08-05-mercamadrid/index.html",
    "href": "posts/2024-08-05-mercamadrid/index.html",
    "title": "Mercamadrid",
    "section": "",
    "text": "library(readr)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\nggplot2::theme_set(ggplot2::theme_light())\n\n\nlink_base &lt;- \"https://datos.madrid.es/egob/catalogo/300357-%s-mercamadrid-volumen-precio.csv\"\nlinks &lt;- sprintf(link_base, seq(0, 10, 2))\n\ndf_merca &lt;- map_dfr(links, function(link) {\n  if (RCurl::url.exists(link)) {\n    datos &lt;- read_csv2(\n      link,\n      locale = locale(encoding = \"latin1\"),\n      show_col_types = FALSE\n    )\n\n    datos &lt;- janitor::clean_names(datos)\n\n    if (is.character(datos$fecha_desde)) {\n      datos &lt;- datos |&gt;\n        mutate(across(\n          contains(\"precio\"),\n          \\(x) parse_number(x, na = c(\"Precio Más Frecuente\", \"Precio Máximo\", \"Precio Mínimo\"))\n        ))\n    }\n\n    if (is.character(datos$kilos)) {\n      datos &lt;- datos |&gt;\n        mutate(\n          kilos = parse_number(kilos, na = \"Kilos\")\n        )\n    }\n\n    datos &lt;- datos |&gt;\n      mutate(\n        across(\n          contains(\"fecha\"),\n          \\(x) as.Date(as.character(x), format = \"%Y%m%d\")\n        ), \n        origen = as.character(origen)\n      )\n\n    return(datos)\n  }\n})\n\ndf_merca &lt;- select(df_merca, -origen)\n\ndf_merca |&gt; \n    distinct(descripcion_variedad)\n\n# A tibble: 754 × 1\n   descripcion_variedad \n   &lt;chr&gt;                \n 1 VACUNO CANAL         \n 2 VACUNO DESPIECE AÑOJO\n 3 VACUNO DESPIECE VACA \n 4 VACUNO CASQUERIA     \n 5 VACUNO ELABORADO     \n 6 VACUNO PRECOCINADO   \n 7 OVINO CANAL          \n 8 OVINO DESPIECE       \n 9 OVINO CASQUERIA      \n10 OVINO ELABORADO      \n# ℹ 744 more rows"
  },
  {
    "objectID": "posts/2024-08-05-mercamadrid/index.html#lectura-de-datos",
    "href": "posts/2024-08-05-mercamadrid/index.html#lectura-de-datos",
    "title": "Mercamadrid",
    "section": "",
    "text": "library(readr)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\nggplot2::theme_set(ggplot2::theme_light())\n\n\nlink_base &lt;- \"https://datos.madrid.es/egob/catalogo/300357-%s-mercamadrid-volumen-precio.csv\"\nlinks &lt;- sprintf(link_base, seq(0, 10, 2))\n\ndf_merca &lt;- map_dfr(links, function(link) {\n  if (RCurl::url.exists(link)) {\n    datos &lt;- read_csv2(\n      link,\n      locale = locale(encoding = \"latin1\"),\n      show_col_types = FALSE\n    )\n\n    datos &lt;- janitor::clean_names(datos)\n\n    if (is.character(datos$fecha_desde)) {\n      datos &lt;- datos |&gt;\n        mutate(across(\n          contains(\"precio\"),\n          \\(x) parse_number(x, na = c(\"Precio Más Frecuente\", \"Precio Máximo\", \"Precio Mínimo\"))\n        ))\n    }\n\n    if (is.character(datos$kilos)) {\n      datos &lt;- datos |&gt;\n        mutate(\n          kilos = parse_number(kilos, na = \"Kilos\")\n        )\n    }\n\n    datos &lt;- datos |&gt;\n      mutate(\n        across(\n          contains(\"fecha\"),\n          \\(x) as.Date(as.character(x), format = \"%Y%m%d\")\n        ), \n        origen = as.character(origen)\n      )\n\n    return(datos)\n  }\n})\n\ndf_merca &lt;- select(df_merca, -origen)\n\ndf_merca |&gt; \n    distinct(descripcion_variedad)\n\n# A tibble: 754 × 1\n   descripcion_variedad \n   &lt;chr&gt;                \n 1 VACUNO CANAL         \n 2 VACUNO DESPIECE AÑOJO\n 3 VACUNO DESPIECE VACA \n 4 VACUNO CASQUERIA     \n 5 VACUNO ELABORADO     \n 6 VACUNO PRECOCINADO   \n 7 OVINO CANAL          \n 8 OVINO DESPIECE       \n 9 OVINO CASQUERIA      \n10 OVINO ELABORADO      \n# ℹ 744 more rows"
  },
  {
    "objectID": "posts/2024-08-05-mercamadrid/index.html#vacuno",
    "href": "posts/2024-08-05-mercamadrid/index.html#vacuno",
    "title": "Mercamadrid",
    "section": "Vacuno",
    "text": "Vacuno\nHay mil cosas. Voy a ver qué puedo sacar solo del vacuno por ahora.\n\ndf_vacuno &lt;- df_merca |&gt;\n  filter(str_detect(descripcion_variedad, \"^VACUNO\"))\n\n\nQué variedad de vacuno se vende más\n\ndf_vacuno |&gt; \n  group_by(fecha_desde, descripcion_variedad) |&gt; \n  summarise(kilos = sum(kilos, na.rm = TRUE), .groups = \"drop\") |&gt; \n  ggplot() + \n  geom_line(aes(x = fecha_desde, y = kilos, col = descripcion_variedad))\n\n\n\n\n\n\n\n\n¿Qué será VACUNO FRESCO?\n\ndf_vacuno_variedades &lt;- df_vacuno |&gt;\n  group_by(fecha_desde, descripcion_variedad) |&gt;\n  summarise(kilos = sum(kilos, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(descripcion_variedad) |&gt;\n  summarise(kilos_mes = sum(kilos) / n())\n\nggplot(df_vacuno_variedades) +\n  geom_col(\n    aes(x = reorder(descripcion_variedad, kilos_mes), y = kilos_mes),\n    fill = \"#800080\"\n  ) +\n  labs(x = \"\", y = \"Kilos por mes\") +\n  coord_flip() \n\n\n\n\n\n\n\n\n\nvariedades_principales &lt;- df_vacuno_variedades |&gt;\n  slice_max(order_by = kilos_mes, n = 4) |&gt;\n  pull(descripcion_variedad)\n\ndf_vacuno |&gt;\n  filter(descripcion_variedad %in% variedades_principales) |&gt;\n  group_by(fecha_desde, descripcion_variedad) |&gt;\n  summarise(kilos = sum(kilos, na.rm = TRUE), .groups = \"drop\") |&gt;\n  ggplot() +\n  geom_line(aes(x = fecha_desde, y = kilos, col = descripcion_variedad))\n\n\n\n\n\n\n\n\n\n\n¿De dónde ha venido el vacuno?\n\ndf_variedades_origen &lt;- df_vacuno |&gt;\n  filter(descripcion_variedad %in% variedades_principales) |&gt;\n  group_by(descripcion_variedad, descripcion_origen) |&gt;\n  summarise(kilos_mes = sum(kilos) / n(), .groups = \"drop\") |&gt;\n  group_by(descripcion_variedad) |&gt;\n  slice_max(order_by = kilos_mes, n = 5) |&gt;\n  ungroup()\n\ndf_vacuno |&gt;\n  semi_join(\n    df_variedades_origen,\n    by = c(\"descripcion_variedad\", \"descripcion_origen\")\n  ) |&gt; \n  ggplot() + \n  geom_line(aes(x = fecha_desde, y = kilos, col = descripcion_origen)) + \n  facet_wrap(~descripcion_variedad, scales = \"free_y\")"
  },
  {
    "objectID": "posts/2025-04-12-semilla/index.html",
    "href": "posts/2025-04-12-semilla/index.html",
    "title": "Cuánto te afecta la semilla al resultado final",
    "section": "",
    "text": "En Cuartil mencionamos que cambiar la semilla puede cambiar la métrica de ajuste de tu modelo.\nVoy a ajustar un RandomForest en algún conjunto de datos varias veces.\nEl objetivo es ver que, cada vez que lo ejecuto, la predicción cambia, por lo que hay cierta incertidumbre cada vez que ajustas un modelo de estos.\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2025-04-12-semilla/index.html#comentarios-iniciales",
    "href": "posts/2025-04-12-semilla/index.html#comentarios-iniciales",
    "title": "Cuánto te afecta la semilla al resultado final",
    "section": "",
    "text": "En Cuartil mencionamos que cambiar la semilla puede cambiar la métrica de ajuste de tu modelo.\nVoy a ajustar un RandomForest en algún conjunto de datos varias veces.\nEl objetivo es ver que, cada vez que lo ejecuto, la predicción cambia, por lo que hay cierta incertidumbre cada vez que ajustas un modelo de estos.\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2025-04-12-semilla/index.html#preparación-de-datos",
    "href": "posts/2025-04-12-semilla/index.html#preparación-de-datos",
    "title": "Cuánto te afecta la semilla al resultado final",
    "section": "Preparación de datos",
    "text": "Preparación de datos\nLos datos no me importan especialmente. Cargo unos de sklearn que me ha sugerido ChatGPT. Los separo en train y test. Esta separación la dejo fija: voy a estudiar cómo afecta la aleatoriedad del modelo, no la de los datos.\n\ndata = load_diabetes()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)"
  },
  {
    "objectID": "posts/2025-04-12-semilla/index.html#ajuste-de-los-modelos",
    "href": "posts/2025-04-12-semilla/index.html#ajuste-de-los-modelos",
    "title": "Cuánto te afecta la semilla al resultado final",
    "section": "Ajuste de los modelos",
    "text": "Ajuste de los modelos\nAjusto 100 modelos. Voy a guardar algunas cosas de los ajustes para luego ver cómo varían de un caso a otro. Lo que quiero estudiar es si hay mucha dispersión entre unos resultados y otros.\n\nnum_trials = 200\nl_r2 = []\nl_preds = []\n\nfor i in range(num_trials):\n    random_state = np.random.randint(0, 10000)\n\n    model = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    l_r2.append(r2_score(y_test, y_pred))\n\n    pred_series = pd.Series(y_pred, name=f\"pred_{i}\")\n\n    l_preds.append(pred_series)\n\nAhora creo el data frame de predicciones:\n\ndf_preds = pd.concat(l_preds, axis=1)"
  },
  {
    "objectID": "posts/2025-04-12-semilla/index.html#visualización",
    "href": "posts/2025-04-12-semilla/index.html#visualización",
    "title": "Cuánto te afecta la semilla al resultado final",
    "section": "Visualización",
    "text": "Visualización\n\nMétrica de ajuste\nAquí la distribución del R2.\n\n\n\n\n\n\n\n\n\n\n\nPredicciones\nAhora muestro cómo varía cada predicción observación a observación. Muestro solo unas pocas porque no se ve nada si intento ver todas una a una.\nQuizá no tenga sentido ver todas una a una, sino ver la desviación en general de todas las observación con respecto a su punto medio o su media o lo que sea. Pero paso de pensar.\n\ndf_long = df_preds.reset_index().melt(\n    id_vars=\"index\", var_name=\"modelo\", value_name=\"pred\"\n)\ndf_long = df_long.rename(columns={\"index\": \"id\"})\n\nsubset_ids = df_long[\"id\"].unique()[:50]\nsubset = df_long[df_long[\"id\"].isin(subset_ids)]\n\nmedianas = subset.groupby(\"id\")[\"pred\"].median().sort_values()\n\nsubset[\"id\"] = pd.Categorical(subset[\"id\"], categories=medianas.index, ordered=True)\n\nplt.figure(figsize=(8, 15))\nsns.boxplot(data=subset, y=\"id\", x=\"pred\", color=\"#800080\", showfliers=False)\nplt.ylabel(\"ID de observación (subconjunto)\")\nplt.xlabel(\"Predicción\")\nplt.title(\"Boxplot de predicciones por observación (unas pocas)\")\nplt.xticks(rotation=90, fontsize=5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHay alguna observación que sí presenta algo de variabilidad entre los modelos pero no me escandizaría por ello."
  },
  {
    "objectID": "posts/2022-08-06-mi-ipc-es-incluso-m-s-alto/index.html",
    "href": "posts/2022-08-06-mi-ipc-es-incluso-m-s-alto/index.html",
    "title": "Mi IPC es (incluso) más alto",
    "section": "",
    "text": "No sé tú pero yo no consumo de todo. Si así fuera, entonces el IPC asociado a mis gastos sí sería el 10% del que hablan.\nPero a mí me afectan más unos grupos de consumo que otros.\nAfortunadamente, no gasto en transporte.\nPobrecitos los que sí.\n\n\n\n\n\n\n\n\n\nP.D. Hacer este gráfico con Python ha sido un suplicio :("
  },
  {
    "objectID": "posts/2024-08-04-viajeros/index.html",
    "href": "posts/2024-08-04-viajeros/index.html",
    "title": "Pernotaciones por Comunidades Autónomas",
    "section": "",
    "text": "El INE tiene unos datos de pernoctaciones a distintos niveles, mes a mes. Me interesan a nivel de Comunidad Autónoma.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nggplot2::theme_set(ggplot2::theme_light())\n\ndf_pernoctaciones &lt;- read_csv2(\"2941 (1).csv\", locale = locale(encoding = \"latin1\"))\n\ndf_pernoctaciones &lt;- janitor::clean_names(df_pernoctaciones)\n\nEl formato de los datos nunca entenderé quién lo ha decidido. Imagino que alguien que no tenía que trabajar con ellos. Los proceso un poco.\n\ndf_series &lt;- df_pernoctaciones |&gt;\n  # Quito NAs porque son el total\n  filter(!is.na(comunidades_y_ciudades_autonomas)) |&gt; \n  mutate(\n    periodo = paste0(periodo, \"D01\"), \n    periodo = as.Date(periodo, format = \"%YM%mD%d\"), \n    total = parse_number(\n      total, \n      locale = locale(grouping_mark = \".\"),\n      na = c(\".\", \"..\")\n    )\n  ) |&gt; \n    group_by(comunidades_y_ciudades_autonomas, periodo) |&gt; \n    summarise(pernoctaciones = sum(total, na.rm = TRUE), .groups = \"drop\")\n\nY aquí, la evolución de pernoctaciones mes a mes de cada Comunidad Autónoma:\n\nformato &lt;- scales::label_number(big.mark = \".\", scale = 0.001, suffix = \"k\")\nggplot(df_series, aes(x = periodo, y = pernoctaciones)) + \n  geom_line(col = \"#800080\") + \n  geom_smooth(method = \"loess\") + \n  scale_y_continuous(labels = formato) + \n  facet_wrap(\n    ~comunidades_y_ciudades_autonomas, \n    scales = \"free_y\", \n    ncol = 3\n  ) + \n  labs(\n    title = \"Pernoctaciones mensuales por CCAA\", \n    caption = \"Fuente: INE\", \n    y = \"Pernoctaciones (miles)\", \n    x = \"\"\n  )"
  },
  {
    "objectID": "posts/2024-03-18-correlation/index.html",
    "href": "posts/2024-03-18-correlation/index.html",
    "title": "Relaciones espurias",
    "section": "",
    "text": "Tienes un datos de divorcio en estados de Estados Unidos (fuente).\nParece que hay correlación entre el ratio de divorcio por estado y la edad mediana a la que se casa la gente:\n\nlibrary(ggplot2)\n\ntheme_set(theme_light())\n\nggplot(d, aes(x = MedianAgeMarriage, y = Divorce)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nEl ratio de divorcio por estado dada la mediana de edad de casarse podrías modelizarlo como:\n\\[\nD_i \\sim \\text{Normal}(\\mu_i, \\sigma)\n\\\\\n\\mu_i = \\alpha + \\beta_AA_i\n\\\\\n\\alpha \\sim \\text{Normal(0, 0,2)}\n\\\\\n\\beta_A \\sim \\text{Normal(0, 0,5)}\n\\\\\n\\sigma \\sim \\text{Exponential(1)}\n\\]\n\n\\(D_i\\) el ratio de divorcio estandarizado (media 0, desviación típica 1) para el Estado \\(i\\).\n\\(A_i\\) es la edad mediana a la que se casa la gente en el Estado \\(i\\).\nComo el ratio que modelizamos está centrado en 0, el intercept del modelo es esperable que sea cercano a 0.\nSobre \\(\\beta_A\\), si fuera igual a 1, estaríamos diciendo que por cada cambio de una desviación estándar de la edad, observaríamos un cambio de una desviación estándar en el ratio del divorcio. La desviación estándar de la edad es 1.2436303, por lo que si \\(\\beta_A = 1\\), un cambio de \\({1,2}\\) años en la mediana de edad de casarse aumentaría el ratio de divorcio en una desviación estándar, lo que parece ser demasiado fuerte.\n\nAhora podemos ajustar el modelo y simular:\n\nm5.1 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma), \n    mu &lt;- a + bA * A, \n    a ~ dnorm(0, 0.2), \n    bA ~ dnorm(0, 0.5), \n    sigma ~ dexp(1)\n  ), \n  data = d\n)\n\n\nset.seed(10)\nprior &lt;- extract.prior(m5.1)\nmu &lt;- link( m5.1 , post=prior , data=list( A=c(-2,2) ) )\nplot( NULL , xlim=c(-2,2) , ylim=c(-2,2) )\nfor ( i in 1:50 ) lines( c(-2,2) , mu[i,] , col=col.alpha(\"black\",0.4) )"
  },
  {
    "objectID": "posts/2023-12-05-adventofcode-ii/index.html",
    "href": "posts/2023-12-05-adventofcode-ii/index.html",
    "title": "Advent of code (2)",
    "section": "",
    "text": "Segundo ejercicio de Advent of Code de 2023. Voy con retraso, lo sé."
  },
  {
    "objectID": "posts/2023-12-05-adventofcode-ii/index.html#parte-1",
    "href": "posts/2023-12-05-adventofcode-ii/index.html#parte-1",
    "title": "Advent of code (2)",
    "section": "Parte 1",
    "text": "Parte 1\nMi propuesta con R base:\n\ntotal_cubes &lt;- list(red = 12, green = 13, blue = 14)\ndata &lt;- readLines(\"2023-02.txt\")\n\ndata[1:5]\n\n[1] \"Game 1: 12 red, 2 green, 5 blue; 9 red, 6 green, 4 blue; 10 red, 2 green, 5 blue; 8 blue, 9 red\"\n[2] \"Game 2: 3 green, 7 red; 3 blue, 5 red; 2 green, 1 blue, 6 red; 3 green, 2 red, 3 blue\"          \n[3] \"Game 3: 12 red, 18 blue, 3 green; 14 red, 4 blue, 2 green; 4 green, 15 red\"                     \n[4] \"Game 4: 14 blue, 8 red, 10 green; 7 green, 9 blue, 4 red; 4 green, 5 red\"                       \n[5] \"Game 5: 2 red, 1 blue, 4 green; 1 blue, 1 red, 5 green; 6 green, 3 red; 10 blue, 4 green, 1 red\"\n\n\nPara cada juego (o sea, cada elemento en data) tengo que comprobar si las extracciones son posibles dada la cantidad de cubos original mencionada: 12 rojos, 13 verdes y 14 azules.\n\nvalidate_game &lt;- function(fila, .total_cubes = total_cubes){\n  fila &lt;- gsub(\"Game [0-9]+: \", \"\", fila)\n  fila &lt;- gsub(\" \", \"\", fila)\n  \n  is_valid_extract &lt;- vapply(\n    strsplit(fila, \";\")[[1]], \n    validate_extract, \n    logical(1)\n  )\n \n  all(is_valid_extract)\n\n}\n\nvalidate_extract &lt;- function(x, .total_cubes = total_cubes) {\n  lgl_out &lt;- vapply(\n    c(\"blue\", \"green\", \"red\"),\n    function(color, text = x) {\n      # Extract with regex the number preceding word blue\n      pattern &lt;- sprintf(\"[0-9]+(?=%s)\", color)\n      count_txt &lt;- regmatches(text, m = regexpr(pattern, text, perl = TRUE))\n      if (length(count_txt) == 0) {\n        return(TRUE)\n      } else  {\n        count &lt;- as.numeric(count_txt)\n        return(count &lt;= total_cubes[[color]]  )\n      }\n      \n    }, \n    logical(1)\n  )\n  \n  all(lgl_out)\n}\n\nvalid_games &lt;- vapply(\n  data, \n  validate_game, \n  logical(1)\n)\n\n# Count IDs of valid games\nsum(which(valid_games))\n\n[1] 2600"
  },
  {
    "objectID": "posts/2023-12-05-adventofcode-ii/index.html#parte-2",
    "href": "posts/2023-12-05-adventofcode-ii/index.html#parte-2",
    "title": "Advent of code (2)",
    "section": "Parte 2",
    "text": "Parte 2\nDebería poner los enunciados aquí… pero paso.\nMi propuesta para la parte 2, siempre con R base:\n\ncompute_power_game &lt;- function(fila){\n  fila &lt;- gsub(\"Game [0-9]+: \", \"\", fila)\n  fila &lt;- gsub(\" \", \"\", fila)\n  \n  color_counts &lt;- vapply(\n    c(\"blue\", \"green\", \"red\"),\n    function(color) {\n      pattern &lt;- sprintf(\"[0-9]+(?=%s)\", color)\n      color_count &lt;- regmatches(fila, m = gregexpr(pattern, fila, perl = TRUE))[[1]]\n      color_count &lt;- as.numeric(color_count)\n      max(color_count)    \n    }, \n    numeric(1)\n  )\n  \n  Reduce(`*`, color_counts)\n\n}\n\ngame_powers &lt;- vapply(\n  data, \n  compute_power_game, \n  numeric(1)\n)\n\n# part 2 solution\nsum(game_powers)\n\n[1] 86036"
  },
  {
    "objectID": "posts/2022-08-09-los-problemas-de-la-moralidad/index.html",
    "href": "posts/2022-08-09-los-problemas-de-la-moralidad/index.html",
    "title": "Los peligros de la moralidad",
    "section": "",
    "text": "Twitter, como tribunal moral.\nEra de la hipermoralización. Despertar religioso sin Dios. Colapso del protestantismo ha dejado un vacío. Eric Hoffer: aunque la nuestra es una época sin Dios, es justo lo opuesto a no religiosa.\nlas creencias morales son muy distintas a otros tipos de creencias.\n\n\n1. Darwin y el origen de la moralidad\n\nLo moralmente atractivo en la naturaleza humana es producto de la selección natural que aumentó el éxito reproductivo de la especie.\nnihilismo moral.b Michael ruse y Richard Joyce. Nuestras creencias morales son ilusiones útiles para regular nuestra vida social.\nno hay colores, solo longitudes de onda. No hay moral, solo actos que favorecen el éxito reproductivo.\nintentos de Kant o Dostoïevski de dar lógica a la moral han fallado. Dilema de Platón: lo bueno es bueno porque Dios lo manda o Dios lo manda porque es bueno?\n\n\n\n2. Teorías evolucionistas de la moral\n\nEl Levítico tiene más sentido si pensamos que los antiguos clasificaban los antiguos en dos temas: me da asco o me da menos asco.\nHume: la razón es esclava de las pasiones y no puede hacer otra cosa que servirlas y obedecerlas.\nLos sentimientos Vienen primero y condicionan la manera de razonar las conclusiones (conclusiones a priori, razonamiento a posteriori).\nLa moralidad no trata de cómo tratamos a otros sino de unir grupos\nLos tres grandes de la moralidad: autonomía, comunidad y divinidad.\nUn sistema moral no puede dar prioridad a las tres patas a la vez.\nSegún la ética de comunidad, ls gente en los puestos de poder debería velar por subordinados y no actuar según sus propios intereses.\nHaidt: moralidad es Cualquier sistema de valores relacionados entre sí, Prácticas, instituciones y mecanismos psicológicos que trabajan de forma conjunta para regular y suprimir el egoísmo y hacer posible la vida social.\nExiste evidencia de que en sociedades de cazadores nómadas, los cazadores más fuertes tenían más hijos y más relaciones exteaconyugales.\nEl daño al servicio de la cooperación puede ser moralmente bueno. O sea, la moralidad puede implicar inmoralidad.\nTeoría de la diada: un agente moral y un paciente moral.\n\nCuando se juzga algo como inmoral dado un agente, se tiende a buscar un paciente para justificar ese juicio. Es instintivo e inevitable completar la pareja.\nCuando alguien sufre, se busca un agente (se ha recurrido a brujas, fuerzas sobrenaturales)\n\nEncasillamiento moral: se categoriza a una persona como agente o paciente, no ambas. De manera permanente.\n\nEstrategia habitual en abogacía es presentar como víctima a su cliente acusado.\n\n\n\n\n3. Conceptos básicos de la psicología moral y evolucionista\n\nGran parte de nuestras conductas morales son dirigidas a observadores.\nLas actitudes se enmarcan en preferencias personales (gustos, mar o montaña), convenciones (normas, conducir por la izquierda en UK) o imperativos morales (principios, el aborto está mal).\n\nLas creencias morales involucran a todos los demás.\n\nMandato moral: actitud con convicción moral\n\nUniversales\nObjetividad\nAutonomía\nEmociones\nMotivación y justificación\nIntolerancia\nInoculación contra la obediencia a autoridades\nBarrera para resolución de conflictos\nActivismo/violencia\nLa moralidad bate a la justicia\n\nlas personas son influenciables y tienden a seguir a mayorías (Asch, Cialdini)\nlas personas con convicciones morales tienden a votar más y a Implicarse en política porque ven como una obligación enfrentarse al problema moral que perciben.\nLas convicciones morales no se votan (conflicto democrático)\nla reciprocidad indirecta hace esencial al cotilleo.\nEn casos de reciprocidad indirecta conocida, la ayuda no es un acto altruista, sino una inversión en reputación.\nAdemás, no ayudar a alguien categorizado a todos como malo no es malo.\nTendemos a atribuir virtudes a personas en lugar de a actos.\nestímulo supernormal: versión exagerada de un estímulo. Puede incitar el ataque o la protección en algunas especies. En humanos, alimentos artirficiales zucarados, o exceso de chocolate, pornografía, implantes, programas de cotilleo, redes sociales\nmoralización : adquisición de cualidades morales por parte de objetos o acciones moralmente neutras (el tabaco se ha moralizado y la homosexualidad de está desmoralizando)\nla moralización es progresiva. Primero cubre a psrte grande la Población y luego entra en juego el gobierno e instituciones.\nLa medicina se está entrelazando cada vez más con valores sociales y sistemas morales (pareja enfermedad/pecado)\ndos tipos de agresión: proactiva y reactiva\n\nEs más probable que los proactivos reincidan, que sean psicópatasy es menos probable que respondan a fármacos.\nhay estudios en no humanos que apoyan que hay diferencia que ambos tipos tienen bases neuronales, hormonales y genéticas diferentes.\nparece que hemos evolucionado a una baja propensión a la reactiva y una alta a la proactiva.\n\nLa gente que hace cosas malas rara vez cree que está haciendo algo malo (Baumeister). Pueden incluso sentirse víctimas.\nhay víctimas que juegan un papel en su propio ataque\nla violencia y la crueldad tienen cuatro causas: ambición, avaricia, alta autoestima e idealismo moral.\nla religión no es un problema sino la fe (en dios, en el comunismo, en nacionalismos)\nes más fácil matar (incluso cara a cara) cuando la víctima ha sufrido la muerte social.\nla muerte social puede darse por tres cosas: pensamiento ellos/nosotros, deshumanización de las víctimas o culpar a las víctimas\nsi descategorizamos, pondremos frenos al mal.\nLo mekor y lo peor de nosotros proviene de ver a los demás como humanos.\n\n\n\n4. La división ellos/nosotros y el tribalismo moral\n\nhipótesis del altruismo parroquial. La guerra pudo haber extendido el altruismo entre la humanidad. La cooperación entre humanos surgió para derrotar a los humanos de otro grupos.\nsentimos menos empatía por individuos de grupos diferentes al nuestro\nla schadenfreude se asocia a circuitos de recompensa del sistema nervioso\nestar a favor une menos que estar en contra. Es mayor el placer por las pérdidas del rival que por ganancias propias.\nA partir de los 3 meses los niños ya muestran sesgos contrarios a personas de otro grupo étnico.\nLa aversión a la violencia disminuye cuando se realiza contra grupos políticos contrarios.\nTendemos a querer pertenecer a un grupo. Para eso, estamos dispuestos a aceptar cualquier creencia, por rara que sea.\nSi se llega con los rivales a un acuerdo, ese tema deja de tener interés moral. Por eso se moralizan temas nuevos.\nCuanto más apartadas estén las creencias de verdades neutrales, mayor será la sensación de coalición. Comunicar verdades neutrales no sirve como señal diferencial.\nun grupo identificado por una creencia no podrá cambiar su punto de vista sobre esa creencia pero sí sobre otras (si te define ser de un equipo de fútbol, te dará igual cambiar tu postura sobre el calentamiento global).\nEsto afecta especialmente a grupos en torno a cuestiones científicas (ya que nunca podrán falsarlas)\ndos tipos de creencias: funcionales y sociales. Es como los empleados en empresas, que algunos están enchufados. O la ropa, que alguna te la pones solo por encajar.\nLas creencias sociales se pueden adoptar para postureo, o adular a alguien, o sentirse superior.\nTenemos creencias irracionales porque tenemos recompensas sociales por esas creencias.\nEl alejamiento de la racionalidad no se debe a una irracionalidad en términos lógicos, sino a una búsqueda en función de nuestro interés social. Para combatir creencias, no se puede aplicar la lógica sino entender la función social de esas creencias.\nEl contenido de una creencia es menos importante que el que todos los miembros crean lo mismo: que todos coincidan en el tema es lo que los une.\nLas mentiras pueden ser señal de lealtad, si sirven para promulgar que se apoya al grupo.\nNo solo los nazis aplicaban una moral distinta a los suyos (los arios) y los demás. Las religiones lo hacen, los nacionalismos lo hacen. Actualmente, la ideología política influye también. En EEUU, las encuestas muestran que los partidarios de un partido (66%) defienden la violencia contra los del otro.\nLa polarización política no solo afecta a la definición de valores morales, sino a la percepción de la propia realidad.\nUna parte importante de la identidad propia es contra quién estamos. Si no tenemos un enemigo (como cuando EEUU perdió a la URSS), hará falta inventarse algo nuevo (como grupos internos)\nLa ausencia de religión entre gente de izquierdas y ateos los está llevando a construir rituales basados précisément en religión.\nEn las sociedades de cazadores no hacen falta las ideologías para detectar a desleales dentro del grupo: los sistemas eran simples y se detectaban mediante las acciones, no las creencias.\nLa división ellos nosotros se ha podido combatir en experimentos poniendo a la gente a trabajar en una misma tarea.\n\n\n\n5. La moralidad en el mundo moderno\n\nCultura del honor hasta siglo XIX. Sensible al insulto, se considera ataque a la reputación, se soluciona agresivamente. Hay una estructura legislativa débil. Siglo XX, cultura de la dignidad. El insulto se pasa por alto, no afecta a la reputación. Siglo XXI, cultura del victimismo. El insulto afecta pero se acude a terceras personas.\nSer víctima da una categoría moral más alta.\nIdeología política como método de atracción sexual. El beneficio económica y político de alardear de postura política es nulo actualmente. Pero tiene interés sexual y social.\nEl exhibicionismo moral consiste en alardear de ser superior moralmente. Una forma de hacerlo es inventarse problemas morales donde otros no los ven. El exhibicionismo es malo moralmente porque aumenta el cinismo, agota el agravio (Pedro y el lobo) y favorece la polarización de grupo.\nHace poco las palabras eran violencia. Ahora, el silencio (ante determinados temas) también es violencia.\nTodo se está moralizando (comer carne, ir en coche) y la consecuencia es que la gente teme decir lo que piensa o hace (por miedo a herir o por miedo a que lo cancelen).\nUna de las razones de esta dictadura de virtud (independiente del Estado) es la ausencia de religión. El papel de los medios es clave: muestran cambios que observan en pequeños grupos, y el resto de grupos con el tiempo absorbe estos cambios. Los medios transmiten lo que observan y, como consecuencia, contaminan.\nAdemás, los conflictos venden más que las relaciones pacíficas (tanto medios de comunicación como en ficción).\nLa falacia del mundo justo: tendemos a pensar que el mundo reparte justicia automáticamente y, si a alfuien le ocurren cosas malas, será por algo. O sea, se culpa a la víctima. Ocurre con violaciones, en situaciones financieras, en salud.\nSe puede moralizar por egoísmo (por mi bien) o altruismo (bien común). Pero parece que se tiende más a lo primero.\nLa Biblia (Carel van Schaik y Kai Michel) pudo surgir como una guía moral para una sociedad agrícola que se enfrentaba a situaciones no conocidas para los cazadores. El sedentarismo cambió la sociedad: desigualdad social, guerras a mayor escala y epidemias. Los grupos nómadas eran menos sensibles a epidemias por estar menos expuestos a contaminaciones y, por su menor tamaño, era más difícil que las propagaran.\n\nEn el monoteísmo, como no hay dioses malignos, Dios envía plagas a los pecadores. O sea, las enfermedades son el castigo contra la inmoralidad.\nademás, la religión provee de recomendaciones de comportamiento anti enfermedades (evitar a enfermos, trato con extraños, enterramiento, nutrición).\n\nLos antiguos no conocían los microbios, pero contra las epidemias idearon un sistema de protomedicina.\ncon la covid hemos copiado comportamientos religiosos: idealización del lavado de manos, moralizacion del uso de mascarilla y culpabilizacion de los enfermos.\n\n\n\n6. La nueva religión de la justicia social crítica\n\nPosmodernismo en la mitad del s. XX. Lleva al nihilismo más adelante. De filosofía se pasa a ideología. Finalmente se acepta que si hay una verdad, una verdad moral narrada por los victimismos.\nLa justicia social crítica surge en ambientes no religiosos, con comportamientos similares a los de los protestantes.\n\n\n\n7. Los peligros y problemas de la moralidad\n\nProblema 1. Cuando conflicto se convierte en moral es más difícil llegar a acuerdos.\n\nSi yo creo que un problema es moral, no sólo yo voy a evitar caer en él sino que quiero que tú no caigas en él. Si me creo en posesión de la verdad moral, me creo con el deber de limitar tus derechos.\n\nProblema 2. Las sociedades morales tienden al autoritarismo, la jerarquía, el elitismo y la desigualdad.\n\nEn el pasado L moralidad se ha usado para justificar que lo que hacen las clases altas es lo bueno y lo de las clases bajas, lo malo.\n\nProblema 3. La moralidad promueve las guerras y los genocidios\n\nLa moralidad pone al Nosotros por encima del Yo, pero también al Nosotros por encima del Ellos. O sea, promueve la cooperación pero también la competición.\n\nEn muchas ocasiones se considera que la violencia es lo correcto (violencia moralista)\nLa única forma de acabar con la violencia (salvo la de psicópatas y casos extremos) es convertirla en inmoral.\nLa moralidad puede ser un problema ante la ciencia, si los avances de esta atentan contra los principios de la otra.\nPropuestas para evitar caer es simplificaciones morales de buenos y malos:\n\nAumentar el sentimiento de humanidad, destacar lo que nos une.\nAumentar contacto con personas de otros grupos.\nImplicar a diferentes grupos en objetivos comunes.\nLos líderes y medios de comunicación deberían dar ejemplo pero promueven el conflicto.\n\nEl peligro de la moralidad en la ciencia es que el código binario de esta (verdadero/falso) no es compatible con el de aquella (bueno/malo).\nEl deseo de ser considerados buenos es más fuerte que el deseo de encontrar la verdad. Por eso la moral es más fuerte que la ciencia.\n\n\n\n8. El futuro de la moralidad\n\nEn el ámbito político necesitamos abandonar los discursos de Buenos y malos en los conflictos.\nEn filosofía se pueden buscar normas en favor de la cooperación saltando la moralidad como herramienta para ello.\nLas RRSS deberían cambiarse para no promover la pelea moral.\n\n\n\nLéelo\n\nVersión papel.\nVersión kindle."
  },
  {
    "objectID": "posts/2024-10-31-pyjanitor/index.html",
    "href": "posts/2024-10-31-pyjanitor/index.html",
    "title": "Quita caracteres raros en tus variables (2)",
    "section": "",
    "text": "El otro día te compartí ideas de cómo adaptar valores de texto que no te sirven como nombres de columnas. En R.\nJosé Luis Cañadas me habló de la librería pyjanitor como alternativa en Python. Me sonaba lejanamente pero nunca la había usado.\nAporta muchas más cosas aparte de cambiar textos para que cumplan con ciertas reglas de estilo. En cierto modo te da toda una nueva sintaxis para hacer las operaciones que haces con pandas de una forma más parecida al tidyverse.\nAhora mismo no es lo busco. Lo que busco es replicar el ejercicio del otro día.\nAsí que vamos a ello."
  },
  {
    "objectID": "posts/2024-10-31-pyjanitor/index.html#pyjanitor",
    "href": "posts/2024-10-31-pyjanitor/index.html#pyjanitor",
    "title": "Quita caracteres raros en tus variables (2)",
    "section": "pyjanitor",
    "text": "pyjanitor\n\nimport pandas as pd\nimport janitor\n\nTeníamos un listado de nombres que en un proyecto real vendrían como valores de la columna de un data frame. Y están repetidos.\n\nnombres = [\"Campaña veintitrés\", \"C'est très petite\", \"Alışveriş Arabası Önü Giydirme\"]\nnombres = nombres * 2\nnombres\n\n['Campaña veintitrés',\n \"C'est très petite\",\n 'Alışveriş Arabası Önü Giydirme',\n 'Campaña veintitrés',\n \"C'est très petite\",\n 'Alışveriş Arabası Önü Giydirme']\n\n\nQueremos reescribirlos para que sigan un estilo válido como nombres de columnas. Este estilo es opinable, pero para ir al grano asumiré que lo que quiero es esto:\n\nsnake case, es decir, todo en minúscula con palabras separadas por barras bajas.\nSin tildes, ni eñes ni otros caracteres raros (donde raro es no anglosajón).\n… Y más cosas que ahora me dan igual.\n\nLa librería janitor está pensada para trabajar directamente con un data frame. Pero no es lo que busco ahora. Es más: la funcionalidad de limpiar valores de una columna (no nombres) ni siquiera está a mano."
  },
  {
    "objectID": "posts/2024-10-31-pyjanitor/index.html#la-función-escondida",
    "href": "posts/2024-10-31-pyjanitor/index.html#la-función-escondida",
    "title": "Quita caracteres raros en tus variables (2)",
    "section": "La función escondida",
    "text": "La función escondida\nHe curioseado el código fuente y he encontrado la función que limpia los nombres de las columnas, es decir, el código que aplica las reglas de limpieza.\nEste código está un poco escondido, pero lo puedes llamar directamenmte sobre una serie de pandas (es decir, lo que podría llegar a ser una columna de un data frame).\nLa función es _clean_names y su primer argumento es la serie que quieres editar. Ninguno de los argumentos de configuración tiene valor por defecto, así que hay que especificarlos todos.\n\nfrom janitor.functions.clean_names import _clean_names\n\nnombres = pd.Series(nombres)\nnuevos_nombres = _clean_names(\n    nombres,\n    strip_underscores=True, \n    case_type='snake', \n    remove_special=True, \n    strip_accents=True, \n    enforce_string=True, \n    truncate_limit=False)\n\nnuevos_nombres\n\n0            campaa_veintitrs\n1             cest_trs_petite\n2    alveri_arabas_n_giydirme\n3            campaa_veintitrs\n4             cest_trs_petite\n5    alveri_arabas_n_giydirme\ndtype: object\n\n\nNo queda igual que en el caso de R (por ejemplo, las eñes las quita pero no las convierte a enes) pero para lo que quiero hacer aquí me vale.\nGracias, José Luis, como siempre, que incluso con Python me ayudas."
  },
  {
    "objectID": "posts/2025-04-17-optimism-correction/index.html",
    "href": "posts/2025-04-17-optimism-correction/index.html",
    "title": "Cómo corregir el optimismo de tu modelo estadístico",
    "section": "",
    "text": "Si entrenas un modelo en un conjunto de datos que no es muy grande, la métrica de ajuste que reportes no deberías calcularla sobre los datos de entrenamiento. Esto es porque el modelo ya conoce esos datos y se ha entrenado con ellos, intentando optimizar esa métrica de ajuste.\nLo típico es reservar un conjunto de validación, unos datos que el modelo no conoce, por lo que la métrica de ajuste no tendrá ese sesgo.\nA ese sesgo lo llamamos optimismo.\nLo malo, según Frank Harrell, es que, si tu conjunto de datos es pequeño, esa validación no será suficiente estable.\nBootstrap es una solución.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.utils import resample\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2025-04-17-optimism-correction/index.html#comentarios-iniciales",
    "href": "posts/2025-04-17-optimism-correction/index.html#comentarios-iniciales",
    "title": "Cómo corregir el optimismo de tu modelo estadístico",
    "section": "",
    "text": "Si entrenas un modelo en un conjunto de datos que no es muy grande, la métrica de ajuste que reportes no deberías calcularla sobre los datos de entrenamiento. Esto es porque el modelo ya conoce esos datos y se ha entrenado con ellos, intentando optimizar esa métrica de ajuste.\nLo típico es reservar un conjunto de validación, unos datos que el modelo no conoce, por lo que la métrica de ajuste no tendrá ese sesgo.\nA ese sesgo lo llamamos optimismo.\nLo malo, según Frank Harrell, es que, si tu conjunto de datos es pequeño, esa validación no será suficiente estable.\nBootstrap es una solución.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.utils import resample\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2025-04-17-optimism-correction/index.html#bootstrap-para-corregir-el-optimismo",
    "href": "posts/2025-04-17-optimism-correction/index.html#bootstrap-para-corregir-el-optimismo",
    "title": "Cómo corregir el optimismo de tu modelo estadístico",
    "section": "Bootstrap para corregir el optimismo",
    "text": "Bootstrap para corregir el optimismo\nLa idea es entrenar el modelo en varias muestras bootstrap y calcular la métrica de ajuste en cada par muestra bootstrap, muestra original, y luego la diferencia. Así, tendrás una lista de diferencias de longitud el número de muestras.\nLuego calculas la media.\nFinalmente, entrenas el modelo en la muestra original completa y le aplicas esa diferencia media.\nLa métrica de ajuste final será la métrica de ajuste en entrenamiento menos la diferencia, es decir, con el optimismo corregido.\n\nAlgunos datos\n\ndata = fetch_openml(name=\"boston\", version=1, as_frame=True)\nX = data.data\ny = data.target\n\nX_np = X.to_numpy()\ny_np = y.to_numpy()\n\n\n\nEntrenamiento en muestras bootstrap\n\nn_bootstraps = 200\n\nLo siguiente creo que quedaría más claro con un bucle for, pero en teoría no están recomendados. Así que creo una función y la llamo en una list comprehension.\n\ndef compute_optimism():\n    X_boot, y_boot = resample(X_np, y_np)\n\n    model = LinearRegression()\n    model.fit(X_boot, y_boot)\n\n    y_pred_boot = model.predict(X_boot)\n    r2_boot = r2_score(y_boot, y_pred_boot)\n\n    y_pred_orig = model.predict(X_np)\n    r2_orig = r2_score(y_np, y_pred_orig)\n\n    return(r2_boot - r2_orig)\n\nY ahora calculo todo.\n\noptimism_estimates = [compute_optimism() for _ in range(n_bootstraps)]\n\n\n\nCálculo del optimismo\nEl optimismo medio es lo que necesito para el próximo paso. Así que calculo la media de la lista que acabo de generar.\n\nmean_optimism = np.mean(optimism_estimates)\nmean_optimism\n\n0.017757491298340335\n\n\nPor curiosidad, así se distribuye el optimismo.\n\nplt.figure(figsize=(8, 5))\nplt.hist(optimism_estimates, bins=20, color=\"#800080\", edgecolor=\"black\", alpha=0.75)\n\nplt.title(\n    \"Distribución del optimismo del modelo (bootstrap)\", fontsize=14, fontweight=\"bold\"\n)\nplt.xlabel(\"Optimismo estimado\", fontsize=12)\nplt.ylabel(\"Frecuencia\", fontsize=12)\n\nplt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.3)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEntrenamiento final\nTodos los entrenamientos anteriores estaban hechos sobre muestras bootrstrap, es decir, no estaban sobre el entrenamiento\n\nfinal_model = LinearRegression()\nfinal_model.fit(X_np, y_np)\nfinal_r2 = r2_score(y_np, final_model.predict(X_np))\n\n\ncorrected_r2 = final_r2 - mean_optimism\n\nprint(f\"R² original: {final_r2:.4f}\")\nprint(f\"Optimismo medio: {mean_optimism:.4f}\")\nprint(f\"R² corregido: {corrected_r2:.4f}\")\n\nR² original: 0.7406\nOptimismo medio: 0.0178\nR² corregido: 0.7229"
  },
  {
    "objectID": "posts/2023-06-01-juegos-correlacion/index.html",
    "href": "posts/2023-06-01-juegos-correlacion/index.html",
    "title": "Juegos con la correlación de la felicidad",
    "section": "",
    "text": "Estoy revisando material de Datacamp sobre estadística.\nTienen un conjunto de datos sobre felicidad:\nRows: 133\nColumns: 9\n$ country             &lt;chr&gt; \"Finland\", \"Denmark\", \"Norway\", \"Iceland\", \"Nether…\n$ social_support      &lt;int&gt; 2, 4, 3, 1, 15, 13, 25, 5, 20, 31, 7, 42, 38, 27, …\n$ freedom             &lt;int&gt; 5, 6, 3, 7, 19, 11, 10, 8, 9, 26, 17, 16, 93, 28, …\n$ corruption          &lt;int&gt; 4, 3, 8, 45, 12, 7, 6, 5, 11, 19, 13, 58, 74, 9, 1…\n$ generosity          &lt;int&gt; 47, 22, 11, 3, 7, 16, 17, 8, 14, 25, 6, 75, 24, 30…\n$ gdp_per_cap         &lt;int&gt; 42400, 48300, 66300, 47900, 50500, 59000, 47200, 3…\n$ life_exp            &lt;dbl&gt; 81.8, 81.0, 82.6, 83.0, 81.8, 84.3, 82.8, 81.9, 82…\n$ happiness_score     &lt;dbl&gt; 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, …\n$ grams_sugar_per_day &lt;dbl&gt; 86.8, 152.0, 120.0, 132.0, 122.0, 166.0, 115.0, 16…\nHay casos con valores faltantes en la columna de corrupción…\nSospechoso xD\ncountry corruption\n1 United Arab Emirates         NA\n2         Saudi Arabia         NA\n3               Kuwait         NA\n4         Turkmenistan         NA\n5                China         NA\n6               Jordan         NA\nComo no tengo ni idea de dónde ha salido el dato, no me voy a entretener ahora por una columna y seis filas.\nAhora voy con una cosa que vi en Datacamp que no me cuadra nada:\nVale, según esto, los siguientes factores tienen una relación negativa con el nivel de felicidad:\nCreo que estos tres valores son mejores con valores bajos, porque si no, no tiene sentido.\nMe salto corrupción porque tiene valores más bajos que los demás –o sea, porque sí.\nLuego está relacionada positivamente:"
  },
  {
    "objectID": "posts/2023-06-01-juegos-correlacion/index.html#azúcar",
    "href": "posts/2023-06-01-juegos-correlacion/index.html#azúcar",
    "title": "Juegos con la correlación de la felicidad",
    "section": "Azúcar",
    "text": "Azúcar\nSí, salvo a los seguidores de Carlos Ríos, el azúcar suele dar felicidad –por lo menos un rato.\nPero ¿tanto como para que afecte al índice este?\n\n\n\n\n\n\n\n\n\n¿Qué países consumen más azúcar?\n\n\n\n\n\ncountry\ngrams_sugar_per_day\n\n\n\n\nUnited States\n175\n\n\nMalta\n168\n\n\nSwitzerland\n166\n\n\nNew Zealand\n162\n\n\nTrinidad and Tobago\n162\n\n\nColombia\n160\n\n\nDenmark\n152\n\n\nCosta Rica\n144\n\n\nBelgium\n144\n\n\nLuxembourg\n141\n\n\nJamaica\n135\n\n\nRussia\n134\n\n\nGermany\n133\n\n\nMexico\n133\n\n\nIceland\n132\n\n\nCanada\n132\n\n\nGuatemala\n132\n\n\nArgentina\n132\n\n\nChile\n131\n\n\nHonduras\n131\n\n\nMontenegro\n131\n\n\nAlbania\n131\n\n\n\n\n\n¿Y cómo son estos países frente a los demás?\n\n\n\n\n\n\n\n\n\nChorrada enorme que me llama la atención: los 20 países con más consumo de azúcar tienden a mayores valores de generosidad ni de apoyo social."
  },
  {
    "objectID": "posts/2023-06-01-juegos-correlacion/index.html#índice",
    "href": "posts/2023-06-01-juegos-correlacion/index.html#índice",
    "title": "Juegos con la correlación de la felicidad",
    "section": "Índice",
    "text": "Índice\nEsto no tiene ni pies ni cabeza. El índice se supone que se calcula a partir de estas métricas. Voy a ver de qué va ese cómputo y vuelvo.\n\n(Edición en 15/07/2023)\n\n\n\n\n\n\n\n\n\nMe gusta lo de la corrupción: parece que, en niveles de poca corrupción, un poquito más de corrupción baja el índice de felicidad, pero a partir de cierto valor (aprox 50), ya da igual todo y la gente pasa.\nAunque tampoco tengo claro que la corrupción sea muy relevante aquí, porque yo solo veo una nube de puntos (aunque la línea marque cierta tendencia en los valores inferiores)."
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html",
    "title": "12 reglas para la vida",
    "section": "",
    "text": "Comentario mío: el libro no me ha gustado. Creo que está mal planteado, porque tiene conceptos interesantes pero muy mal hilados. Pero eso: tiene conceptos interesantes. De algún capítulo no hay notas o bien porque me parecía una tontería o bien porque no tenía nada a mano para apuntar. Y algunas de las notas son frases literales, otras son reinterpretadas; con algunas estoy a favor, con otras estoy en contra, y otras me han llamado la atención pero no me he posicionado (aún)"
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#introducción",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#introducción",
    "title": "12 reglas para la vida",
    "section": "Introducción",
    "text": "Introducción\n\nLa pérdida de creencias grupales lleva a una vida caótica. Y la presencia de creencias grupales lleva a conflictos con otros grupos.\n\nEn occidente llevamos tiempo reduciendo nuestra tradición, religión y sentimientos nacionales, en parte para evitar los conflictos grupales. Pero así cada vez tendemos más a a vidas sin sentido, ni valores.\n\nla serotonina da una postura de victoria a las langostas. La octapina, de derrota.\nley de price, para distribución financiera y creativa\nMateo 25:29: para los que tenéis todo, se os dará más; para los que no tienen nada, todo se os quitará\nla tradición oral religiosa tiene más un fin moral que un fin descriptivo"
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-1.-enderézate-y-echa-los-hombros-hacia-atrás",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-1.-enderézate-y-echa-los-hombros-hacia-atrás",
    "title": "12 reglas para la vida",
    "section": "Regla 1. Enderézate y echa los hombros hacia atrás",
    "text": "Regla 1. Enderézate y echa los hombros hacia atrás\n\nUna langosta victoriosa tiene una postura más amplia. Y mayor proporción de serotonina que de octopamina. Inyectar serotonina en una langosta derrotada hace que cambie su postura.\nLas hembras dejan que los varones se peleen y luego eligen entre los más fuertes.\n\nConfían en que el comportamiento de estos se calme.\nEs un esquema llevado a la Bella y la Bestia o 50 sombras de Grey.\n\nLa naturaleza selecciona pero no es un concepto estático.\n\n“En mi reino tienes que correr con todas tus fuerzas para no quedarte en el mismo lugar” (Reina, Alicia en el País de las Maravillas)\n\nAtrévete a ser un peligro\nErguirse significa aceptar voluntariamente la carga del Ser.\nSi te presentas físciamente como alguien derrotado, la ente te tratará como alguien derrotado. Y te sentirás como tal.\nSi comienzas a erguirte, la gente te mirará como a tal."
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-2.-trátate-a-ti-mismo-como-tratarías-a-alguien-que-depende-de-ti",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-2.-trátate-a-ti-mismo-como-tratarías-a-alguien-que-depende-de-ti",
    "title": "12 reglas para la vida",
    "section": "Regla 2. Trátate a ti mismo como tratarías a alguien que depende de ti",
    "text": "Regla 2. Trátate a ti mismo como tratarías a alguien que depende de ti\n\nEl equilibrio entre caos y orden es importante para no frustrarnos pero que haya algo de motivación. Algo de riesgo.\nEl orden puede descontrolarse y convertirse en una migración forzosa o un campo de concentración.\nEl caos se relaciona con lo femenino. Todo se origina en el desorden y todos nos originamos en una madre.\nIncluso el jardín del Edén tenía una serpiente porque toda calma tiene un punto de caos, como el punto negro en la serpiente blanca tahoista.\nsiempre hay un enemigo. Incluso aunque derrotáramos a todos, estaríamos nosotros mismos.\nEs mejor enseñar a quienes dependen de ti a que sean competentes que protegerlos demasiado.\nNuestra vista (aguda) nos permite identificar rápidamente los frutos maduros de los árboles, que son los comestibles.\nigual que pensamos mal de otras personas, hay personas que de quienes realmente piensan mal es de sí mismas.\nAlguien que tiene un porqué tendrá casi siempre un cómo (Nietzsche)"
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-3.-hazte-amigo-de-aquellos-que-queran-lo-mejor-para-ti",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-3.-hazte-amigo-de-aquellos-que-queran-lo-mejor-para-ti",
    "title": "12 reglas para la vida",
    "section": "Regla 3. Hazte amigo de aquellos que queran lo mejor para ti",
    "text": "Regla 3. Hazte amigo de aquellos que queran lo mejor para ti\n\nFreud: compulsión de repetición. Gente que no tiene buena impresión de sí misma, busca rodearse del mismo tipo de personas que en el pasado le hicieron algún mal. Consideran que no merecen nada mejor.\nLa gente suele aceptar o incluso magnificar su propio sufrimiento, o incluso el de los demás , si lo pueden presentar como demostración de las injusticias del mundo.\nCuando metes en un grupo civilizado (un equipo de trabajo, reclusos organizados) a una persona más rebelde, el grupo no influye en el individuo, sino al revés, y el grupo empeora sus hábitos. Es más fácil ir hacia abajo.\nCuando ayudas a alguien, ¿lo haces porque quieres ayudarlo? ¿O porque quieres demostrar lo bondadoso que eres?\nHay quien se rodea de gente irresponsable porque en comparativa ellos están mejor. Es una comparativa fácil para hacerte creer especialmente bondadoso.\nSi tienes un amigo que no lo recomendarías a tu hermana, a tu padre o a tu hijo, ¿por qué lo tienes tú?"
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-4.-no-te-compares-con-otro-sino-con-cómo-eras-tú-antes",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-4.-no-te-compares-con-otro-sino-con-cómo-eras-tú-antes",
    "title": "12 reglas para la vida",
    "section": "Regla 4. No te compares con otro, sino con cómo eras tú antes",
    "text": "Regla 4. No te compares con otro, sino con cómo eras tú antes\n\nEn el mundo rural es más fácil destacar porque hay menos personas que en las ciudades.\nhay sobrerrepresentación de personajes famosos nacidos en pueblos.\nTiempos de espuma de pomelo y helado de whisky con tabaco.\n“siempre habrá alguien mejor que tú” es un tópico nihilista. No aporta información esa frase.\nla religión se encarga de lo que es correcto. Un religioso buscará hacer lo que es correcto, lo que está bien, aunque eso simplemente sea ser obediente.\nQue tu te creas algo, que te etiquetes en una identidad, no significa que lo seas. Tus acciones dicen qué eres. Y eres demasiado complejo para entenderte a ti mismo.\nLa Biblia como documento escrito durante mucho tiempo por muchos y nadie.\nPresta atención a lo que molesta:\n\n¿Qué me molesta?\nes algo que podría arreglar?\nestaría verdaderamente dispuesto a arreglarlo?\nsi alguna respuesta es no, busca molestaias en otro lado.\n\nNo sobrestimes tu conocimiento de ti mismo.\nPregúntate “qué podría hacer y estoy dispuesto a hacer para que me vida sea un poquito mejor?”. Puede ser algo tan básico como una gestión pendiente. O si crees que un tarea te va a llevar media hora y no te pones porque es mucho tiempo, dedícale 5 minutos hoy. Ayudará a que la veas como algo más asequible.\nLa vida no tiene el problema; lo tienes tú. Si la vida no te va bien, quizá es tu conocimiento lo que resulta insuficiente. Quizá tu estructura de valores necesita una remodelación. Quizá lo que quieres e ciega y no te deja ver oras posibilidades."
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-6.-antes-de-criticar-a-alguien-asegúrate-de-tener-tu-casa-en-perfecto-orden",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-6.-antes-de-criticar-a-alguien-asegúrate-de-tener-tu-casa-en-perfecto-orden",
    "title": "12 reglas para la vida",
    "section": "Regla 6. Antes de criticar a alguien, asegúrate de tener tu casa en perfecto orden",
    "text": "Regla 6. Antes de criticar a alguien, asegúrate de tener tu casa en perfecto orden\n\nCuando te das cuenta de las injusticias del mundo, puedes ignorarlo, puedes bucar el placer, puedes resignarte y aceptarlo o puedes tomar medidas y suicidarte.\nA la gente le sorprende que haya tantas masacres en EEUU o similares, pero es el primer acontecimiento narrado según la Biblia.\nTolstoi entiende que no hay otra escapatoria que el asesinato para combatir las injusticias “el bien de los muertos es mejor que el bien de los vivos”\nlos asesinos de masacres (y Caín) asesinan para contrariar el sistema, porque el mundo es injusto en sí mismo."
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-7.-dedica-tus-esfuerzos-a-hacer-cosas-con-significado-no-lo-que-más-te-convenga",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-7.-dedica-tus-esfuerzos-a-hacer-cosas-con-significado-no-lo-que-más-te-convenga",
    "title": "12 reglas para la vida",
    "section": "Regla 7. Dedica tus esfuerzos a hacer cosas con significado, no lo que más te convenga",
    "text": "Regla 7. Dedica tus esfuerzos a hacer cosas con significado, no lo que más te convenga\n\nLa vida es sufrimiento.\n“No tentaraás al señor tu Dios” ( Mateo 4:7) No renuncies a tu responsabilidad. No pidas que una tercera persona intervenga en tu favor cuando tú tienes margen de acción y no lo estás aprovechando.\nel socialismo se motiva más por una motivación de odio a los ricos y triunfadores más que en un verdadero interés por los pobres.\nlos socialistas creen en el dinero igual que los capitalistas y acaban siendo capitalistas.\nDios castigó al humano echándolo del Edén y condenándolo a trabajar.\nA veces cuando las cosas van mal, elproblema viene de cómo interpretas las cosas.\nSi el mundo que ves no es el que quieres, revisa tus valores.\nEs hora de que sacrifiques lo que más quieres para convertirte en quien quieres convertirte y dejar de ser quien eres."
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-9.-da-por-hecho-que-la-persona-a-la-que-escuchas-sabe-algo-que-tú-no-sabes",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-9.-da-por-hecho-que-la-persona-a-la-que-escuchas-sabe-algo-que-tú-no-sabes",
    "title": "12 reglas para la vida",
    "section": "Regla 9. Da por hecho que la persona a la que escuchas sabe algo que tú no sabes",
    "text": "Regla 9. Da por hecho que la persona a la que escuchas sabe algo que tú no sabes\n\nCuando participas en una verdadera conversación, hablas y escuchas, pero sobre todo escuchas.\nPuedes ser bastante listo si te limitas a callarte.\nNo sabemos escuchar. Tendemos a evaluar porque escuchar es peligroso. Puede cambiarte (Carl Rogers)\nregla: cuando estés en una discusión, antes de aportar una idea tuya o una opinión, tienes que repetir la idea anterior que ha dicho la otra persona, para que ella valide si lo has entendido bien.\nAl escuchar, permites que la otra persona piense. La primera vez que alguien expone algo en voz alta, llega a contradicciones y tiene que orientarlas, deciendo qué olvida de sus anécdotas para dejar claros sus sentimientos.\nsi una conversación te resulta aburrida, es que no estás escuchando.\nla gente organiza su cabeza conversando"
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-10.-cuando-hables-procura-expresarte-con-precisión",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-10.-cuando-hables-procura-expresarte-con-precisión",
    "title": "12 reglas para la vida",
    "section": "Regla 10. Cuando hables, procura expresarte con precisión",
    "text": "Regla 10. Cuando hables, procura expresarte con precisión\n\nLos objetos como extensión de un sistema, de un árbol con ramas u hojas. No pasa nada porque se rompa un objeto, porque el sistema sigue en pie.\ntú no acabas en lo que acota tu piel. Llegas más allá. Si usas una herramienta, pasa a formar parte de ti y la usas como si sintieras con su extremo. Tu familia o amigos forman parte de ti (¿qué prefieres perder: un brazo o un ser querido?). Tus niveles de testosterona varían si tu equipo pierde o gana.\nCuando usas una herramienta, no hace falta que la entiendas: solo te hace falta saber con precisión qué quieres hacer con ella.\nSomos más conscientes de nuestras limitaciones cuando algo que siempre usamos deja de funcionar y no lo sabemos arreglar, como un coche u ordenador. Simplificamos el mundo en el día a día.\nEl mundo es fácil mientras es simple, y eso solo ocurre cuando ignoramos cómo funciona. Ante una catástrofe, un desastre, o un simple desajuste, nos damos cuenta de que nuestra percepción era equivocada.\nSi no hay una tradición consesuada de quién hace qué, se acaba en tres escenarios. La esclavitud, en la que el esclavo se acaba revelando y llega el caos; la tiranía, en la que el tirano se aburre por exceso de obediencia y abandona al subordinado a su propia suerte; o la negociación. Esta exige que ambos jugadores hablen con franqueza y admitan que hay un problema.\nvivir significa realizar una labor constante de mantenimiento\nlas relaciones pueden acabarse por problemas que nunca nos atrevemos a solucionar ni siquiera plantear.\nNos negamos a ser específicos con un problema porque el serlo signfica admitir su existencia.\nsi no eres específico, no solo no defines el triunfo, sino que tampoco defines tu fracaso. Pero eso en sí mismo está abocado al fracaso."
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-11.-deja-en-paz-a-los-chavales-que-montan-en-monopatín",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-11.-deja-en-paz-a-los-chavales-que-montan-en-monopatín",
    "title": "12 reglas para la vida",
    "section": "Regla 11. Deja en paz a los chavales que montan en monopatín",
    "text": "Regla 11. Deja en paz a los chavales que montan en monopatín\n\nLos que patinan no se ponen protección porque no buscan la falsa seguridad de un equipo protector, sino la seguridad mediante la competencia.\nSegún Orwell, el socialismo en reino unido estaba representado políticamente por unas personas que no defendían a la clase obrera, sino que odiaban a los ricos.\nLos chicos actualmente tienden a ser etiquetados de peores estudiantes que las chicas y menos simpáticos y más desobedientes. Pero también más independientes y menos susceptibles a la depresión. Los intereses de los chicos tienden a relacionarse con cosas, y los de las chicas con personas.\nEstas diferencias están influidas por razones biológicos.\nPolíticas igualitarias como intentar que dos personas con el mismo trabajo cobren igual tienen la dificultad de decidir qué significa que dos trabajos sean iguales.\nhay políticas que buscan que se eduque igual a los chicos que a las chicas, con la idea de que sean menos violentos. Pero la violencia a veces es innata. Y además hay mujeres que el psicólogo les recomienda ser más violentas, porque un exceso de simpatía hace que te seas demasiado servicial y te afecta en la autoestima."
  },
  {
    "objectID": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-12.-si-ves-un-gato-por-la-calle-acarícialo",
    "href": "posts/2023-07-19-12-reglas-para-la-vida/index.html#regla-12.-si-ves-un-gato-por-la-calle-acarícialo",
    "title": "12 reglas para la vida",
    "section": "Regla 12. Si ves un gato por la calle, acarícialo",
    "text": "Regla 12. Si ves un gato por la calle, acarícialo\n\nSomos frágiles. Tenemos que aceptarnos y querernos entre nosotros con nuestras limitaciones sin intentar corregir.\n¿Qué le falta a un ser omnisciente, omnipresente y omnipotente? Limitación. Si uno no tiene limitaciones, consigue todo, y entonces no tiene una historia. La limitación te da una historia.\n“En una vasija, es su interior vacío lo que la hace útil. En una ventana, es su interior vacío lo que la hace útil. Lo que hay es provecho; lo que no hay, útil.” (Lao Tse)"
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "",
    "text": "Manson’s idea of “kill yourself” is similar to Paul Graham’s idea of “keep your identity small.” The central point is that if you don’t have an identity to protect, then change becomes much easier. (James Clear)"
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#no-lo-intentes",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#no-lo-intentes",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "1. No lo intentes",
    "text": "1. No lo intentes\n\nSutileza 1. Que algo te importe una mierda no significa ser indiferente, signifca estar cómodo siendo diferente\nSutileza 2. Para que la adversidad te importe una mierda, primero debe importante algo diferente de la adversidad.\n\nCuando una persona no tiene problemas, la mente automáticamente encuentra la forma de inventarse alguno.\n\nSutileza 3. Siempre estás eligiendo qué es importante para ti.\nLa gente no se da cuenta de que es normal que las cosas, a veces, estén mal."
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#la-felicidad-es-un-problema",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#la-felicidad-es-un-problema",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "2. La felicidad es un problema",
    "text": "2. La felicidad es un problema\n\nEl dolor y la pérdida son inevitables así que no deberíamos resistirnos a ellos.\nEl sufrimiento es útil biológicamente para inspirar el cambio.\nla mente no distingue mucho entre dolor psicológico y dolor físico.\nLa solución a un problema es simplemente la creación del siguiente.\nLa felicidad se consigue resolviendo problemas, no evitandolos.\ncon la negación no aceptas que tienes problemas cuya solución dará felicidad.\ncon el sentimiento de víctima niegas que tus problemas tengan solución y culpas a otros de ellos.\nconcepto de la caminadora hedónica: buscamos cambios pero nunca nos sentimos diferentes.\nqué dolor deseas en tu vida? Por qué estás dispuesto a luchar?"
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#el-valor-del-sufrimiento",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#el-valor-del-sufrimiento",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "4. El valor del sufrimiento",
    "text": "4. El valor del sufrimiento\n\nel sufrimiento se soporta si tiene una causa, si sirve para una causa de fuerza mayor\nHay capas de autoconciencia:\n\nQué siento.\nPor qué lo siento. Estas preguntas destacan qué identificas por éxito o fracaso\nValores personales. Por qué considero X un éxito? Por qué considero Y un fracaso? BAjo qué estándar me evalúo? Y bajo cuál juzgo a quienes me rodean?\n\nLos problemas serán inevitables pero el significado de cada problema no lo es.\nDe manera instintiva, nos comparamos con otros y vivimos para el estatus. Pero la cuestión real es bajo qué estándar nos medimos a nosotros mismos.\nEn función de tus valores, incluso bajo unas mismas circunstancias, puedes ser un fracaso o un éxito.\nPete Best: batería de los Beatles antes que Ringo, a quien despidieron antes de grabar love me do. Después de años de depresión, acabó diciendo que nunca habría sido tan feliz como ahora si hubiera seguido en los beatles. Porque ese despido contribuyó a que conociera a su mujer y tuviera hijos. Sus valores de lo que signfica el éxito cambiaron.\nValores mediocres:\n\nPlacer. Lo más fácil de obtener y lo más fácil de perder. Nos venden placer todo el rato. El placer no es la causs de la felicidad, sino su consecuencia.\nÉxito material. A más éxito material, menos valor tiene (10000 euros no valen igual para alguien que vive en la calle que para alguien que gana 50000). Y el éxito material se convierte en prioridad sobre valores como Honestidad, no violencia y compasión.\nTener siempre razón. Eso no permite aprender de tus errores\nMantenerse siempre positivo. La vida a veces es una mierda y es mejor aceptarlo. Las emociones negativas hay que expresarlas y hay que manifestarlas alineadas con nuestros valores\n\nAlgún día en retrospectiva los años de más esfuerzo te parecerán los más hermosos (Freud) algunos de los mejores momentos en nuestra vida no son placenteros, no son exitosos, no son reconocidos y no son positivos\nLos buenos valores se basan en la realidad, son socialmente constructivos y son inmediatos y controlables.\n\nSi tu parámetro es la popularidad, tu éxito se basa en que los demás te consideren popular, algo que no depende solo de ti, no lo puedes controlar.\n\nLa mejora personal consiste en priorizar tus valores, elegir las mejores cosas en las que centrar tu atención. Mark Manson propone 5:\n\neres responsable de todo lo que te pasa,\nvive en incertidumbre,\nacepta el fracaso,\nacepta el rechazo (di no)\ncontempla la mortalidad."
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#siempre-decides-algo",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#siempre-decides-algo",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "5. Siempre decides algo",
    "text": "5. Siempre decides algo\n\nA veces, la diferencia entre un problema doloroso y la sensación de poder es la percepción de que nosotros lo escogimos, que somos responsables de ello.\nSomos reponsables de todo en nuestras vidas, sin importar las circunstancias externas. No siempre controlamos lo que nos sucede, pero siempre controlamos cómo interpretamos lo que nos sucede.\nSiempre somos responsables, es imposible no serlo. Si elegimos no responder a los eventos, Es una forma de responder a los eventos.\nLa responsabilidad y la culpa no deberian ir siempre de la mano. Si alguien abandona un bebé a la puerta de tu casa, no es culpa tuya, pero es responsabilidad tuya el decidir qué hacer con el bebé.\nCulpa es tiempo pasado; responsabilidad es tiempo presente.\nReconocer tus errores quizá te ayude a no verte como la víctima inocente que crees ser.\nNos encanta asumir la responsabilidad de nuestros éxitos pero es más importante y más Fructífero aceptar la responsabilidad de nuestros fracasos. Simplemente culpar a los demás es hacerte daño a ti mismo.\nLa intensidad del evento no cambia la verdad. Si tu hijo muere, o te atracan, o un gobierno extremista atenta contra los derechos, no es tu culpa pero sí eres responsable de tus actuaciones y tus emociones ante ello.\nEl juego de la vida consiste en qué elecciones tomamos con las cartas que nos tocan, no en quejarnos sobre esas cartas.\nVictimismo chic: echar la culpa a otro grupo de mis desgracias. Todos los grupos se victimizan a sí mismos.\nCuantas más personas se proclaman víctimas, más difícil es detectar a las verdaderas víctimas.\nVivir en sociedad libre y democracia consiste en compartir la vida con otras opiniones, que quizá no te gusten. Ese es el precio."
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#te-equivocas-respecto-a-todo",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#te-equivocas-respecto-a-todo",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "6. Te equivocas respecto a todo",
    "text": "6. Te equivocas respecto a todo\n\nNo deberíamos buscar certeza sino duda. Duda de nuestros valores y creencias. Es la única forma de crecer. No deberíamos dedicar el tiempo a explorar en qué estamos acertados sino en qué estamos equivocados.\nExperimento. Persona en una habitación con botones. Si hacen una tarea concreta (desconocida para ella), una luz destella y ganan un punto. Tienen que ganar tantos puntos como puedan en 30 minutos. Una vez consiguen un punto, la tarea ya no funciona y tienen que buscar la siguiente, cada vez más complicada, con secuencias complejas o pulsar a la pata coja. Pero no hay patrón real: las luces son aleatorias. La gente hace algo complicado porque creen que es necesario, pero no sirve de nada.\nTendemos a encontrar patrones donde no los hay porque relacionar cosas nos simplifica el entendimiento. Pero hay dos problemas:\n\nque el cerebro se equivoca observando y escuchando. Y recordando.\nuna vez creamos un significado, nos cuesta desprendernos de él.\n\nTodas nuestras creencias están equivocadas: algunas están menos equivocadas que otras.\n\nme recuerda a George Box y su “todos los modelos están mal, pero algunos son útiles.”\n\nLa gente mala no cree que sea mala: está segura de que los demás son malos.\nCuanta más certezas buscas tener sobre algo, más inseguro te sientes. Cuanto más abierto estes a la ignorancia ante algo, más cómodo estarás con esa incertidumbre.\nLa incertidumbre ayuda a que no nos juzguemos a nosotros mismos: no sabemos cómo de exitosos podemos llegar a ser y la manera de averiguarlo es probando.\ncuanto más amenaza algo cambiar cómo te percibes, lo exitoso o fracasado que te consideres, lo seguro que te consideras de estar a la altura de tus valores, más evitarás decidirte a hacerlo. Por eso la gente tiene miedo al éxito (igual que al fracaso): el éxito puede cambiar quién eres.\nConocerte a ti mismo puede ser peligroso. Es mejor que nunca te conozcas: así nunca estarás anclado a una personalidad a la que ser fiel.\nel budismo defiende que te liberes. Me recuerda a Cohen. Cuando no nos contamos historias a nosotros mismos, nos liberamos para actuar.\nno te compares con identidades raras, porque te sentirás único y excepcional. Compárate con conceptos amplios y mundanos.\npreguntas pro incertidumbre:\n\nY si estoy equivocado? Todos somos los peores observadorrs de nosotros mismos.\nQué supondría si estuviera equivocado? Considerar un pensamiento sin aceptarlo es la marca de una persona educada (Aristóteles)\nEstar equivocado crearía un problema mejor o peor que el actual?\n\nsi parece que eres tú contra el mundo, seguramente seas tú contra ti mismo"
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#el-fracaso-es-un-paso-hacia-adelante",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#el-fracaso-es-un-paso-hacia-adelante",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "7. El fracaso es un paso hacia adelante",
    "text": "7. El fracaso es un paso hacia adelante\n\nsolo podemos ser exitosos en algo en lo que estemos dispuestos a fallar.\nlos mejores valores están orientados a procesos. Si cumples con tu propósito, se te acaba el valor, ¿y ahora qué? Así llegan las crisis, pprque te has quedado sin propósito en la vida.\nel miedo, la ansiedad, la tristeza pueden representar el dolor necesario para cambiar y crecer psicológicamente.\nLa acción no es solo efecto de la motivation. También es causa de ella. Principio Haz algo.\nLo importante para conseguir resultados es hacer algo, lo que sea, independiente del resultado. El fracaso no importará en ese caso, porque el objetivo es solo hacer algo. Y eso nos dará la motivación para seguir adelante."
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#la-importancia-de-decir-no",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#la-importancia-de-decir-no",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "8. La importancia de decir no",
    "text": "8. La importancia de decir no\n\nEl significado en la vida se basa en el rechazo de alternativas, la supresión de libertad: el compromiso a un estilo de vida, a una persona…\nSe nos ha convencido de que hay que estar abierto a todo, decir sí a todas las oportunidades. Pero necesitamos rechazar, porque necesitamos tener una postura favorable hacia algo. Si aceptamos todo, no nos dedicamos honestamente a algo, y nos sentiremos vacíos.\nPara apreciar algo, necesitamos rechazar activamente todo lo contrario a ese algo.\nNos quedamos con relaciones que no nos dan felicidad, trabajos que nos queman, nos quedamos con ganas de decir lo que pensamos. Para ser honestos, tenemos que estar a gusto diciendo y escuchando “no”\nEl rechazo contribuye a la mejora.\nno debes responsabilizar a los demás de tus problemas pero tampoco debes responsabilizarte de los problemas de los demás.\nen una relación enfermiza, cada uno intenta resolver los problemas del otro. Para sentirse así a gusto consigo mismos.\nLa honestidad es más importante que sentirse bien todo el rato.\ncuantas más elecciones nos den, menos satisfechos estaremos con nuestra decisión.\nSi elegimos una opción pero dejamos puertas abiertas al cambio, por si acaso, no profundizaremos suficiente en nuestra decisión. Y estaremos siempre insatisfechos. Si nos comprometemos con nuestra decisión y nos olvidamos de las demás, aunque rechacemos oportunidades el compromiso nos hará aprovechar totalmente nuestra decisión.\nCuando pasas por muchos trabajos, muchas relaciones, muchas posesiones, muchos pasatiempos, el siguiente te aporta ya poco.\nEl compromiso es libertad y liberación."
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#y-luego-te-mueres",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#y-luego-te-mueres",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "9. Y luego te mueres",
    "text": "9. Y luego te mueres\n\n¿Por qué te importa la muerte si tienes tanto miedo a la vida?\nBecker: la negación de la muerte.\nMark Twain; una persona que vive plenamente está siempre preparada para la muerte.\n¿Cuál es tu legado? Cómo de mejor y diferente será el mundo cuando hayas muerto?\nVas a morir y será porque fuiste afortunado para vivir."
  },
  {
    "objectID": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#léelo",
    "href": "posts/2022-08-05-el-sutil-arte-de-que-todo-te-importe-una-mierda/index.html#léelo",
    "title": "El sutil arte de que todo te importe una mierda",
    "section": "Léelo",
    "text": "Léelo\n\nVersión papel\nVersión kindle"
  },
  {
    "objectID": "posts/2024-11-26-espurias/index.html",
    "href": "posts/2024-11-26-espurias/index.html",
    "title": "Relaciones espurias de datos de divorcios",
    "section": "",
    "text": "En el libro Statistical Rethinking el autor estudia la relación entre el divorcio y dos variables:\n\nLa edad media en la que la pareja se casa.\nLa cantidad de gente que se casa.\n\n\nlibrary(rethinking)\nlibrary(ggplot2)\nggplot2::theme_set(ggplot2::theme_bw())\n\ndata(WaffleDivorce)\ndf_divorce &lt;- WaffleDivorce\nhead(df_divorce)\n\n    Location Loc Population MedianAgeMarriage Marriage Marriage.SE Divorce\n1    Alabama  AL       4.78              25.3     20.2        1.27    12.7\n2     Alaska  AK       0.71              25.2     26.0        2.93    12.5\n3    Arizona  AZ       6.33              25.8     20.3        0.98    10.8\n4   Arkansas  AR       2.92              24.3     26.4        1.70    13.5\n5 California  CA      37.25              26.8     19.1        0.39     8.0\n6   Colorado  CO       5.03              25.7     23.5        1.24    11.6\n  Divorce.SE WaffleHouses South Slaves1860 Population1860 PropSlaves1860\n1       0.79          128     1     435080         964201           0.45\n2       2.05            0     0          0              0           0.00\n3       0.74           18     0          0              0           0.00\n4       1.22           41     1     111115         435450           0.26\n5       0.24            0     0          0         379994           0.00\n6       0.94           11     0          0          34277           0.00\n\n\nPrimero estudia ambas variables por separado, como explicativas de la variable objetivo. Y luego usa las dos a la vez.\nLos modelos los plantea con las variables escaladas, con media 0 y desviación típica 1.\n\ndf_divorce$A &lt;- scale(df_divorce$MedianAgeMarriage)\ndf_divorce$D &lt;- scale(df_divorce$Divorce)\ndf_divorce$M &lt;- scale(df_divorce$Marriage)\n\nggplot(df_divorce) + \n  geom_histogram(aes(D), binwidth = 0.3)\n\n\n\n\n\n\n\n\n\nfit_edad &lt;- rethinking::quap(\n    alist(\n        D ~ dnorm(mu, sigma), \n        mu &lt;- a + bA * A, \n        a ~ dnorm(0, 0.2),\n        bA ~ dnorm(0, 0.5),\n        sigma ~ dexp(1)\n    ), \n    data = df_divorce\n)\n\nprecis(fit_edad)\n\n               mean         sd       5.5%      94.5%\na      2.249960e-07 0.09737883 -0.1556300  0.1556304\nbA    -5.684030e-01 0.10999989 -0.7442041 -0.3926019\nsigma  7.883263e-01 0.07801150  0.6636489  0.9130038\n\n\nParece que edades tempranas de matrimonio se asocian a un mayor ratio de divorcios.\nPor otro lado:\n\nfit_casados &lt;- quap(\n    alist(\n        D ~ dnorm(mu, sigma), \n        mu &lt;- a + bM * M,\n        bM ~ dnorm(0, 0.5), \n        a ~ dnorm(0, 0.2),\n        sigma ~ dexp(1)\n    ), \n    data = df_divorce\n)\n\nprecis(fit_casados)\n\n              mean        sd       5.5%     94.5%\nbM    3.500456e-01 0.1259276  0.1487889 0.5513022\na     2.580548e-06 0.1082465 -0.1729962 0.1730014\nsigma 9.102662e-01 0.0898626  0.7666484 1.0538839\n\n\nEl ratio de matrimonios también muestra una relación (positiva en este caso) con los divorcios. Parece razonable: cuanta más gente se case, más divorcios habrá.\nAhora bien, ¿es esto útil para predecir divorcios?\n\nfit_ambas &lt;- quap(\n    alist(\n        D ~ dnorm(mu, sigma),\n        mu  &lt;- a + bA * A + bM * M,\n        bA ~ dnorm(0, 0.5),\n        bM ~ dnorm(0, 0.5), \n        a ~ dnorm(0, 0.2),\n        sigma ~ dexp(1)\n    ), \n    data = df_divorce\n)\n\nplot(coeftab(fit_edad, fit_casados, fit_ambas), par = c(\"bA\", \"bM\"))\n\n\n\n\n\n\n\n\nCuando están ambas variables, el ratio de matrimonios no tiene una relación significativa con el ratio de divorcios; no aporta información que la edad al casarse no esté aportando ya.\nQueda ahora pendiente ver si hay relación entre la edad al casarse y la cantidad de matrimonios.\n\nfit_edad_casados &lt;- quap(\n    alist(\n        M ~ dnorm(mu, sigma), \n        mu &lt;- a + bA * A, \n        bA ~ dnorm(0, 0.5), \n        a ~ dnorm(0, 0.5),\n        sigma ~ dexp(1)\n    ), \n    data = df_divorce\n)\n\nprecis(fit_edad_casados)\n\n               mean         sd       5.5%      94.5%\nbA    -6.947577e-01 0.09572744 -0.8477487 -0.5417668\na      3.730622e-05 0.09466898 -0.1512620  0.1513366\nsigma  6.817422e-01 0.06758133  0.5737341  0.7897502\n\n\nLas prioris del modelo:\n\nprior &lt;- extract.prior(fit_edad_casados)\nmu &lt;- link(fit_edad_casados, post = prior, data = list(A=c(-2, 2)))\nhead(mu)\n\n            [,1]        [,2]\n[1,]  0.40754123  0.55198721\n[2,] -0.97045287 -0.26903069\n[3,] -1.04597053  0.04440014\n[4,] -0.33992995  1.06002650\n[5,]  0.07748926  0.69109599\n[6,]  1.96460272 -0.69449517\n\nplot(NULL, xlim = c(-2, 2), ylim = c(-5, 5))\nfor (i in 1:50) {\n    lines(c(-2, 2), mu[i, ], col = \"gray\")\n}\n\n\n\n\n\n\n\n\n\nmu &lt;- link(fit_edad)\nmu_mean &lt;- apply(mu, 2, mean)\nmu_pi &lt;- apply(mu, 2, PI)\n\n# D_sim &lt;- sim(fit_edad, n = 1e3)\n# D_sim &lt;- apply(D_sim, 2, PI)\n\nplot(mu_mean ~ df_divorce$D, col=rangi2, ylim = range(mu_pi), \n     xlab = \"Observed divorce\", ylab = \"Predicted divorce\")\nabline(a = 0, b = 1, lty = 2)\nfor (i in 1:nrow(df_divorce)) {\n    lines(rep(df_divorce$D[i], 2), mu_pi[, i], col = \"gray\")\n    # lines(rep(df_divorce$D[i], 2), D_sim[, i], col = \"blue\")\n}"
  },
  {
    "objectID": "posts/2024-07-30-concatena-strings/index.html",
    "href": "posts/2024-07-30-concatena-strings/index.html",
    "title": "Concatenación de strings con R, con benchmark",
    "section": "",
    "text": "Tienes una variable con un texto que tienes que introducirlo dentro de otro texto. Este ocurre habitualmente cuando un usuario pone algún parámetro de tipo texto y tienes que incluir en un párrafo preconstruido, o interactuar de alguna forma con ese nombre, por ejemmplo, un chatbot clásico que recibía como entrada el nombre del usuario y luego le preguntaba “¿Qué tal estás, ?” (no sé si eso es tan clásico o sigue siendo orden del día).\nEn este post comparo tiempos de ejecución de varias opciones que tienes en R.\nLa idea general te la da este código:\n\nnombre_entrada &lt;- \"Pepito\"\n\npaste(\"Hola,\", nombre_entrada)\n\n[1] \"Hola, Pepito\"\n\n\nVoy a crear varias funciones con distintas formas de hacer esto y luego cuento tiempos de cómputo.\n\ncon_paste &lt;- function(nombre_entrada) paste(\"Hola,\", nombre_entrada)\ncon_sprintf &lt;- function(nombre_entrada) sprintf(\"Hola, %s\", nombre_entrada)\ncon_glue &lt;- function(nombre_entrada) glue::glue(\"Hola, {nombre_entrada}\")\ncon_gsub &lt;- function(nombre_entrada) {\n  gsub(\"nombre_entrada\", nombre_entrada, \"Hola, nombre_entrada\") \n}\n\nY aquí los tiempos.\n\nmicrobenchmark::microbenchmark(\n  con_paste = con_paste(\"Pepito\"),\n  con_sprintf = con_sprintf(\"Pepito\"),\n  con_glue = con_glue(\"Pepito\"),\n  con_gsub = con_gsub(\"Pepito\")\n)\n\nUnit: nanoseconds\n        expr    min       lq      mean   median       uq      max neval\n   con_paste   2202   2650.5  12954.99   3401.0   4201.0   850801   100\n con_sprintf    901   1250.5  11382.94   2101.0   3151.0   790101   100\n    con_glue 135902 143701.0 591651.04 154401.0 180501.5 41454102   100\n    con_gsub  10301  12150.0  30976.88  14801.5  18202.0  1449701   100\n\n\nEn casos más complicados, glue::glue() puede parecer más intuitivo. ¿Es ese el único motivo para usarlo?\n\ncon_paste &lt;- function(nombre_entrada) {\n  paste0(\"Hola, \", nombre_entrada, \". ¿Cómo estás, \", nombre_entrada, \"?\")\n} \ncon_sprintf &lt;- function(nombre_entrada) {\n  sprintf(\"Hola, %s. ¿Cómo estás, %s?\", nombre_entrada, nombre_entrada)\n  \n} \ncon_glue &lt;- function(nombre_entrada) {\n  glue::glue(\"Hola, {nombre_entrada}. ¿Cómo estás, {nombre_entrada}?\")\n  \n}\ncon_gsub &lt;- function(nombre_entrada) {\n  gsub(\"nombre_entrada\", nombre_entrada, \"Hola, nombre_entrada. ¿Cómo estás, nombre_entrada?\") \n}\n\nmicrobenchmark::microbenchmark(\n  con_paste = con_paste(\"Pepito\"),\n  con_sprintf = con_sprintf(\"Pepito\"),\n  con_glue = con_glue(\"Pepito\"),\n  con_gsub = con_gsub(\"Pepito\")\n)\n\nUnit: microseconds\n        expr     min       lq      mean   median       uq      max neval\n   con_paste   2.701   3.2010  14.34601   4.0010   4.4005 1040.201   100\n con_sprintf   1.200   1.7015  15.13189   2.6515   3.2010 1225.901   100\n    con_glue 149.201 157.8510 182.23310 162.9015 168.9510 1041.801   100\n    con_gsub  13.601  15.5005  33.93807  18.0510  19.5515 1564.500   100"
  },
  {
    "objectID": "posts/2024-05-25-viviendas/index.html",
    "href": "posts/2024-05-25-viviendas/index.html",
    "title": "Evolución de compraventa mensual en España",
    "section": "",
    "text": "El INE tiene datos mensuales sobre compraventa de viviendas por Comunidades Autónomas (lo tiene por provincias pero he pasado de ese nivel):\n\nimport pandas as pd\n\ndf_viviendas = pd.read_csv(\n  'ine-compraventa.csv', \n  sep=';', \n  thousands='.',\n  encoding='iso-8859-1'\n)\n\ndf_viviendas = (\n  df_viviendas\n  .drop(['Régimen y estado', 'Total Nacional', 'Provincias'], axis=1)\n  .rename(\n    columns=lambda x: x.lower().replace(' ', '_').encode('ascii', 'ignore').decode('utf-8'))\n)\n\ndf_viviendas.head()\n\n\n\n\n\n\n\n\ncomunidades_y_ciudades_autnomas\nperiodo\ntotal\n\n\n\n\n0\n01 Andalucía\n2024M03\n8805\n\n\n1\n01 Andalucía\n2024M02\n9606\n\n\n2\n01 Andalucía\n2024M01\n10290\n\n\n3\n01 Andalucía\n2023M12\n7519\n\n\n4\n01 Andalucía\n2023M11\n9857\n\n\n\n\n\n\n\nVamos a tratarlo un poco (tampoco mucho, que es sábado):\n\ndf_viviendas['periodo'] = pd.to_datetime(df_viviendas['periodo'], format='%YM%m')\n\ndf_viviendas['comunidades_y_ciudades_autnomas'] = df_viviendas['comunidades_y_ciudades_autnomas'].str.replace(r'\\d{2} ', '', regex=True)\n\n\ndf_viviendas = (\n  df_viviendas\n  .rename(columns={'comunidades_y_ciudades_autnomas': 'ccaa'})\n)\n\ndf_viviendas.head()\n\n\n\n\n\n\n\n\nccaa\nperiodo\ntotal\n\n\n\n\n0\nAndalucía\n2024-03-01\n8805\n\n\n1\nAndalucía\n2024-02-01\n9606\n\n\n2\nAndalucía\n2024-01-01\n10290\n\n\n3\nAndalucía\n2023-12-01\n7519\n\n\n4\nAndalucía\n2023-11-01\n9857\n\n\n\n\n\n\n\nMucho mejor. Ahora toca visualizar esto.\nEl siguiente código es de ChatGPT… y sinceramente, creo que está mal, que pinta lo que le da la gana y no lo que yo quiero (concretamente, se inventa el orden del eje x).\nPero llevo horas con problemas de VSCode, versiones de Python, entornos virtuales y paso de dedicarle más tiempo a esto hoy (cosa que en R habrían sido 5 minutos).\nOtro día.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ng = sns.FacetGrid(\n  df_viviendas, \n  col='ccaa', \n  col_wrap=5, \n  hue='ccaa', \n  sharey=False,\n)  \n\ng.map_dataframe(sns.lineplot, x='periodo', y='total')\n\n# Ajustar las etiquetas y el layout\nfor ax in g.axes.flatten():\n    # Ajustar los locators para que solo se muestren unos pocos ticks\n    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=12))  # Ajusta el intervalo según sea necesario\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n    ax.tick_params(axis='x', rotation=45)\n\ng.set_axis_labels('Fecha', 'Total')\n\nplt.show()"
  },
  {
    "objectID": "posts/2024-04-22-empleo/index.html",
    "href": "posts/2024-04-22-empleo/index.html",
    "title": "Cómo aplicar non standard evaluation con dplyr y rlang",
    "section": "",
    "text": "Tengo datos de empleos en el sector público y privado del INE. Y los quiero explorar con dplyr y ggplot2.\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(ggplot2)\ntheme_set(theme_light())\nLeer los ficheros del INE siempre es aventura con sus formatos, pero aquí tienes el código.\ndf_empleo &lt;- readr::read_csv2(\"../../data/4262.csv\", \n                              show_col_types = FALSE,\n                              locale = locale(encoding = \"latin1\"))\n\ndf_empleo &lt;- janitor::clean_names(df_empleo) |&gt; \n  rename(ccaa = comunidades_y_ciudades_autonomas)\n\nhead(df_empleo)\n\n# A tibble: 6 × 5\n  ccaa         sexo    tipo_de_sector periodo total\n  &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;\n1 01 Andalucía Hombres Empleo público 2023T4   286.\n2 01 Andalucía Hombres Empleo público 2023T3   298.\n3 01 Andalucía Hombres Empleo público 2023T2   289 \n4 01 Andalucía Hombres Empleo público 2023T1   294.\n5 01 Andalucía Hombres Empleo público 2022T4   297 \n6 01 Andalucía Hombres Empleo público 2022T3   299"
  },
  {
    "objectID": "posts/2024-04-22-empleo/index.html#una-exploración-rápida",
    "href": "posts/2024-04-22-empleo/index.html#una-exploración-rápida",
    "title": "Cómo aplicar non standard evaluation con dplyr y rlang",
    "section": "Una exploración rápida",
    "text": "Una exploración rápida\nEsto es lo que te planteo explorar: una agregación trimestral por tipo de sector.\n\ndf_agg &lt;- df_empleo |&gt;\n  group_by(periodo, tipo_de_sector) |&gt; \n  summarise(total = sum(total), .groups = \"drop\") |&gt; \n  convierte_periodo() # función propia para convertir a fecha\n\ndf_agg\n\n# A tibble: 24 × 3\n   periodo    tipo_de_sector  total\n   &lt;date&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 2021-01-01 Empleo privado 15809.\n 2 2021-01-01 Empleo público  3397.\n 3 2021-04-01 Empleo privado 16232.\n 4 2021-04-01 Empleo público  3440.\n 5 2021-07-01 Empleo privado 16547.\n 6 2021-07-01 Empleo público  3484.\n 7 2021-10-01 Empleo privado 16709.\n 8 2021-10-01 Empleo público  3476.\n 9 2022-01-01 Empleo privado 16617.\n10 2022-01-01 Empleo público  3468.\n# ℹ 14 more rows\n\n\nQue devuelve un gráfico así.\n\nggplot(df_agg) + \n  geom_line(aes(x = periodo, y = total, col = tipo_de_sector)) + \n  scale_color_discrete(\"Tipo de empleo\") + \n  labs(\n    x = \"Trimestre\", y = \"Empleados\", \n    title = \"Evolución trimestral de empleados privados y públicos\", \n    caption = \"Fuente: INE\"\n  )"
  },
  {
    "objectID": "posts/2024-04-22-empleo/index.html#más-libertad",
    "href": "posts/2024-04-22-empleo/index.html#más-libertad",
    "title": "Cómo aplicar non standard evaluation con dplyr y rlang",
    "section": "Más libertad",
    "text": "Más libertad\nAhora voy a preparar una función para que un usuario pueda elegir una columna más con la que agregar.\nLo que quiero es una función con la que funcione un código así:\n\nagrega_empleo(sexo)\n\nSi intentas plantear eso solo con dplyr te dará un error diciendo que cierto objeto no es nada. En el código siguiente, grouping_var no sería nada sino fuera por esa combinación de enquo() con otros elementos.\n\nlibrary(rlang)\n\nagrega_empleo &lt;- function(grouping_var) {\n  nombre_metrica &lt;- as_name(enquo(grouping_var))\n  nombre_metrica &lt;- paste0(\"total_por_\", nombre_metrica)\n  df_empleo |&gt;\n    group_by(periodo, tipo_de_sector, !!enquo(grouping_var)) |&gt; \n    summarise(!!nombre_metrica := sum(total), .groups = \"drop\") |&gt; \n    convierte_periodo() \n}\n\n\nagrega_empleo(ccaa)\n\n# A tibble: 456 × 4\n   periodo    tipo_de_sector ccaa                       total_por_ccaa\n   &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;                               &lt;dbl&gt;\n 1 2021-01-01 Empleo privado 01 Andalucía                        2469.\n 2 2021-01-01 Empleo privado 02 Aragón                            451.\n 3 2021-01-01 Empleo privado 03 Asturias, Principado de           294 \n 4 2021-01-01 Empleo privado 04 Balears, Illes                    416 \n 5 2021-01-01 Empleo privado 05 Canarias                          626 \n 6 2021-01-01 Empleo privado 06 Cantabria                         190.\n 7 2021-01-01 Empleo privado 07 Castilla y León                   752 \n 8 2021-01-01 Empleo privado 08 Castilla - La Mancha              658.\n 9 2021-01-01 Empleo privado 09 Cataluña                         2913.\n10 2021-01-01 Empleo privado 10 Comunitat Valenciana             1671.\n# ℹ 446 more rows\n\n\nEn lo anterior, !!enquo() se encarga de evaluar la expresión dentro de grouping_var (ccaa en este caso) en el entorno adecuado.\nSi tienes muchas variables, necesitarás algo como lo siguiente (aunque aún no sé cómo afecta eso entonces a los nombres de nuevas columnas).\n\nagrega_empleo &lt;- function(...) {\n  df_empleo |&gt;\n    group_by(periodo, tipo_de_sector, !!!enquos(...)) |&gt; \n    summarise(total= sum(total), .groups = \"drop\") |&gt; \n    convierte_periodo() \n}\n\nagrega_empleo(ccaa, sexo)\n\n# A tibble: 912 × 5\n   periodo    tipo_de_sector ccaa                       sexo    total\n   &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;                      &lt;chr&gt;   &lt;dbl&gt;\n 1 2021-01-01 Empleo privado 01 Andalucía               Hombres 1474 \n 2 2021-01-01 Empleo privado 01 Andalucía               Mujeres  995.\n 3 2021-01-01 Empleo privado 02 Aragón                  Hombres  268 \n 4 2021-01-01 Empleo privado 02 Aragón                  Mujeres  183.\n 5 2021-01-01 Empleo privado 03 Asturias, Principado de Hombres  164.\n 6 2021-01-01 Empleo privado 03 Asturias, Principado de Mujeres  130.\n 7 2021-01-01 Empleo privado 04 Balears, Illes          Hombres  233.\n 8 2021-01-01 Empleo privado 04 Balears, Illes          Mujeres  183.\n 9 2021-01-01 Empleo privado 05 Canarias                Hombres  335.\n10 2021-01-01 Empleo privado 05 Canarias                Mujeres  291.\n# ℹ 902 more rows"
  },
  {
    "objectID": "posts/2024-08-23-sin-comillas/index.html",
    "href": "posts/2024-08-23-sin-comillas/index.html",
    "title": "Más sobre evaluación de expresiones en R",
    "section": "",
    "text": "En R puedes evaluar una expresión a partir de texto (o sea, como si la expresión fuera un objeto de tipo character).\n\na &lt;- 1\nb &lt;- 3\n\nexpr1 &lt;- \"a + b\"\neval(parse(text = expr1))\n\n[1] 4\n\n\nEsa expresión es la operación \\(a + b\\). También podrías tener la operación \\(2\\cdot a - 3 \\cdot b\\), que podrías guardar en un objeto character de forma parecida a la anterior.\nDe manera intuitiva, si quieres juntar o concatenar las dos operaciones mediante una división, ¿qué operación te sale?\nA mí me sale esta: \\(\\frac{a + b}{2\\cdot a - 3 \\cdot b}\\), que no es la misma que \\(a + \\frac{b}{2\\cdot a} - 3 \\cdot b\\).\nPero si no tienes cuidado, para R sí lo será:\n\nexpr2 &lt;- \"2 * a - 3 * b\"\n\nexpr_division &lt;- paste(expr1, expr2, sep = \" / \")\nexpr_division\n\n[1] \"a + b / 2 * a - 3 * b\"\n\n\nEl resultado de esa operación es -6.5, que no es lo mismo que:\n\n(a + b) / (2 * a - 3 * b)\n\n[1] -0.5714286"
  },
  {
    "objectID": "posts/2024-08-23-sin-comillas/index.html#orden-de-operaciones",
    "href": "posts/2024-08-23-sin-comillas/index.html#orden-de-operaciones",
    "title": "Más sobre evaluación de expresiones en R",
    "section": "",
    "text": "En R puedes evaluar una expresión a partir de texto (o sea, como si la expresión fuera un objeto de tipo character).\n\na &lt;- 1\nb &lt;- 3\n\nexpr1 &lt;- \"a + b\"\neval(parse(text = expr1))\n\n[1] 4\n\n\nEsa expresión es la operación \\(a + b\\). También podrías tener la operación \\(2\\cdot a - 3 \\cdot b\\), que podrías guardar en un objeto character de forma parecida a la anterior.\nDe manera intuitiva, si quieres juntar o concatenar las dos operaciones mediante una división, ¿qué operación te sale?\nA mí me sale esta: \\(\\frac{a + b}{2\\cdot a - 3 \\cdot b}\\), que no es la misma que \\(a + \\frac{b}{2\\cdot a} - 3 \\cdot b\\).\nPero si no tienes cuidado, para R sí lo será:\n\nexpr2 &lt;- \"2 * a - 3 * b\"\n\nexpr_division &lt;- paste(expr1, expr2, sep = \" / \")\nexpr_division\n\n[1] \"a + b / 2 * a - 3 * b\"\n\n\nEl resultado de esa operación es -6.5, que no es lo mismo que:\n\n(a + b) / (2 * a - 3 * b)\n\n[1] -0.5714286"
  },
  {
    "objectID": "posts/2024-05-09-csv/index.html",
    "href": "posts/2024-05-09-csv/index.html",
    "title": "Un problema habitual en ficheros CSV",
    "section": "",
    "text": "Tienes el conjunto de datos iris en un fichero de texto plano con esta pinta:\nComo verás, lo he tuneado un poco: le he añadido una columna con valor constante 1000.23, escrito con el formato anglosajón de separador de miles usando comas.\nMaravilloso. A ver cómo lo leemos."
  },
  {
    "objectID": "posts/2024-05-09-csv/index.html#r",
    "href": "posts/2024-05-09-csv/index.html#r",
    "title": "Un problema habitual en ficheros CSV",
    "section": "R",
    "text": "R\nEn R, puedes tener la tentación de leerlo así:\n\nlibrary(readr)\ndf_iris &lt;-  read_csv(\"iris.txt\")\ndf_iris\n\n# A tibble: 150 × 1\n   sepal length (cm);sepal width (cm);petal length (cm);petal width (cm);speci…¹\n   &lt;chr&gt;                                                                        \n 1 5.1;3.5;1.4;0.2;0;1,000.23                                                   \n 2 4.9;3.0;1.4;0.2;0;1,000.23                                                   \n 3 4.7;3.2;1.3;0.2;0;1,000.23                                                   \n 4 4.6;3.1;1.5;0.2;0;1,000.23                                                   \n 5 5.0;3.6;1.4;0.2;0;1,000.23                                                   \n 6 5.4;3.9;1.7;0.4;0;1,000.23                                                   \n 7 4.6;3.4;1.4;0.3;0;1,000.23                                                   \n 8 5.0;3.4;1.5;0.2;0;1,000.23                                                   \n 9 4.4;2.9;1.4;0.2;0;1,000.23                                                   \n10 4.9;3.1;1.5;0.1;0;1,000.23                                                   \n# ℹ 140 more rows\n# ℹ abbreviated name:\n#   ¹​`sepal length (cm);sepal width (cm);petal length (cm);petal width (cm);species;extra`\n\n\nTodo horrible porque está todo dentro de una columna. La función no ha sabido separar en columnas porque esperaba la coma como delimitador. La coma está en los valores de la columna extra pero no en la primera fila, con los nombres de columnas. Así que read_csv entiende que solo hay una columna.\nread.csv() se hace bastante lío y daría error con algo así.\nUn clásico cuando no te funciona read_csv() es probar read_csv2(), que asume que la separación es \";\".\n\ndf_iris &lt;-  read_csv2(\"iris.txt\")\nhead(df_iris)\n\n# A tibble: 6 × 6\n  `sepal length (cm)` `sepal width (cm)` `petal length (cm)` `petal width (cm)`\n                &lt;dbl&gt;              &lt;dbl&gt;               &lt;dbl&gt; &lt;chr&gt;             \n1                  51                 35                  14 0.2               \n2                  49                 30                  14 0.2               \n3                  47                 32                  13 0.2               \n4                  46                 31                  15 0.2               \n5                  50                 36                  14 0.2               \n6                  54                 39                  17 0.4               \n# ℹ 2 more variables: species &lt;dbl&gt;, extra &lt;dbl&gt;\n\n\nTampoco funciona.\nLas columnas sí están separadas, pero los valores no tienens sentido. read_csv2() ha usado la coma como separador decimal. Y los puntos que había en los números, se los ha pasado por ahí. Y la columna extra la ha puesto como si la coma fuera el separador decimal, cuando realmente es el separador de miles.\nAsí que mal.\nSi usaras R base con read.csv2() tendrías también un poco de lío, con columnas identificadas como texto en lugar de número.\nEn estos casos, yo tiro de readr::read_delim() especificando cuál es el separador de miles y cuál el decimal (algo equivalente puedes hacer en R base con read.table()).\nCon la función readr::locale() especifico estos separadores.\n\ndf_iris &lt;- read_delim(\"iris.txt\", delim = \";\", locale = locale(decimal_mark = \".\", grouping_mark = \",\"))\nhead(df_iris)\n\n# A tibble: 6 × 6\n  `sepal length (cm)` `sepal width (cm)` `petal length (cm)` `petal width (cm)`\n                &lt;dbl&gt;              &lt;dbl&gt;               &lt;dbl&gt;              &lt;dbl&gt;\n1                 5.1                3.5                 1.4                0.2\n2                 4.9                3                   1.4                0.2\n3                 4.7                3.2                 1.3                0.2\n4                 4.6                3.1                 1.5                0.2\n5                 5                  3.6                 1.4                0.2\n6                 5.4                3.9                 1.7                0.4\n# ℹ 2 more variables: species &lt;dbl&gt;, extra &lt;dbl&gt;\n\n\nEs un esperpento anti-estandarizaciones, pero ocurre. Debe de haber algunas herramientas de estas que descargan informes en texto plano que tienen esta configuración horrorosa, porque me llegan ficheros así cada dos por tres.\nY como no son estándar, las funciones habituales de lectura de CSVs se hacen bastante lío.\nPara casos así, intento ser muy explícito con el código y read_delim() me obliga a ello por sistema."
  },
  {
    "objectID": "posts/2024-05-09-csv/index.html#con-python",
    "href": "posts/2024-05-09-csv/index.html#con-python",
    "title": "Un problema habitual en ficheros CSV",
    "section": "Con Python",
    "text": "Con Python\nEn Python usaría directamente la función de pandas pd.read_csv() especificando todo.\nPor defecto no funciona:\n\nimport pandas as pd\ndf_iris = pd.read_csv('iris.txt')\ndf_iris.head()\n\nPero cada cosa tiene su argumento:\n\nimport pandas as pd\ndf_iris = pd.read_csv(\n  'iris.txt', \n  sep=';', \n  decimal='.', #no necesario en este caso\n  thousands=','\n  \n  )\ndf_iris.head()\n\nLa función pd.read_csv() ya asume que el separador decimal es el punto, así que no hace falta especificar nada de eso esta vez…"
  },
  {
    "objectID": "cuadernos.html",
    "href": "cuadernos.html",
    "title": "Cuadernos",
    "section": "",
    "text": "Tema\n\n\nFecha\n\n\nDescripción\n\n\n\n\n\n\nCómo corregir el optimismo de tu modelo estadístico\n\n\n17 abr 2025\n\n\nBootstrap aplicado a la corrección del optimismo de un modelo estadístico cuando tienes pocos datos.\n\n\n\n\nCuánto te afecta la semilla al resultado final\n\n\n12 abr 2025\n\n\nLa semilla afecta a la aleatoriedad del ajuste del modelo. Esto puede generar una incertidumbre que no siempre se tiene en cuenta.\n\n\n\n\nTiempos de lectura de un fichero Excel\n\n\n11 mar 2025\n\n\nEjercicio de exploración de tiempos de lectura de un fichero Excel\n\n\n\n\nOtro experimento con relaciones espurias\n\n\n24 ene 2025\n\n\nEjercicio con variables espurias y regresión\n\n\n\n\nExperimento con multicolinealidad\n\n\n16 ene 2025\n\n\nExperimentos con multicolinealidad para ver si siempre es tan mala como se dice en la regresión lineal\n\n\n\n\nContraste de medias, estilo bayesiano\n\n\n11 dic 2024\n\n\nContraste de medias y de distribución con estadística bayesiana\n\n\n\n\nRegresión lineal bootstrap y bayesiana\n\n\n6 dic 2024\n\n\nComparativa de regresión lineal bootstrap y bayesiana\n\n\n\n\nRelaciones espurias de datos de divorcios\n\n\n26 nov 2024\n\n\nEjemplos de relaciones espurias con Statistical Rethinking\n\n\n\n\nQuita caracteres raros en tus variables (2)\n\n\n31 oct 2024\n\n\nPequeña práctica para limpiar los textos de las variables de tus tablas con Python\n\n\n\n\nEvolución de la población por provincias en España\n\n\n27 oct 2024\n\n\nExploratorio con Python de los datos públicos de residentes por provincia en España\n\n\n\n\nJuego probabilístico con números aleatorios\n\n\n23 oct 2024\n\n\nPráctica de pensamiento probabilístico con ejemplos de generación de números aleatorios con Python\n\n\n\n\nQuita caracteres raros en tus variables\n\n\n18 oct 2024\n\n\nPequeña práctica para limpiar los textos de las variables de tus tablas\n\n\n\n\nEjemplo de regresión múltiple con Python\n\n\n6 oct 2024\n\n\nPequeña práctica de regresión múltiple con Python, siguiendo ISLP\n\n\n\n\nProcesos ergódicos\n\n\n4 oct 2024\n\n\nPequeña práctica y ejemplo con procesos ergódicos en Python\n\n\n\n\nIntroducción práctica a Rust\n\n\n8 sept 2024\n\n\nAlguna prueba con Rust y su interacción con R y Python\n\n\n\n\nMás sobre evaluación de expresiones en R\n\n\n23 ago 2024\n\n\nTutorial sobre evaluación manual de expresiones en R con rlang\n\n\n\n\nEvaluación de expresiones en R\n\n\n17 ago 2024\n\n\nTutorial sobre evaluación manual de expresiones en R con rlang\n\n\n\n\nEstadísticos en regresión lineal por variable\n\n\n16 ago 2024\n\n\nRevisión paso a paso de cómo calcular el p-valor de una variable en una regresión lineal\n\n\n\n\nEncuesta de presupuestos familiares\n\n\n9 ago 2024\n\n\nExploración de datos de la Encuesta de Presupuestos Familiares del INE\n\n\n\n\nMercamadrid\n\n\n5 ago 2024\n\n\nExploración de datos públicos de mercamadrid\n\n\n\n\nPapel del punto en R\n\n\n4 ago 2024\n\n\nCreación de operadores propios en R para operaciones en línea\n\n\n\n\nPernotaciones por Comunidades Autónomas\n\n\n4 ago 2024\n\n\nExploración de datos del INE de Pernotaciones por Comunidades Autónomas\n\n\n\n\nConcatenación de strings con R, con benchmark\n\n\n30 jul 2024\n\n\nComparativa con microbenchmark de distintas formas de concatenar strings con R\n\n\n\n\nEvolución de compraventa mensual en España\n\n\n25 may 2024\n\n\nEvolución de compraventa mensual en España\n\n\n\n\nUn problema habitual en ficheros CSV\n\n\n9 may 2024\n\n\nEl formato de los ficheros CSV depende de cómo se separen las columnas y de cómo se indique la separación decimal. ues hay una mezcla horrorosa que no tiene ningún sentido.\n\n\n\n\nPor qué puedes necesitar non standard evaluation en dplyr\n\n\n24 abr 2024\n\n\nComparativa de non-stantard evaluation y tidyeval\n\n\n\n\nCómo aplicar non standard evaluation con dplyr y rlang\n\n\n22 abr 2024\n\n\nCómo aplicar non standard evaluation con dplyr y rlang\n\n\n\n\nCómo procesas datos que no te caben en RAM\n\n\n21 abr 2024\n\n\nProcesamiento de datos con más gigas que memoria RAM\n\n\n\n\nRelaciones espurias\n\n\n18 mar 2024\n\n\nExploración de relaciones espurias en datos de divorcios\n\n\n\n\n¿Cómo surge la distribución normal?\n\n\n24 feb 2024\n\n\nExploración de la distribución normal con R\n\n\n\n\nStatistical Rethinking (2)\n\n\n24 ene 2024\n\n\nPor qué es importante la distribución normal\n\n\n\n\n¿Cómo se usan las prioris conjugadas?\n\n\n31 dic 2023\n\n\nTutorial en R de cómo usar las prioris conjugadas en estadística bayesiana, con el ejemplo de lanzar una moneda\n\n\n\n\nStatistical Rethinking (1)\n\n\n27 dic 2023\n\n\nIntroducción de métodos generativos en estadística bayesiana, basados en el libro Statistical Rethinking de Richard McElreath\n\n\n\n\nIntroducción a análisis de supervivencia con R\n\n\n22 dic 2023\n\n\nEjemplo de introducción al análisis de supervivencia con R\n\n\n\n\nComparativa pequeña entre tidyverse y R base\n\n\n7 dic 2023\n\n\nUna comparativa entre algunos códigos en R base y tidyverse para tratar un data frame\n\n\n\n\nAdvent of code (2)\n\n\n2 dic 2023\n\n\nEjercicio 2 de Advent of code 2023\n\n\n\n\nAdvent of code (1)\n\n\n1 dic 2023\n\n\nEjercicio 1 de Advent of code 2023\n\n\n\n\n¿Qué formato de datos es más rápido de leer y escribir?\n\n\n26 nov 2023\n\n\nExperimentos para comparar tiempos de ejecución en lectura y escritura de distintos ficheros de datos con R y Python\n\n\n\n\nCómo interpretas un intervalo de confianza\n\n\n11 nov 2023\n\n\nCómo calcular e interpretar intervalos de confianza\n\n\n\n\nPirámide de población española, desde 1971\n\n\n20 oct 2023\n\n\nEvolución de la población española por edades desde 1971, con R y ggplot2.\n\n\n\n\nPaseo en bicicleta eléctrica por Madrid\n\n\n12 oct 2023\n\n\n\n\n\n\n\nCómo visualizar cualquier conjunto de datos en 2 dimensiones\n\n\n30 sept 2023\n\n\nUso de t-SNE para visualizar en 2 dimensiones un conjunto de datos, independientemente de cuántas variables tenga \n\n\n\n\n¿Existe la suerte?\n\n\n28 sept 2023\n\n\nNotas sobre el libro Existe la suerte de Nassim Taleb\n\n\n\n\nMuertes de humanos causadas por otros animales\n\n\n14 ago 2023\n\n\nExploración de muertes de humanos anuales causadas por otros animales.\n\n\n\n\nPersonas centenarias [tidytuesday]\n\n\n21 jul 2023\n\n\nExploración de datos de personas centenarias, con datos del tidytuesday de 2023-05-30\n\n\n\n\nMuestreo (II)\n\n\n21 jul 2023\n\n\nRepaso a más conceptos introductorios al muestreo estadístico\n\n\n\n\n7 hábitos de la gente altamente efectiva\n\n\n20 jul 2023\n\n\nResumen con interpretación propia sobre el libro 7 hábitos de la gente altamente efectiva, de Stephen Covey\n\n\n\n\n12 reglas para la vida\n\n\n19 jul 2023\n\n\nResumen con interpretación propia sobre el libro 12 reglas para al vida, de Jordan Peterson\n\n\n\n\nMuestreo (I)\n\n\n18 jul 2023\n\n\nUn repaso a algunos conceptos introductorios al muestreo estadístico\n\n\n\n\nMatrimonios y nacimientos\n\n\n16 jul 2023\n\n\nEvolución anual de matrimonios, nacimientos y defunciones en España. Los matrimonios están decreciendo. ¿Será porque la institución del matrimonio está caduca? ¿Será que ya no aporta nada? ¿O sí aporta pero los jóvenes no le vemos interés, ignorantes nosotros?\n\n\n\n\nPernoctaciones\n\n\n15 jul 2023\n\n\nAnálisis de la evolución de pernoctaciones por CCAA\n\n\n\n\nCara o cruz, modo Bayes (I)\n\n\n24 jun 2023\n\n\nExperimentos estadísticos frecuentistas y bayesianos en lanzamiento de monedas.\n\n\n\n\nLos hijos de padres altos son bajitos\n\n\n12 jun 2023\n\n\nLa regresión a la media aplicada a alturas de padres e hijos.\n\n\n\n\nJuegos con la correlación de la felicidad\n\n\n1 jun 2023\n\n\nLas causualidades de la felicidad. Cacharreos con datos de felicidad\n\n\n\n\nCómo ser un estoico\n\n\n27 may 2023\n\n\nNotas de lectura del libro de Massimo Pigliucci\n\n\n\n\nDiferencia de edad entre parejas sentimentales en películas\n\n\n19 feb 2023\n\n\nBuenas diferencias de edades entre parejas en películas\n\n\n\n\nRompe la barrera del no\n\n\n11 sept 2022\n\n\nResumen del libro Rompe la barrera del no\n\n\n\n\n27 aforismos de Nassim Taleb\n\n\n3 sept 2022\n\n\nSelección de aforismos de Taleb, del Lecho de Procusto\n\n\n\n\nLos peligros de la moralidad\n\n\n9 ago 2022\n\n\nResumen de Los peligros de la moralidad, de Pablo Malo\n\n\n\n\nMi IPC es (incluso) más alto\n\n\n6 ago 2022\n\n\nExploración de datos del IPC\n\n\n\n\nEl sutil arte de que todo te importe una mierda\n\n\n5 ago 2022\n\n\nResumen del libro El sutil arte de que casi todo te importe una mierda\n\n\n\n\nBurbuja turística\n\n\n26 dic 2021\n\n\nExploración de datos de turismo\n\n\n\n\nAsalariado, Taleb habla de ti\n\n\n1 oct 2021\n\n\n\n\n\n\n\nNate Silver describe el comienzo de una pandemia que no fue\n\n\n7 ago 2021\n\n\nPárrafo de La señal y el ruido, sobre pandemias\n\n\n\n\nLiteratura y totalitarismo\n\n\n24 may 2020\n\n\nPárrafo de un ensayo de George Orwell\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Leonardo Hansa",
    "section": "",
    "text": "«Quien tiene convicciones demasiado fuertes es porque no ha profundizado lo suficiente.» (E. M. Cioran)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2023-11-05-confianza/index.html",
    "href": "posts/2023-11-05-confianza/index.html",
    "title": "Cómo interpretas un intervalo de confianza",
    "section": "",
    "text": "Creo que no entendí qué eran los intervalos de confianza en ninguna clase de estadística.\nA ver, no digo que no me los explicaran.\nDigo que, pese a que me los explicaron, no entendí nada de ellos.\nEn el mundo laboral me encuentro con dos situaciones contradictorias:\nY no digo que sea culpa suya.\nLa verdad, con lo complicado que es el concepto (porque no tiene ni pies ni cabeza) y lo mal que lo dan, normal que no se enteren.\n¿Se deberían dar los intervalos de confianza? Pues dado que algo de la incertidumbre de tus estimaciones deberías dar, mientras no tengas nada mejor, sí, deberías darlos.\nTe muestro aquí un ejemplo de cómo se interpretan."
  },
  {
    "objectID": "posts/2023-11-05-confianza/index.html#cálculo-del-intervalo",
    "href": "posts/2023-11-05-confianza/index.html#cálculo-del-intervalo",
    "title": "Cómo interpretas un intervalo de confianza",
    "section": "Cálculo del intervalo",
    "text": "Cálculo del intervalo\nVoy a usar un ejemplo muy concreto.\n¿Significa eso que lo que te voy a explicar solo sirve para este ejemplo?\nNo, es extrapolable. Pero necesito un ejemplo concreto porque si no, pasa lo de siempre: que nadie se entera de nada.\nEl ejemplo es estimar la media de una población a partir de la media de una muestra. Parece una bobada, lo sé, pero verás en breve que hay cierta incertidumbre al usar la media muestral para esto.\nCon esto no digo que no uses la media muestral. Lo que digo es que seas consciente de que hay cierta incertidumbre.\nPara jugar con esta incertidumbre, voy a recrear un ejemplo del que conozco la media poblacional, y veremos qué tal se comporta la media muestra.\nUso una población de distribución normal de media 1000 y desviación típica 100. ¿Lo puedes hacer con otros números? Sí, claro. Ya te he dicho que esto es extrapolable a cualquier cosa de estimación.\nVoy a tomar una muestra de tamaño 100 de esta población. Y calculo la media y desviación típica.\n\nset.seed(3718)\nsample_size &lt;- 100\npopulation_mean &lt;- 1000\npopulation_sd &lt;- 100\n\nnorm_values &lt;- rnorm(sample_size, population_mean, population_sd)\n\nsample_mean &lt;- mean(norm_values)\n\nLa media muestral es 1007.718297.\nEl objetivo de estos párrafos es calcular el intervalo de confianza para esa estimación, o sea, a la hora de usar la media muestral como estimación de la media poblacional.\nPara ello necesitamos la media y desviación típica poblacional[^1] (las tenemos), el tamaño muestral (me lo he inventado yo, 100).\n[^1 Si no tienes la poblacional y usas una muestral necesitas hacer un ajuste en lo que voy a contar hoy. Te lo cuento otro día.]\nY necesitamos también los valores de la distribución normal que delimitan la confianza al nivel que queramos (95% para no salir de lo habitual). En R lo puedes calcular con qnorm(0.975) y qnorm(0.025).\nEl intervalo estará centrado en tu media muestral. Las cotas vendrán dadas por un margen de error.\nEste margen de error se calcula con:\n\nLa desviación típica poblacional.\nLa raíz cuadrada del tamaño muestral.\nUn valor que ahora te explico.\n\nLa desviación típica de la muestra dividida por la raíz cuadrada del tamaño muestral no es un valor cualquiera. Es la desviación típica de la media muestral.\nO sea, tienes una población, y tomas una muestra; con esa muestra te calculas la media y te la guardas. Ahora tomas otra muestra, vuelves a calcular la media y te la guardas. Y así muchas veces. Todas estas medias no serán iguales entre sí: serán parecidas, y deberían estar cerca de la media poblacional, pero varían. Varían con una desviación típica. ¿Cuál? Ese valor que hemos dicho.\nMira.\nVoy a calcular 1000 medias muestrales de diferentes muestras de tamaño 100 y calculo su desviación típica.\n\nmuchas_medias &lt;- replicate(1000, {\n  mean(rnorm(sample_size, population_mean, population_sd))\n})\n\nsd(muchas_medias)\n\n[1] 10.00571\n\n\nY ahora lo comparo con la división de la desviación típica de la población entre la raíz cuadrada del tamaño muestral: 10. ¿A que se parecen bastante?\nPero el margen de error no depende solo de eso, sino que falta el numerito extra. O sea, tiene sentido que el margen de error dependa de la desviación típica de lo que estás estimando, pero falta lo de la confianza. Si quieres mucha confianza, tendrás que aumentar de alguna forma el margen de error; y si quieres menos confianza, tendrás que reducirlo.\nPara eso necesitas el estadístico \\(z\\). Nos interesa el intervalo de confianza al 95%, ¿no? Pues este valor \\(z\\) lo puedes ver como el valor de la distribución normal \\(\\mathcal{N}(0, 1)\\) que deja a su izquierda el 95% de la probabilidad. O sea, los números mayores que él, solo aparecerán en una distribución normal con un 5% de probabilidad.\nAl multiplicar este valor por la desviación típica de la media muestral, lo que estás haciendo es llevar el margen de error a la escala de tu población (y no dejarla en la de una normal 0-1).\nPues ahora ya está todo.\nSi eres alguien anclado en el siglo XX, buscarás el valor de \\(z\\) en una tabla. Si vives en el presente, usarás algún software que te lo busque.\nEn R puedes usar la función qnorm().\nQuieres el 95% de probabilidad, pero ten en cuenta que la distribución normal es simétrica. Así que necesitas el extremo que deje el 2.5% a su derecha y el que deje el 2.5% a su izquierda. Lo puedes hacer con qnorm(c(0.025, 0.975)).\nEl resultado es que el intervalo es (988.1186572, 1027.3179369).\nSi tienes en cuenta que la media poblacional era 1000, no es una estimación horrible.\nPero…"
  },
  {
    "objectID": "posts/2023-11-05-confianza/index.html#interpretación-del-intervalo",
    "href": "posts/2023-11-05-confianza/index.html#interpretación-del-intervalo",
    "title": "Cómo interpretas un intervalo de confianza",
    "section": "Interpretación del intervalo",
    "text": "Interpretación del intervalo\n¿Cómo interpretas esos valores? La interpretación no es que el valor de la media poblacional está ahí dentro con un 95% de probabilidad.\nLa media poblacional es un dato, es una realidad. No sabemos si está dentro o no, pero no hay una probabilidad para indicar dónde está.\nVamos a ver cómo se interpreta.\nEn lugar de dar la definición formal de intervalo de confianza, que no hay dios que la entienda, vamos a construir muchos intervalos de confianza.\nConstruyo 10^{4} intervalos de confianza, cada uno a partir de una muestra diferente, pero todas provenientes de la misma población.\nCada muestra tendrá una media, que aprovecho y calculo también.\n\nlibrary(purrr)\nexperiments &lt;- map(seq_len(computations), function(i) {\n  compute_interval(sample_size, population_mean, population_sd) \n})\n\nFíjate en el siguiente gráfico. Ha habido muestras cuya media muestral ha sido cercana a 960 ó a 1040. Y esas muestras tendrán sus intervalos de confianza centrados en ese valor… tiene pinta de que van a estar lejos de la media poblacional.\n\n\n\n\n\n\n\n\n\nDe esos experimentos que hemos hecho, podemos extraer también los intervalos de confianza. Voy a pintártelos para que veas dónde han caído.\n\n\n\n\n\n\n\n\n\nToda esa banda morada son los 10^{4} intervalos de confianza calculados. La línea amarilla realmente es una sucesión de puntos (como todas las líneas) con cada media muestral.\nLa línea negra es la media poblacional. Y fíjate en que hay unos pocos intervalos (los de más abajo y los de más arriba, los extremos) que no tocan esa línea negra. O sea, hay intervalos que no contienen a la media poblacional.\nEsto es muy fuerte.\nAl principio he tomado una muestra, he calculado la media muestral como estimación de la poblacional, y en torno a ella he construido un intervalo de confianza.\nEse intervalo era un posible intervalo de confianza al 95% por ciento. Y en ese caso sí contenía a la media poblacional (cosa que sé porque estoy en un experimento controlado, pero lo normal es que no sepas la media poblacional).\nPero si me hubiera salido como muestra una diferente, podría haber ocurrido que el intervalo no hubiera contenido a la media poblacional.\nMuy fuerte esto.\nPero la pregunta que surge ahora es: ¿cuántos intervalos no contienen a la media poblacional?\nPues aquí entra el 95%.\n\nEl 95% de los intervalos de confianza al 95% contienen la media poblacional\nA continuación calculo el porcentaje de intervalos que sí contienen a la media poblacional, teniendo en cuenta a los 10^{4} que he calculado.\n\n\n[1] 0.9519\n\n\nMe sale que el 95,19% de los intervalos calculados contienen a la media poblacional.\nEso es lo que quiere decir que tengas un intervalo de confianza al 95%. Significa que, de todos los intervalos que puedes calcular, el 95% tendrá al parámetro real. Pero el tuyo no sabes si está en ese 95% o no lo está (repito, en este experimento sí sabemos si está en ese porcentaje porque es un experimento controlado pero normalmente no lo sabes).\nDe hecho, puedo cambiar el nivel de confianza, y el porcentaje cambiará acorde a ello:\nConfianza al 99%\n\n\n# A tibble: 1 × 1\n    pct\n  &lt;dbl&gt;\n1 0.990\n\n\nConfianza al 80%\n\n\n# A tibble: 1 × 1\n    pct\n  &lt;dbl&gt;\n1 0.803"
  },
  {
    "objectID": "posts/2024-10-27-poblacion-evolutivo/index.html",
    "href": "posts/2024-10-27-poblacion-evolutivo/index.html",
    "title": "Evolución de la población por provincias en España",
    "section": "",
    "text": "El INE tiene datos de evolución trimestral semestral de la población por provincias en España.\nEn un formato horrible, por supuesto.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport locale\n\n\nlocale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n\ndf_ine = pd.read_csv(\n    'ine-poblacion-provincias.csv', \n    sep=';', \n    header=0, \n    names=['edad', 'provincia', 'sexo', 'periodo', 'residentes']\n    # usecols=['provincia', 'periodo', 'residentes']\n)\n\ncondicion = (df_ine['edad'] == 'Todas las edades') & (df_ine['sexo'] == 'Total')\ndf_ine = df_ine.loc[condicion, ['provincia', 'periodo', 'residentes']]\n\ndf_ine['residentes'] = pd.to_numeric(df_ine['residentes'].str.replace('.',''))\ndf_ine['periodo'] = pd.to_datetime(df_ine['periodo'], format='%d de %B de %Y')\n\ndf_ine.head()\n\n\n\n\n\n\n\n\nprovincia\nperiodo\nresidentes\n\n\n\n\n0\nTotal Nacional\n2023-01-01\n48085361.0\n\n\n1\nTotal Nacional\n2022-10-01\n47940295.0\n\n\n2\nTotal Nacional\n2022-07-01\n47781354.0\n\n\n3\nTotal Nacional\n2022-04-01\n47609145.0\n\n\n4\nTotal Nacional\n2022-01-01\n47486727.0\n\n\n\n\n\n\n\nSi los tratas un poco, puedes usar mi librería favorita para hacer gráficos, matplotlib, en la que si tuviera que trabajar dejaría mi trabajo, me pagaran lo que me pagaran.\n\ndf_solo_provincias = df_ine[df_ine['provincia'] != 'Total Nacional']\ndf_solo_provincias['residentes'] = df_solo_provincias['residentes'].fillna(method='ffill')\n\nprovincias_destacadas = ['28 Madrid', '08 Barcelona', '46 Valencia/València', '29 Málaga']\n\ndf_solo_provincias['destacada'] = df_solo_provincias['provincia'].apply(lambda x: x if x in provincias_destacadas else 'Otras')\n\nDicen que construyendo más vivienda se solucionan los problemas de precios.\n\ncolors = {'28 Madrid': 'blue', '08 Barcelona': 'red', '46 Valencia/València': 'green', '29 Málaga': 'orange', 'Otras': 'lightgray'}\n\ndf_pivot = df_solo_provincias.pivot(index='periodo', columns='provincia', values='residentes')\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor provincia in df_pivot.columns:\n    color = colors[provincia] if provincia in provincias_destacadas else colors['Otras']\n    label = provincia if provincia in provincias_destacadas else '_nolegend_'\n    df_pivot[provincia].plot(ax=ax, color=color, label=label)\n\nplt.title('Evolución de la población por provincias en España')\nplt.ylabel('Residentes (millones)')\nplt.xlabel('')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nSuerte con pillar el ritmo a la demanda."
  },
  {
    "objectID": "posts/2022-09-03-el-lecho-de-procusto/index.html",
    "href": "posts/2022-09-03-el-lecho-de-procusto/index.html",
    "title": "27 aforismos de Nassim Taleb",
    "section": "",
    "text": "Selección personal de aforismos de Nassim Taleb, publicados en El Lecho de Procusto.\n\nLa gente está mucho menos interesada en lo que tratas de mostrarle que en lo que tratas de esconder.\nEl trabajo te destruye el alma invadiéndote furtivamente el cerebro durante las horas que oficialmente no dedicas a trabajar. Sé selectivo con las profesiones.\nNo hay un estado intermedio entre el hielo y el agua. Pero sí lo hay entre la vida y la muerte : un empleo.\nNadie quiere ser completamente transparente; no para los demás y, desde luego, no para sí mismo.\nFrancia se hizo con Argelia confiando en que el país comiera cassoulette y resulta que, ahora, Francia come cuscús.\nPreguntamos “¿por qué la crisis es tan profunda?” y no “¿por qué la crisis no es más profunda?”\nDejar que otros ganen las batallas pequeñas es una manipulación muy poderosa.\nMe pregunto cuántas personas buscarían riqueza excesiva si no conllevara una métrica de estatus.\nLa diferencia entre amor y felicidad es que los que hablan de amor tienden a estar enamorados y los que hablan de felicidad tienden a no ser felices.\nA la larga es más fácil que te engañes a ti mismo que engañes a otros.\nHay dos clases de personas: las que intentan ganar discusiones y las que intentan ganar. Nunca son las mismas.\nLa burocracia es una construcción diseñada para maximizar la distancia entre quien toma las decisiones y los riesgos de una decisión.\nLas tres adicciones más perjudiciales son la heorina, los carbohidratos y un sueldo mensual.\nMe pregunto si un león o un caníbal pagaría un recargo elevado por un humano criado en libertad.\nUna heurística sobre si tienes el control de tu vida: ¿puedes echarte siestas?\nUno de los libros más breves que he leído tenía 745 páginas. El libro más largo que he leído tenía 205 páginas.\nSea cual sea el tema, si no sientes que no sabes lo suficiente es que no sabes lo suficiente.\nLa tragedia es que gran parte de lo que consideras aleatorio está bajo tu control y, lo que es peor, también sucede la inversa.\nMi mayor problema con la modernidad quizá resida en la creciente separación entre lo ético y lo legal.\nTu deber es gritar las verdades que se deberían gritar pero solo se susurran.\nVirtud es cuando los ingresos que deseas mostrar a la agencia Tributaria superan a los que deseas mostrar a tu vecino.\nEl problema con la idea de aprender de los propios errores es que la mayor parte de lo que la gente llama errores no lo son.\nPara el robusto, un error es información; para el frágil, un error es un error.\nÁnclate en lo que no ha sucedido en lugar de en lo que ha sucedido.\nContra la creencia imperante, éxito no significa estar en la cima de una jerarquía. Significa estar fuera de todas las jerarquías.\nSi algo parece irracional, y lo ha sido por mucho tiempo, es posible que tengas una definición equivocada de la racionalidad.\nLo que cuenta no es lo que la gente dice de ti, sino la energía que gasta para decirlo.\n\nLinks de afiliado: - Comprar libro de papel. - Comprar libro electrónico."
  },
  {
    "objectID": "posts/2025-03-11-tiempos-lectura/index.html",
    "href": "posts/2025-03-11-tiempos-lectura/index.html",
    "title": "Tiempos de lectura de un fichero Excel",
    "section": "",
    "text": "Voy a intentar mejorar el tiempo de lectura de un fichero Excel."
  },
  {
    "objectID": "posts/2025-03-11-tiempos-lectura/index.html#preparación-de-datos",
    "href": "posts/2025-03-11-tiempos-lectura/index.html#preparación-de-datos",
    "title": "Tiempos de lectura de un fichero Excel",
    "section": "Preparación de datos",
    "text": "Preparación de datos\nNo tengo ninguno a mano, así que me lo invento. Genero un data frame con 1000 filas y 100 columnas y lo guardo en un fichero Excel.\nLas columnas serán de distintos tipos:\n\nLas 10 primeras columnas serán de tipo fecha.\nLas 10 columnas siguientes serán de tipo entero.\nLas 10 columnas siguienets serán de tipo character.\nLas demás columnas serán de tipo numérico.\n\n\nset.seed(123)\n\nn &lt;- 10000\nm &lt;- 100\n\ndf_fechas &lt;- data.frame(\n  lapply(1:10, function(i) {\n    as.Date(\"2025-01-01\") + sample(1:1000, n, replace = TRUE)\n  })\n)\n\nnames(df_fechas) &lt;- paste0(\"fecha_\", 1:10)\n\ndf_enteros &lt;- data.frame(\n  lapply(1:10, function(i) {\n    sample(1:1000, n, replace = TRUE)\n  })\n)\n\nnames(df_enteros) &lt;- paste0(\"entero_\", 1:10)\n\ndf_caracter &lt;- data.frame(\n  lapply(1:10, function(i) {\n    sample(letters, n, replace = TRUE)\n  })\n)\n\nnames(df_caracter) &lt;- paste0(\"caracter_\", 1:10)\n\ndf_numericos &lt;- data.frame(\n  lapply(1:(m - 30), function(i) {\n    rnorm(n)\n  })\n)\n\nnames(df_numericos) &lt;- paste0(\"numerico_\", 1:(m - 30))\n\ndf &lt;- cbind(df_fechas, df_enteros, df_caracter, df_numericos)\n\nwritexl::write_xlsx(df, \"~/Desktop/datos.xlsx\")"
  },
  {
    "objectID": "posts/2025-03-11-tiempos-lectura/index.html#prueba-1",
    "href": "posts/2025-03-11-tiempos-lectura/index.html#prueba-1",
    "title": "Tiempos de lectura de un fichero Excel",
    "section": "Prueba 1",
    "text": "Prueba 1\nAhora leo el fichero Excel con la función readxl::read_xlsx y cuento tiempos.\n\nlibrary(microbenchmark)\nlibrary(readxl)\n\nmicrobenchmark(\n  excel = readxl::read_excel(\"~/Desktop/datos.xlsx\"),\n  xlsx = readxl::read_xlsx(\"~/Desktop/datos.xlsx\"),\n  times = 10\n)\n\nUnit: milliseconds\n  expr      min       lq     mean   median       uq       max neval\n excel 818.9073 822.9448 859.8217 831.0066 932.1684  945.2294    10\n  xlsx 815.5541 844.9630 941.4324 855.8534 938.4878 1501.5206    10\n\n\nFlipo por primera vez porque dejar que averigüe la extensión es más rápido que especificársela."
  },
  {
    "objectID": "posts/2025-03-11-tiempos-lectura/index.html#prueba-2",
    "href": "posts/2025-03-11-tiempos-lectura/index.html#prueba-2",
    "title": "Tiempos de lectura de un fichero Excel",
    "section": "Prueba 2",
    "text": "Prueba 2\nAhora intento acelerar esto. Una opción es usar la función readxl::read_xlsx con el argumento guess_max.\n\nmicrobenchmark(\n  xlsx_1000 = readxl::read_xlsx(\"~/Desktop/datos.xlsx\"),\n  xlsx_10 = readxl::read_xlsx(\"~/Desktop/datos.xlsx\", guess_max = 10),\n  times = 10\n)\n\nUnit: milliseconds\n      expr      min       lq     mean   median       uq       max neval\n xlsx_1000 810.2359 828.9868 891.8215 887.9481 942.6632 1013.5234    10\n   xlsx_10 819.4436 830.5079 872.8056 848.1881 908.9467  987.3453    10\n\n\nFlipo por segunda vez porque dejar que adivine con 10 filas es más lento que dejarle que adivine con 1000."
  },
  {
    "objectID": "posts/2025-03-11-tiempos-lectura/index.html#prueba-3",
    "href": "posts/2025-03-11-tiempos-lectura/index.html#prueba-3",
    "title": "Tiempos de lectura de un fichero Excel",
    "section": "Prueba 3",
    "text": "Prueba 3\nAhora voy a especificar los tipos de columnas en el argumento col_types.\n\ncol_types &lt;- c(\n  rep(\"date\", 10),\n  rep(\"numeric\", 10),\n  rep(\"text\", 10),\n  rep(\"numeric\", m - 30)\n)\n\nmicrobenchmark(\n  xlsx_1000 = readxl::read_xlsx(\"~/Desktop/datos.xlsx\"),\n  xlsx_col_types = readxl::read_xlsx(\"~/Desktop/datos.xlsx\", col_types = col_types),\n  times = 10\n)\n\nUnit: milliseconds\n           expr      min       lq     mean   median       uq      max neval\n      xlsx_1000 819.3382 826.5695 860.4852 834.8801 854.2142 989.9864    10\n xlsx_col_types 818.9725 829.1812 855.8303 846.0230 854.6138 923.9795    10\n\n\nY me quedo flipando aún más porque lo de especificar col_types apenas ayuda (de hecho, si el número de filas es 1000 en lugar de 10000, empeora el tiempo; por lo menos, en una prueba que he hecho pero no he publicado).\n\nMira, esto tiene trampa. El fichero Excel con el que estoy probando es pequeño. Si pruebas con un fichero grande, tipo 500.000 filas, o 1.000.000, es posible que los resultados sean más razonables.\nYo no lo hago porque mi ordenador peta. Pero lo puedes probar tú. Seguramente sí consigas en ese caso mejoras según dicta la intuición. Ahora bien, no creo que sean mejoras enormes.\nSon mejoras que, si tienes que leer el fichero una vez, te van a dar igual. Te pueden venir bien si tienes muchos ficheros que leer, de forma que te ahorres 1 segundo por fichero y, en suma, aporte."
  },
  {
    "objectID": "posts/2023-02-19-hollywood-age-gaps/index.html",
    "href": "posts/2023-02-19-hollywood-age-gaps/index.html",
    "title": "Diferencia de edad entre parejas sentimentales en películas",
    "section": "",
    "text": "El tidytuesday del 14 de febrero de 2023 trae datos de diferencias de edad entre parejas sentimentales en películas de Hollywood.\nDiferencia de edad en las 40 películas con más diferencia:\n\n\n\n\n\n\n\n\n\nLas décadas con mayor diferencia son las que menos muestra tienen.\n\n\n\n\n\n\n\n\n\nSelecciono las 40 películas con más diferencia relativa con respecto al actor más joven."
  },
  {
    "objectID": "posts/2023-07-18-muestreo/index.html",
    "href": "posts/2023-07-18-muestreo/index.html",
    "title": "Muestreo (I)",
    "section": "",
    "text": "Vi en Twitter esto:\n¿Es inútil el muestreo?\nSi lo haces mal, sí. Como todo.\nRepaso algunos conceptos relacionados con el muestreo, sacados principalmente del curso de Sampling with R de Datacamp."
  },
  {
    "objectID": "posts/2023-07-18-muestreo/index.html#de-qué-sirve-el-muestreo",
    "href": "posts/2023-07-18-muestreo/index.html#de-qué-sirve-el-muestreo",
    "title": "Muestreo (I)",
    "section": "De qué sirve el muestreo",
    "text": "De qué sirve el muestreo\nAntes de las elecciones se hacen encuestas sobre intención de voto. ¿Qué se te viene a la mente con estas encuestas?\nPues que nunca te preguntan a ti.\n(Sí, ya, justo quizá a ti sí, una vez… pero no suele ocurrir. Lo normal es que nunca te hayan preguntado).\nA la mayoría de las personas no les preguntan nada.\nPero un muestreo adecuado puede hacer que eso tenga valor.\nEs más, un muestreo puede ser tu única opción para estudiar unos datos.\n\nEn un caso como el de las encuestas, no tienen recursos para encuestar a todos los votantes de un país como, pongamos, España (unos 37 millones de personas)\nEn un análisis de sangre, no estudian todos los litros de sangre de tu cuerpo: te sacan una muestra y generalizan (sacarte toda la sangre es peligroso: no lo intentes).\nSi tienes una base de datos con millones de clientes, algún algoritmo de machine learning puede suponer mucha carga de trabajo para tu ordenador (no todos tenemos presupuesto para contratar servidores enormes en Amazon). )\n\n¿Tiene sentido siempre el muestreo? No. Si estás corrigiendo unos exámenes de unos alumnos, no tiene sentido que corrijas un porcentaje y extrapoles los demás por ahorrarte el tiempo."
  },
  {
    "objectID": "posts/2023-07-18-muestreo/index.html#muestreo-aleatorio-sistemático",
    "href": "posts/2023-07-18-muestreo/index.html#muestreo-aleatorio-sistemático",
    "title": "Muestreo (I)",
    "section": "Muestreo aleatorio sistemático",
    "text": "Muestreo aleatorio sistemático\nSi has estudiado algo de estadística, te habrás topado con el muestreo aleatorio. Consiste en seleccionar las observaciones para una muestra sin ningún criterio en concreto: esta sí, esta no, esta sí, esta también porque sí, esta no, esta tampoco…\nMe gusta un conjunto de datos con métricas de pingüinos disponible en cualquier software estadístico que se precie.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\nUna de las métricas que tiene es la longitud del pico, en milímetros.\n\n\n\n\n\n\n\n\n\n¿Cómo cambia eso si lo dibujas sobre una muestra aleatoria de los pingüinos?\nLo primero es elegir la muestra aleatoria. Supón que etiquetas a cada uno como 1, 2, 3… así hasta 344, que son todos los pingüinos que tienes.\nEn el siguiente gráfico tienes coloreados en azul los pingüinos que eliges para tu muestra (50).\n\n\n\n\n\n\n\n\n\n¿Cómo cambia la distribución de la longitud del pico en esta muestra?\n\n\n\n\n\n\n\n\n\nHemos perdido valores extremos pero se mantiene una forma parecida.\nUn tipo diferente de muestreo es el sistemático, que consiste en hacer una selección basada en un criterio con respecto a la posición de la observación en el conjunto de datos. En el siguiente gráfico, pinto en azul al pingüino 1, 7, 13, etc… o sea, selecciono al primer pingüino de cada 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAparte del caso extremo, la distribución no ha cambiado mucho, en parte porque la distribución de las observaciones no sigue ningún orden en concreto. Si tu tabla de datos sigue un orden, deberás valorar si ese orden afecta al sistema que eliges para muestrear tus datos."
  },
  {
    "objectID": "posts/2023-07-18-muestreo/index.html#muestreo-estratificado",
    "href": "posts/2023-07-18-muestreo/index.html#muestreo-estratificado",
    "title": "Muestreo (I)",
    "section": "Muestreo estratificado",
    "text": "Muestreo estratificado\nEn el conjunto de datos tenemos tres especies de pingüinos.\n\n\n\n\n\n\n\n\n\nAlgo habitual en machine learning forzar a mantener esa proporción diferente de especies en la muestra elegida, para que el conjunto de datos de trabajo mantenga esa distribución original.\n¿Pero qué pasa si no quieres que así sea?\nImagina que estás estudiando si una medicación es efectiva o no. Si mantienes la proporción original, puede ser que de la especie menos representada (Chinstrap) te quedes casi sin muestra.\nEso te puede venir mal, porque tú necesitas que haya muestra de todas las especies, para saber si en una especie el medicamento es más o menos efectivo.\nEn un caso así, no querrías un muestreo estratificado, sino que forzarías incluso a que en la muestra, todos los grupos estuvieran igualmente representados."
  },
  {
    "objectID": "posts/2023-07-18-muestreo/index.html#cómo-afecta-el-tamaño-de-la-muestra",
    "href": "posts/2023-07-18-muestreo/index.html#cómo-afecta-el-tamaño-de-la-muestra",
    "title": "Muestreo (I)",
    "section": "Cómo afecta el tamaño de la muestra",
    "text": "Cómo afecta el tamaño de la muestra\nCuando tienes a mano la población total, puedes comparar los cálculos que hagas en tu muestra con los de la población (para cuando no tengas la población, te hablaré de esto en unos días).\nEn el caso de los pingüinos, puedo calcular la media de la longitud del pico para la población: 43.9219298.\nO también para la muestra que calculé antes:\n\n\n# A tibble: 1 × 1\n  media_muestral\n           &lt;dbl&gt;\n1           44.6\n\n\n¿Son muy distintas?\nUna medida de esta diferencia es el error relativo:\n\\[\\frac{|media_{pop} - media_{sample}|}{media_{pop}}\\]\n¿Cuál es la gracia?\nPues que si tu tamaño muestral es muy pequeño, no hay ninguna garantía de que la media (o lo que estés midiendo) se parezca a la poblacional:\n\n\n# A tibble: 1 × 1\n  media_muestral\n           &lt;dbl&gt;\n1           45.8\n\n\nPero si dejas que el tamaño muestral crezca, tu cálculo se estabilizará.\n\n\n\n\n\n\n\n\n\nEn unos días te escribo sobre cómo aproximar esto cuando no tienes acceso a la media poblacional, pero que puedas valorar si tu muestra es adecuada o no."
  },
  {
    "objectID": "posts/2024-12-11-contraste-bayesiano/index.html",
    "href": "posts/2024-12-11-contraste-bayesiano/index.html",
    "title": "Contraste de medias, estilo bayesiano",
    "section": "",
    "text": "Los contrastes de hipótesis me parecen de lo más contraintuitivo en el mundo frecuentista.\nEl resultado que da la estadística bayesiana es mucho más natural y fácil de interpretar (frase de Copilot)\nSimulo unos datos y luego voy a intentar contrastar si su la media de los datos es la que creo que es.\n\nlibrary(rethinking)\nlibrary(ggplot2)\n\nn_data &lt;- 10000\npop_mean &lt;- 25\npop_sd &lt;- 5\nreal_data &lt;- rnorm(n_data, mean = pop_mean, sd = pop_sd)\n\nggplot() + \n  geom_histogram(aes(real_data), binwidth = 0.3, fill = \"#800080\")\n\n\n\n\n\n\n\n\nPongo de prioris los datos que ya sé válidos:\n\nfit1 &lt;- quap(\n    alist(\n        y ~ dnorm(mu, sigma), \n        mu &lt;- a, \n        a ~ dnorm(pop_mean, 2), \n        sigma ~ dnorm(pop_sd, 1)\n    ),\n    data = list(y = real_data)\n)\n\nprecis(fit1)\n\n           mean         sd      5.5%    94.5%\na     24.987641 0.05066636 24.906666 25.06862\nsigma  5.068262 0.03581411  5.011025  5.12550\n\n\nIncluso con unos prioris malísimos, el resultado es el correcto (y más fácil de interpretar).\n\nfit2 &lt;- quap(\n    alist(\n        y ~ dnorm(mu, sigma), \n        mu &lt;- a, \n        a ~ dnorm(0, 5), \n        sigma ~ dexp(1)\n    ), \n    data = list(y = real_data)\n)\n\nprecis(fit2)\n\n           mean         sd      5.5%     94.5%\na     24.985067 0.05066808 24.904089 25.066044\nsigma  5.067067 0.03581597  5.009826  5.124308"
  },
  {
    "objectID": "posts/2023-10-20-piramide-de-poblacion/index.html",
    "href": "posts/2023-10-20-piramide-de-poblacion/index.html",
    "title": "Pirámide de población española, desde 1971",
    "section": "",
    "text": "Te traigo una película de terror, llamada La quiebra de las pensiones en España.\nEs un GIF que he hecho con la librería ggplot2 de R, y gganimate.\nSi quieres el código, me lo pides y te lo mando (o lo buscas en mi GitHub).\nLos datos vienen del INE.\nEn el gráfico aparecen unos valores missing en un momento dado, pero ni idea de por qué, porque dato hay.\n\n\n[1] \"\"\n\n\n\nPuede serte más práctico tener todos los gráficos seguidos, en lugar de quedarte mirando un GIF.\nAquí los tienes:\n\n\n\nPirámides de población desde 1971 hasta 2021"
  },
  {
    "objectID": "posts/2023-09-28-fooled-by-randomness/index.html",
    "href": "posts/2023-09-28-fooled-by-randomness/index.html",
    "title": "¿Existe la suerte?",
    "section": "",
    "text": "Probabilidad: falta de certidumbre en nuestro conocimiento y el desarrollo de métodos para lidiar con nuestra ignorancia. en el mundo real muchas veces tienes que averiguar el problema más que la solución.\nDe la misma forma que un antepasado se rescaba la nariz y veía llover, pensaba que el rascarse la nariz inducía lluvia, acualmente ligamos la prosperidad económica a un recorte de los bancos centrales.\nPresentamos conjeturas como verdades porque no manejamos bien las probabilidades.\nLo que obtienes por suerte es fácil que te desaparezca por azar. Lo que obtienes sin la cción de la suerte es más robusto a la incertidumbre.\nDos problemas ante un estado:\n\nLos cisnes negros.\nLa simetría (de poco sirven muchos éxitos si un fracaso te lo quita todo)\n\nExperimento s xix. Paciente amnésica, hay que presentarse cada 15 minutos. Un día, estrechan la mano con una chincheta. Al día siguiente, la paciente no recuerda al interlocutor pero retira la mano de manera instintiva.\nNos fijamos en los éxitos (como tecnologías que han cambiado nuestra vida) pero no en los fracasos (como patentes que se han quedado en el olvido)\nEl ratio de información no verificada frente a la verificada aumenta.\n\nMisma idea de la señal y el ruido de Nate Silver.\n\nEn la Italia renacentista, las aseguradoras cobraban la misma tasa a un varón de 20 años que a uno de 50, porque su vida extra esperada era la misma (alguien que sobrepasaba los 40 había demostrado resistencia a muchos obstáculos y posiblemente ya llegaría a viejo).\nLo que uno observa en los mercados financieros es, en el mejor de los casos, varianza más retornos, no solo retornos.\nCentrarse en la aleatoriedad, en el corto plazo, desgasta, quema, frustra y deprime, por la constante exposición a sus golpes de dolor.\nmy problem is that I’m not rational and I’m extremely prone to drown in randomness and to incur emotional torture. My only advantage in life is that i know some of my weaknesses, mostly that I’m incapable of taming my emotions facing news and incapable of seeing a performance with a clear head. Silence is far better.\nFashionable nonsense, by alan sokal\nand such a crowd (y esta banda)\nsi lo siguiente se parece a un discurso de un mandamás de tu empresa, búscate otro trabajo:\n\n\nWe look after our customer’s interests/the road ahead / our assets are our people / creation of shareholder value / our vision / our expertise lies in / we provide interactive solutions / we position ourselves in this market / how to serve our customers better / short- term pain for long-term gain/we will be rewarded in the long run / we play from our strength and improve our weaknesses / courage and determination will prevail / we are committed to innovation and technology / a happy employee is a productive employee / commitment to excellence / strategic plan / our work ethics.\n\n\nLe preguntaron en un evento si creía que el mercado iba a subir o bajar en la próxima semana. Dijo que subiría. Y respondieron que entonces por qué estaba en corto. Por asimetría. Él creía que había mucha probabilidad de que subiera (pero muy poco, solo un 1%). - Pero había una probabilidad de 30% de bajar 10%. Con todo, el resultado esperado era una bajada.\nwe could either be too lax or too stringent in accepting past information as a prediction of the future. I need a lot more than data. My major reason is the rare event, but i have plenty of others.\nla historia nos enseña que cosas que nunca han ocurrido ocurren.\nTiendes a ser más sensible a la presencia o ausencia de un estímulo que a la variación en su magnitud. Esto implica (en mercados financieros) que una pérdida es al principio solo una pérdida, con drásticas consecuencias a la larga\nEmocionalmente, prefieres muchas ganancias y pocas pérdidas, aunque en magnitud eso no tenga que ser bueno.\nEn ciencia, puedes estar haciendo experimentos fallidos muchas veces, hasta que una por fin sale un resultado exitoso.\nLa serie temporal de éxito puede parecer plana con un pico al final, pero se puede ver como una probabilidad acumulada (relación con James Clear)\nEsto mismo se puede aplicar al mercado editorial, a entrenamiento físico (basado en el ritmo de cazadores recolectores del paleolítico) o inversión para aprovechar crisis.\nLa estadística nos falla cuando tenemos distribuciones asimétricas: imagina una urna con bolas rojas y negras, pero en la que sacar bola roja tiene una probabilidad muy muy baja. Si quiero saber si hay bolas rojas y mi método consiste en ir sacando bolas (por probabilidad, serán casi siempre negras), el aumentar el tamaño muestral no me ayuda a estar más seguro de si hay bolas rojas o no si siempre saco bolas negras. Y si saco una roja, aumentar el tamaño muestral tampoco me aporta más información.\nLo importante de esto es que aumentar progresivamente el tamaño muestral no garantiza que aumente la información\nRobert Lucas: si la gente fuera racional, entonces aplicarían es razón para entender patrones del pasado y adaptarse, de forma que la información pasada seria inutil para predecir el futuro.\nAprende a gestionar la aleatoriedad manteniendo una mente abierta y cambiando de opinión sin que te dé vergüenza.\nUsaré la estadística cuando me beneficie y la evitaré cuando me perjudique. Aprenderé del pasado pero sin caer en sus peligros. Por ello usaré la estadística para hacer apuestas agresivas (estadísticamente, qué probabilidad hay de que algo poco probable pero beneficioso ocurra) pero no la usaré para protegerme frente a riesgos (que algo malo históricamente nunca haya ocurrido no significa que no vaya a ocurrir).\nEl coste de estar equivocado en una apuesta tiene que ser asumible y estar limitado.\nSi juntas a infinitos monos frente a un teclado, alguno acabará tecleando la Iliada. Ahora bien, ¿cuál es la probabilidad de que ese mono esceiba a continuación la Odisea? Dicho de otra forma, ¿hasta qué punto el rendimiento pasado es un buen indicador del rendimiento futuro?\nDe cara a valorar una empresa, si tienes muchas empresas, es probable que alguna lo esté haciendo bien por casualidad, no por capacidad. ¿Cómo sabes cuál es para no invertir en ella? y en inversiones, ves a inversores que les sale bien su inversión, pero ¿cómo sabes que no es por casualidad? Además, solo ves a los buenos. Los fracasados están escondidos y no conoces la probabilidad de ser bueno.\nEl sesgo del superviviente es habitual porque estamos acostumbrados a aprovechar la información que tenemos delante de nosotros e ignorar la que no vemos.\nNadie acepta la aleatoriedad en su éxito; solo en su fracaso.\nSi la nariz de Cleopatra hubiera sido distinta el mundo sería totalmente diferente (porque Julio César quedó prendado por su nariz). - (Pascal, ejemplo de caos)\nwhat has gone wrong with the development of economics as a science? Answer: there was a bunch of inteligent people who felt compelled to use mathematics just to tell themselves that they were rigorous in their thinking, that theirs was a science.\nEl camino hacia el éxito no tiene por qué depender de la suerte, sino de la constancia. La constancia en un camino sin frutos es la que acaba llevándote al evento raro, al momento en que ves que has avanzado y progresado. El camino hacia el éxito puede no ser alestorio, pero muy poca gente tiene la entereza de seguirlo hasta el final. Aquellos que dan el paso extra son los recompensados.\nMe estoy dando cuenta del efecto no lineal tras el éxito en cualquier campo. Es mejor tener unos pocos entusiastas de tu trabajo que muchos seguidores a los que les pareces bien. Mejor ser amado por una docena que gustar a cientos.\nEl valor esperado es aquel en el que si te quieres ir de vacaciones a París o al Caribe, puedes imaginarte a la vez tumbado en una playa paridisíaca visitando el museo de Orsay. O sea, algo imposible pero que se usa una y otra vez para cuantificar escenarios. las reglas tienen su utilidad. No las seguimos porque sean correctas, sino porque nos ahorran tiempo y esfuerzo (por ejemplo, salir corriendo cuando ves algo que pueda ser un depredador, lo sea o no lo sea).\nSesgos cognitivos: si vemos un número, tendemos a anclarnos a él en nuestras estimaciones. Las emociones son atajos para tomar decisiones. Una decisión puramente matemática requeriría tiempo infinito por la cantidad de información que hay que procesar.\nSi estás pensando en equipaje para un viaje, la varianza en la temperatura puede condicionarte más que el valor esperado.\nNo soy tan inteligente ni tan fuerte como para controlar mis emociones. Es más, las necesito para tomar decisiones y ejecutarlas.\nSi mides una mesa con una regla de cuya precisión no te fías, obtienes más información de la regla que de la mesa. Si lees una valoración de un usuario al que no conoces, obtienes más información del usuario que del contenido revisado. tratado sobre la risa, henri Bergson\nA los 25, ignoraba totalmente la ciencia del comportamiento. Me habían hecho creer erróneamente en mi juventud que mis supersticiones eran culturales, y que por ello podía deshacerme de ellas mediante el uso del raciocinio.\nLa ciencia está bien; los científicos son humanos. Un científico puede tener que actuar como un picapleitos defensor de una causa sin sentido en lugar de limitarse a buscar la verdad.\nNo culpes a otros por tu destino (incluso aunque tengan algo de culpa).\nLo único de ti sobre lo que no tiene control la suerte es tu comportamiento.\nCuanto más asciendes profesionalmente, más te pagan. Esto puede estar justificado, ya que tiene sentido pagar a un trabajador acorde a su contribución. Sin embargo, cuanto más asciendes, la evidencia de esta contribución disminuye.\nUn empleado en un nivel bajo en la jerarquía (un perfil técnico) se le evalúa tanto por sus resultados como por su metodología. A medida que asciende, se le evalúa más por resultados. En niveles altos, cómo llegar a buenos resultados da igual, con tal de que se llegue a buenos resultados. Pero el jefe de una gran compañía se puede enfrentar a unas cuantas grandes decisiones cuyo método no se puede repetir (porque las circunstancias del mercado y de la empresa no se repitan). Así, “ser CEO” puede no requerir unas habilidades especiales para conseguir buenos resultados, ya que estos dependen en gran medida de que la empresa esté en las circunstancias adecuadas. Y lo que requiere realmente es tener el carisma y las habilidades sociales correspondientes para caer bien, sin más.\nSi algo se rige por normas aleatorias, viene a funcionarte de la misma forma que si no lo conocieras.\nValoramos lo visible, lo asentado, lo personal, lo narrado, lo medible, lo tangible; desdeñamos lo abstracto."
  },
  {
    "objectID": "posts/2023-12-22-survival-analysis/index.html",
    "href": "posts/2023-12-22-survival-analysis/index.html",
    "title": "Introducción a análisis de supervivencia con R",
    "section": "",
    "text": "El nombre del análisis de supervivencia apunta maneras. Inevitablemente, yo lo relaciono con la muerte, porque seguramente se originara para saber cuánto tiempo le quedaba de vida a un paciente con alguna enfermedad.\nPero no sirve solo para eso.\nAlgunos ejemplos de casos que puedes resolver con análisis de supervivencia son:"
  },
  {
    "objectID": "posts/2023-12-22-survival-analysis/index.html#teoría-del-análisis-de-supervivencia",
    "href": "posts/2023-12-22-survival-analysis/index.html#teoría-del-análisis-de-supervivencia",
    "title": "Introducción a análisis de supervivencia con R",
    "section": "Teoría del análisis de supervivencia",
    "text": "Teoría del análisis de supervivencia\nLos modelos de supervivencia buscan modelizar la función de supervivencia, dada por esta ecuación:\n\\[S(t) = 1 - F(t) = P(T &gt; t)\\]\nViene a interpretarse como la probabilidad de que la duración del experimento sea mayor de \\(t\\). \\(F(t)\\) representa la función de distribución del evento.\n\n\n\n\n\n\n\n\n\nEsa gráfica ya daría algo de información. Por ejemplo:\n\n\n\n\n\n\n\n\n\n\nSabemos que la probabilidad de la probabilidad de superar el instante \\(t = 3\\) es de 0,97.\nSabemos que el tercer cuartil es 5,65.\n\nEl objetivo del análisis de supervivencia es estimar esta función de supervivencia."
  },
  {
    "objectID": "posts/2023-12-22-survival-analysis/index.html#el-método-keplen-meier-para-estimar-la-función-de-supervivencia",
    "href": "posts/2023-12-22-survival-analysis/index.html#el-método-keplen-meier-para-estimar-la-función-de-supervivencia",
    "title": "Introducción a análisis de supervivencia con R",
    "section": "El método Keplen-Meier para estimar la función de supervivencia",
    "text": "El método Keplen-Meier para estimar la función de supervivencia\nLa fórmula que usa este método para estimar la función de supervivencia en \\(t\\) es:\n\\[S(t) = \\prod_{i: t_i\\leq t}\\frac{n_i - d_i}{n_i}\\]\ndonde \\(t_i\\) representa a cada uno de los instantes anteriores al instante \\(t\\), \\(n_i\\) es el número de individuos en \\(t_i\\) y \\(d_i\\) es el número de defunciones en \\(t_i\\).\nLo podemos ver mejor con numeritos y este gráfico:\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{S}(t) =  \\prod_{i: t_i\\leq t}\\frac{n_i - d_i}{n_i} \\\\\n\\] \\[\n\\hat{S}(2) = \\frac{5-0}{5} = \\frac{5}{5} = 1 \\\\\n\\] \\[\n\\hat{S}(3) = \\frac{4-0}{4} = \\frac{4}{4} = 1 \\\\\n\\] \\[\n\\hat{S}(4) = \\frac{4-2}{4} = \\frac{2}{4} = 0,5 \\\\\n\\] \\[\n\\hat{S}(5) = \\frac{1}{2} \\cdot \\frac{2-1}{2} = \\frac{1}{4} = 0,25 \\\\\n\\] \\[\n\\hat{S}(6) = \\frac{1}{4} \\cdot \\frac{1-0}{1} = \\frac{1}{4} = 0,25\n\\]\nAunque después de avanzar un poco con el curso, me entero de que Keplen-Meier se usa principalmente con fines descritivos."
  },
  {
    "objectID": "posts/2023-12-22-survival-analysis/index.html#método-weibull",
    "href": "posts/2023-12-22-survival-analysis/index.html#método-weibull",
    "title": "Introducción a análisis de supervivencia con R",
    "section": "Método Weibull",
    "text": "Método Weibull\nUn método que sí se usa para estimar la curva es el de Weibull. Sinceramente, ni idea de cómo funciona.\nPero una forma directa de ajustar y visualizarlo es esta (para visualizar simplifica mucho si usas la librería survminer):\n\nlibrary(survival)\nwb &lt;- survreg(Surv(time, status) ~ 1, lung)\n# Retrieve survival curve from model\nsurv &lt;- seq(.99, .01, by = -.01)\n\n# Get time for each probability\nt &lt;- predict(wb, type = \"quantile\", p = 1 - surv, newdata = data.frame(1))\n\n# Create data frame with the information needed for ggsurvplot_df\nsurv_wb &lt;- data.frame(time = t, surv = surv, \n  upper = NA, lower = NA, std.err = NA)\n\n# Plot\n\nsurvminer::ggsurvplot_df(fit = surv_wb, surv.geom = ggplot2::geom_line)"
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html",
    "title": "Rompe la barrera del no",
    "section": "",
    "text": "Escuchar es la concesión menos costosa y más eficaz que podemos hacer.\nhay que discernir entre deseos (aspiraciones) y necesidades ( el mínimo acuerdo).\nLos seres humanos somos irracionales.\nLa vida es negociar."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#las-nuevas-reglas",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#las-nuevas-reglas",
    "title": "Rompe la barrera del no",
    "section": "",
    "text": "Escuchar es la concesión menos costosa y más eficaz que podemos hacer.\nhay que discernir entre deseos (aspiraciones) y necesidades ( el mínimo acuerdo).\nLos seres humanos somos irracionales.\nLa vida es negociar."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#sé-un-espejo",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#sé-un-espejo",
    "title": "Rompe la barrera del no",
    "section": "2. Sé un espejo",
    "text": "2. Sé un espejo\n\nAl entrar en una negociación, tu objetivo debe ser sonsacar toda la información posible. Si entras creyendo que ya lo sabes todo, la negociación no te saldrá bien.\nEl fbi tenía equipos de escucha. Cada miembro se encargaba de entender una parte de la información sobre el secuestrador. Escuchar bien no es fácil porque nuestros sesgos nos llevan a buscar coherencia con nuestras asunciónws en lugar de la verdad.\ncinco pasos: uno, emplea un tono de locutor de radio de programa nocturno. Dos, empieza por un lo siento. Tres, usa el reflejo (repite las dos o tres últimas palabras de lo que dice tu interlocutor). Cuatro, mantén silencio después del reflejo. Cinco, repite.\nLa negociación no es una batalla de argumentos sino un proceso de descubrimiento.\nLa otra persona debe ser tu único centro de atención.\nBaja el ritmo, no te apresures.\nSonríe. Incluso por teléfono te cambiará la voz (positivamente).\nTres tonos de voz: locutor de radio (útil para dejar un argumento claro y crear autoridad), positivo/alegre (el genérico, hay que relajarse y sonreír) y asertivo (rara vez se usa)."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#no-sientas-su-dolor.-etiquétalo",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#no-sientas-su-dolor.-etiquétalo",
    "title": "Rompe la barrera del no",
    "section": "3. No sientas su dolor. Etiquétalo",
    "text": "3. No sientas su dolor. Etiquétalo\n\nLas emociones son el medio para llegar a la información, es decir, a lo que el interlocutor necesita.\nIgual que el psicólogo, tienes que hurgar en las emociones del paciente para hacer aflorar sus problemas. Luego devuelves la respuesta para que el paciente siga profundizando y modifique su comportamiento.\nHay que hablar poco y escuchar mucho.\nEn una negociación te puedes hacer el tonto o responder “no entiendo”. Pero no puedes ignorar a la otra parte —disminuye las probabilidades de que hagan lo que queremos que hagan porque no se genera confianza.\nLa empatía es la habilidad de reconocer la perspectiva de tu interlocutor y la vocalización de ese reconocimiento. O sea, escuchas a la otra persona, entiendes lo que le pasa y lo repites.\n\nNo es estar de acuerdo ni apoyar. Es comprender.\n\nLa empatía táctica consiste en comprender sentimientos y estado de ánimo de otra persona y también escuchar lo que está detrás de esos sentimientos, de forma que puedas influir en lo que viene a continuación.\n\nEs pensar desde el punto de vista de otra persona y evaluar sus motivaciones.\n\nSi de verdad prestas atención a lo que otra persona está diciendo, puedes llegar a saber con antelación lo que va a decir.\n“Hay que mostrar respeto incluso a los enemigos. Intentar comprender y, en la medida de lo posible, empatizar con sus perspectivas y sus puntos de vista.” (Hillary Clinton)\nEtiqueta las emociones de la otra persona. Usa frases indirectas como “Parece que…”, “suena a que”,”da la sensación de que…”.\nEs incómodo porque parece ofensivo pero la gente no se suele dar cuenta de lo que estás haciendo.\nPresta atención a los cambios (físicos, gestos, posturas) de otra persona ante tus palabras.\nLos videntes estudian el lenguaje no verbal de sus clientes ante preguntas vagas y de ahí extraen lo que quieren oír.\nSi tu interlocutor no está de acuerdo con la etiqueta no pasa nada. Reculas: “No quise decir que sea eso lo que sucede. Solo que lo parece.”\nCuando exploras las objeciones con etiquetas, estas te permiten dar con la emoción primaria que causa el comportamiento de tu interlocutor. Esta emoción es la clave para resolver todo lo demás.\nEtiquetar los miedos hace que pierdan efecto.\nDecir “mira, soy un gilipollas” ante un error tuyo facilitará la interlocución con la otra persona.\nLos abogados defensores en su primera intervención enumeran todas las acusaciones y los puntos débiles que tendrán para defenderlo. Empiezan acentuando lo negativo (sacarse el aguijón).\nEl primer paso es echarte en cara todas las cosas terribles de las que tu interlocutor pueda acusarte: autoacusación."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#cuidado-con-el-sí.-domina-el-no",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#cuidado-con-el-sí.-domina-el-no",
    "title": "Rompe la barrera del no",
    "section": "4. Cuidado con el “sí”. Domina el “no”",
    "text": "4. Cuidado con el “sí”. Domina el “no”\n\nUna trampa en la que muchos caen es en tomarse lo que dicen los demás en sentido literal.\nUn no es el comienzo de una conversación. Eliminas lo que no Interesa a tu interlocutor y eso te da más información que un sí falso o un quizá.\nCuando dices no a una propuesta, estás más predispuesto a escuchar lo que te ofrecen porque ya Has dejado clara tu posición.\nDeberías ver el no como una decisión temporal, no suele ser una decisión racional, y sirve para mantener el status quo.\nEn de entrada diga no, el autor plantea que tienes que dejar decir no al principio, ya que tu adversario quiere defenderlo a toda costa. Si se lo pones en bandeja al principio de la conversación, ya no le queda nada que defender.\nNo” puede significar muchas cosas: aún no estoy preparado, Estás haciéndome sentir incómodo, no lo entiendo, no creo que pueda permitírmelo, quiero otra cosa, Necesito más información, quiero hablarlo con otra persona.\nHay tres clases de sí: de engaño, de confirmación y de compromiso.\nProvoca un no con un correo: ¿se ha dado por vencido con este proyecto?"
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#cómo-provocar-las-dos-palabras-que-transformarán-inmediatamente-cualquier-negociación",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#cómo-provocar-las-dos-palabras-que-transformarán-inmediatamente-cualquier-negociación",
    "title": "Rompe la barrera del no",
    "section": "5. Cómo provocar las dos palabras que transformarán inmediatamente cualquier negociación",
    "text": "5. Cómo provocar las dos palabras que transformarán inmediatamente cualquier negociación\n\nPasos para conseguir un “así es”. 1. Pausas efectivas. 2. Pequeños alientos. 3. Reflejar, repetir. 4. Etiquetar (“parece que…”). 5. Parafrasear, para mostrar que se está entendiendo. 6. Resumir\nCuando tu adversario dice así es, está aceptando tu discurso.\nUn así es es mejor que un sí."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#moldea-su-realidad",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#moldea-su-realidad",
    "title": "Rompe la barrera del no",
    "section": "6. Moldea su realidad",
    "text": "6. Moldea su realidad\n\nEn todas las negociaciones, el que parece el débil tiene siempre un punto de ventaja, porque todos tenemos puntos ciegos irracionales, necesidades ocultas y nociones inconscientes poco desarrolladas.\nLas fechas límites o la idea de lo justo puede hacer que una persona actúe de manera irracional y lo puedes aprovechar.\nNo hagas concesiones. La idea de “buscar un término medio con el que todos ganen” no tiene sentido. Si quieres ir con zapatos marrones y tu pareja te recomienda negros, no te pongas uno marrón y uno negro.\nNo hacer ningún trato es mejor que hacer un mal trato.\nSaber ceder se ha convertido en un bien moral.\nNunca te comprometas a llegar a un punto medio.\nEl tiempo juega un papel clave para ambas partes. Todos tendemos a actuar de manera impulsiva cuando se acerca el plazo, aunque haya sido decidido de manera arbitraria.\nLos plazos y las fechas límite son, con frecuencia, arbitrarios y casi siempre flexibles, y su incumplimiento rara vez desencadena las consencuencias que creemos que tendrá.\nCuando los negociadores comunican sus plazos a la contraparte, consiguen mejores acuerdos.\nEn una negociación no puedes pensar que el otro piensa como tú.\nLas decisiones van de la mano de las emociones. Antonio Damasio cuenta que personas con daños en la psrte del cerebro que se encarga de las emociones no pueden tomar decisiones. Pueden describir los pros y contras de una situación, pero no decidirse ante ella.\nJusto es una palabra muy poderosa. Tendemos a cumplir más acuerdos cuando sentimos que se nos trata justamente y tendemos a romperlos en caso contrario.\nLo malo es que no valoramos si algo es justo de acuerdo con su valor, sino basándonos en sesgos. La aversión a la pérdida hace que perder 20 euros se sienta como más injusto que lo justo que verías ganar 20 euros.\nQue alguien te acuse de injusto hará que te sientas insultado. Este sentimiento, el sentirnos injustos, nos llevará a hacer concesiones irracionales.\nLa frase “solo quiero lo que es justo” puede hacer que tu interlocutor ceda. Cuidado:puede usarse como manipulación. Si te la plantean, discúlpate por haber sido injusto en algo concreto o aplica el reflejo: ¿justo?\n“Quiero que sientas en todo momento que te estoy tratando de forma justa, así que te pido por favor que si en algún momento crees que no lo estoy haciendo, me lo digas, y veremos cómo resolverlo”\nCuando tengas una propuesta, no puedes venderla con argumentos racionales sino llegando a los sentimientos de tu interlocutor. Una niñera no vende cuidado de los niños si una noche de relax para los padres.\nAunque nuestras decisiones sean irracionales, existen patrones. En general, tiendes a dar más valor a un objeto si es tuyo que si lo tuviera otra persona.\nPara ganar ventaja en una negociación tienes que demostrar que la otra persona perderá algo si el pacto se rompe.\nDebes dejar claro que sabes qué emociones tendrá tu interlocutor si pierde la oportunidad que le brinda el pacto. Aunque el pacto sea malo, no quieren perder lo que obtendrían.\nEs mejor ceder a la otra parte la iniciativa en negociaciones económicas, en general. Depende de la cantidad de información que cada parte tenga sobre la otra.\nOfrecer un rango en lugar de una cifra es más efectivo.\nNo todo es dinero. Piensa si tu contraparte puede ofrecerte algo que les salga barato pero a ti te aporte, como publicidad.\nNúmeros no redondos, como 34627, parece un presupuesto más inamovible que 35000.\nPregunta para una entrevista de trabajo, después de haber convencido a tu interlocutor de que serás un representante de sus ideas: qué hace falta para tener éxito aquí?\nEstás técnicas tienen sus retractores porque parecen manipulación. Son técnicas que de manera racional aprovechan la parte irracional de nuestras decisiones. Es la única forma de moldear la percepción de los demás.\nEl valor real de cualquier cosa depende del punto de vista del que lo observe."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#crea-una-ilusión-de-control",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#crea-una-ilusión-de-control",
    "title": "Rompe la barrera del no",
    "section": "7. Crea una ilusión de control",
    "text": "7. Crea una ilusión de control\n\nLa negociación no es un combate en el que nunca debas recular.\nLa clave en la negociación es que la otra persona llegue a sugerir la solución que a nosotros nos vale.\nTienes que hacer creer a tu oponente que es él quien controla la negociación.\nNo intentes hacer que el otro crea lo mismo que tú. Intenta anular su insistencia en no aceptar lo tuyo, pero sin convencerlo de manera directa.\nla mejor forma de montar a caballo es dirigirse en el sentido que quiera ir él.\nHaz que el oponente sienta que tiene el poder con preguntas que sirvan para pedir su ayuda.\nPreguntas con un planteamiento adecuado pueden convertir peticiones tuyas en dudas para las que necesitas la ayuda de tu interlocutor. Una pregunta puede hacer que tu interlocutor sienta que un problema tuyo es también suyo.\nNo deben permitir respuestas cerradas del tipo sí o no. Hay que usar preguntas del tipo wh periodístico. Las prioritarias son qué, cómo y por qué. Cuidado con la última porque a veces esconde una acusación. Funciona por ejemplo en “por qué ibais a cambiar vuestro proveedor de siempre para probar con nosotros?”\npor qué hiciste esto se puede cambiar a qué te movió a hacer esto.\nen qué sentido esto es algo importante para ti\ncómo puedo contribuir para hacer que esto sea mejor para ambos?\ncómo te gustaría que procediera?\nqué es lo que nos ha conducido hasta esta situación?\ncuál es el objetivo?\ncómo se supone que voy a conseguir eso?\nimplícitamente, debes convencer a tu interlocutor a base de preguntas que quieres lo mismo que él pero necesitas de su inteligencia para conseguirlo.\nEs imprescindible que controles tus emociones. Si no, cualquier técnica será en vano.\nNo contraataques. Responde a un ataque con una pregunta abierta.\nEl control de una conversación no lo tiene quien habla sino quien escucha."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#garantiza-la-ejecución",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#garantiza-la-ejecución",
    "title": "Rompe la barrera del no",
    "section": "8. Garantiza la ejecución",
    "text": "8. Garantiza la ejecución\n\nLlegar a un acuerdo no sirve de nada si la ejecución del mismo no está bien planteada.\npreguntas de cómo harán que la contraparte tenga que pensar en esa ejecución y ayudará a que crea que la solución final es idea suya.\nLas personas se esfuerzan más en llevar a cabo ideas que consideran suyas.\nLa negociación es el arte de hacer que los demás se salgan con la tuya.\nDos preguntas: cómo sabremos que vamos por el buen camino? Qué haremos si vemos que la cosa se tuerce? Cuando respondan, resume con tus palabras para obtener un así es.\nQue te digan tienes razón o lo intentaré son signos de que te están dando la. Razón como a los tontos, sin compromiso.\nMuchas veces las negociaciones no tienen tanto que ver con ganar o ahorrar un dinero sino con autoestima y estatus.\nEs importante entender al resto del equipo, aunque tú solo hables con tu interlocutor: “cómo afecta esto a los demás?”\nPresta siempre atención al tono de voz y, sobre todo, al lenguaje corporal. Si no se corresponden con el mensaje, seguramente sea porque es mentira.\nEtiquetar lo que ha dicho el otro (parece que) ayudará a confirmar si es mentira."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#regateo",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#regateo",
    "title": "Rompe la barrera del no",
    "section": "9. Regateo",
    "text": "9. Regateo\n\ntres perfiles de negociador: acomodador, asertivo y analista.\nLos analistas son minuciosos y detallistas pero pueden resultar fríos. Si eres analista, sonríe al hablar. Si hablas con un analista, plantea pocas preguntas de golpe y proporciona datos.\nLos acomodadores disfrutan con la relación. Pueden ofrecerte algo que no pueden cumplir simplemente por caer bien. Cuidado con el exceso de cháchara si eres uno.\nUn asertivo es directo y quiere que se le entienda. Hazle saber que entiendes lo que dice. Si eres uno, seguramente suenes brusco a los demás\nEn silencios, el acomodador cree que el otro se ha enfadado, el asertivo se pone a hablar y el analista intenta pensar.\nLa paradoja del normal soy yo: los demás deben ver el mundo como lo veo yo.\nEl enfado puede darte ventaja si tu reacción es comedida y real. Si results fingido, será contraproducente.\nUsa el por qué solo cuando cuestiones algo que realmente favorecen, como por qué cambiarías a tu proveedor actual.\nNos gusta más 1,99 que 2, incluso aunque sepamos que es un truco."
  },
  {
    "objectID": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#encuentra-el-cisne-negro",
    "href": "posts/2022-09-11-rompe-la-barrera-del-no/index.html#encuentra-el-cisne-negro",
    "title": "Rompe la barrera del no",
    "section": "10. Encuentra el cisne negro",
    "text": "10. Encuentra el cisne negro\n\nEn una negociación habrá cosas que no sabes que no sabes. Eso puede dar ventaja a tu interlocutor.\nhay tres tipos de ventaja: ventaja positiva (tienes algo que tu interlocutor quiere), negativa (cuando puedes amenazar; puede usarse de manera legítima con etiquetas como parece que no te importa dejarme en esta situación) y normativa (te apoyas en las creencias de la otra persona y aprovechad que nadie quiere ser hipócrita)\nParadoja del poder: cuanto más presionas, más fácil es que encuentres resistencia.\nPrincipio de similitud: confiamos más en las personas similares a nosotros. Eso se debe a que la similitud puede conllevar una capacidad de entendimiento mayor.\nLas personas tienden a aceptar más una propuesta tuya si saben que hay una razón concreta detrás de ella.\nCuando tu interlocutor haga algo irracional, que te parezca una locura, piensa que igual no entiendes bien su situación.\nElimina el miedo al conflicto. El conflicto es una herramienta para conseguir lo que quieres."
  },
  {
    "objectID": "posts/2024-08-04-puntos-en-R/index.html",
    "href": "posts/2024-08-04-puntos-en-R/index.html",
    "title": "Papel del punto en R",
    "section": "",
    "text": "Guillermo Luijk criticaba el punto en R porque se permite usar en el nombre de variables, lo que limita su uso fuera de ellas.\nHe intentado crear un operador en R con . que sirviera para sustituir en cierto modo al operador $. Imposible.\nPero comparto avances porque quizá, aunque sea al Leo del futuro, le puede venir bien."
  },
  {
    "objectID": "posts/2024-08-04-puntos-en-R/index.html#operadores-infix-propios-en-r",
    "href": "posts/2024-08-04-puntos-en-R/index.html#operadores-infix-propios-en-r",
    "title": "Papel del punto en R",
    "section": "Operadores infix propios en R",
    "text": "Operadores infix propios en R\nTodo lo que haces en R es una función. Pero no todas las funciones necesitan paréntesis. Por ejemplo, puedes sumar de dos formas:\n\n1 + 1\n\n[1] 2\n\n`+`(1, 1)\n\n[1] 2\n\n\nLa primera opción creo que se llama infix. Hay operaciones que se entienden mucho mejor así escritas que escritas como la segunda opción.\nDe hecho, esa es la filosofía del pipe. El pipe está disponible desde R 4.1.0 con |&gt; pero antiguamente había que usar la librería magrittr.\nComo esta librería no formaba parte de lo fundamentos del lenguaje, todos los operadores tenían que ser propios y para hacer eso hay que usar una sintaxis muy particular.\nLos pipes de magrittr van acotados por %, porque esa es la sintaxis obligatoria para cualquier operador que quieras hacer y usar infix. Así resultó en %&gt;%."
  },
  {
    "objectID": "posts/2024-08-04-puntos-en-R/index.html#creación-de-un-operador-propio",
    "href": "posts/2024-08-04-puntos-en-R/index.html#creación-de-un-operador-propio",
    "title": "Papel del punto en R",
    "section": "Creación de un operador propio",
    "text": "Creación de un operador propio\nMi objetivo era un operador . con el que pudiera hacer df.columna, por ejemplo, que mtcars.cyl obtuviera resultado. Sería algo así:\n\nmtcars.cyl\n\n\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n\n\nSi intentas ejecutar eso, te dará error. Y no he dado con ninguna forma de hacerlo funcionar. Porque cualquier operador propio tiene que ir rodeado de %.\nPor ello, esto es lo que he conseguido:\n\n`%.%` &lt;- function(df, columna) {\n    df[[as.character(substitute(columna))]]\n}\n\nmtcars%.%cyl\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4"
  },
  {
    "objectID": "posts/2020-05-24-literatura-y-totalitarismo/index.html",
    "href": "posts/2020-05-24-literatura-y-totalitarismo/index.html",
    "title": "Literatura y totalitarismo",
    "section": "",
    "text": "«El totalitarismo ha abolido la libertad de pensamiento hasta unos límites inauditos en cualquier época anterior. Y es importante que comprendamos que este control del pensamiento no es solo de signo negativo, sino también positivo: no solo nos prohíbe expresas -e incluso tener- ciertos pensamientos; también nos dica lo que debemos pensar, crea una ideología para nosotros, trata de gobernar nuestra vida emocional al tiemo que establece un código de conducta. Y, en la medida de lo posible, nos aísla del mundo exterior, nos encierra en un universo artifical en el que carecemos de criterios con los que comparar. El Estado totalitario trata en todo caso de controlar los pensamientos y emociones de sus súbditos al menos de modo tan absoluto como controla sus acciones.\n»La pregunta que nos preocupa es: ¿puede sobrevivir la literatura en una atmósfera semejante? […]\n»Hay varias diferencias fundamentales entre el totalitarismo y todas las ortodoxias del pasado, tanto en Europa como en Oriente. La más importante es que las ortodoxias del pasado no cambiaban, o al menos no lo hacían rápidamente. En la Euripa medieval, la Iglesia dictaba lo que debíamos creer, pero al menos nos permitía conservar las mismas creencias desde el nacimiento hastal a muerte. No nos decía que creyésemos una cosa el lunes y otra distinta el martes. […] Pues bien, con el totalitarismo ocurre exactamente lo contrario. La peculiaridad del Estado totalitario es que, si bien controla el pensamiento, no lo fija. Establece dogmas incuestionables y los modifica de un día para otro. Necesita dichos dogmas, pues precisa una obediencia absoluta por parte de sus súbditos, pero no puede evitar los cambios, que vienen dicatados por las necesidades de la política del poder. Se afirma infalible y, al mismo tiempo, ataca el propio concepto de verdad objetiva.»\n(George Orwell, 21 de mayo de 1941)"
  },
  {
    "objectID": "posts/2023-12-27-rethinking-1/index.html",
    "href": "posts/2023-12-27-rethinking-1/index.html",
    "title": "Statistical Rethinking (1)",
    "section": "",
    "text": "En Statistical Rethinking Richard McEalreath presenta los métodos generativos en estadística bayesiana."
  },
  {
    "objectID": "posts/2023-12-27-rethinking-1/index.html#introducción",
    "href": "posts/2023-12-27-rethinking-1/index.html#introducción",
    "title": "Statistical Rethinking (1)",
    "section": "Introducción",
    "text": "Introducción\nEl problema que plantea es el de estimar la proporción de agua que hay en la Tierra.\nEl procedimiento consiste en mirar en varios puntos de la Tierra y ver si hay agua o tierra. De esa forma obtienes una secuencia de valores: \"A\", \"A\", \"T\", \"A\", \"T\", ... y usas esos datos para estimar la proporción real.\nConcretamente, en 9 observaciones has obtenido 6 porciones de agua.\nHay tres formas de ajustar un modelo estadístico. Las ejemplificaremos con ese problema de la proporción de agua.\nLos códigos serán los del libro."
  },
  {
    "objectID": "posts/2023-12-27-rethinking-1/index.html#mallado",
    "href": "posts/2023-12-27-rethinking-1/index.html#mallado",
    "title": "Statistical Rethinking (1)",
    "section": "Mallado",
    "text": "Mallado\nLa primera forma de ajustar un modelo estadístico es mediante mallado. Computacionalmente es costoso pero es más educativo que el resto de métodos.\nLo que vas a hacer es proponer unos posibles valores para el parámetro que buscas (la proporción de agua) y les das una distribución a priori a estos valores (no pasa nada ahora por que la distribución a priori no sume 1 porque luego estandarizarás el resultado final).\n\ngrid_size &lt;- 20\n# define grid\np_grid &lt;- seq(from = 0, to = 1, length.out = grid_size)\n# define prior\nprior &lt;- rep(1 , grid_size)\n\nTus priori (20 proporciones equiprobables) ya están definidas y ahora te sirven para generar posibles escenarios. La idea es ver la verosimilitud de tus datos observados ante los distintos posibles valores del parámetro.\n\nlikelihood &lt;- dbinom(6, size = 9, prob = p_grid)\nlikelihood\n\n [1] 0.000000e+00 1.518149e-06 8.185093e-05 7.772923e-04 3.598575e-03\n [6] 1.116095e-02 2.668299e-02 5.292110e-02 9.082698e-02 1.383413e-01\n[11] 1.897686e-01 2.361147e-01 2.666113e-01 2.714006e-01 2.450051e-01\n[16] 1.897686e-01 1.179181e-01 5.026670e-02 8.853845e-03 0.000000e+00\n\n\nAhora calculas la posteriori, ponderando estas verosimilitudes por los prioris que tenías antes. Como los prioris no seguían una distribución de probabilidad, te toca estandarizar el resultado.\n\nunstd_posterior &lt;- likelihood * prior\nposterior &lt;- unstd_posterior / sum(unstd_posterior)\nposterior\n\n [1] 0.000000e+00 7.989837e-07 4.307717e-05 4.090797e-04 1.893887e-03\n [6] 5.873873e-03 1.404294e-02 2.785174e-02 4.780115e-02 7.280739e-02\n[11] 9.987296e-02 1.242643e-01 1.403143e-01 1.428349e-01 1.289433e-01\n[16] 9.987296e-02 6.205890e-02 2.645477e-02 4.659673e-03 0.000000e+00\n\n\n\nplot(p_grid, posterior, type = \"b\",\n     xlab = \"Probabidad de agua\" , \n     ylab = \"Posteriori\")\nmtext(\"20 puntos\")\n\n\n\n\n\n\n\n\nAhora bien, ten en cuenta que los prioris forman parte del modelo. Deberías tener un criterio para elegir unos u otros.\nLos puedes cambiar y los resultados se verán afectados.\n\np_grid &lt;- seq(0, 1, length.out = grid_size)\nprior &lt;- exp(-5 * abs(p_grid - 0.5))\n\nlikelihood = dbinom(6, 9, prob = p_grid)\n\nunstd_posterior &lt;-  likelihood * prior\nposterior &lt;- unstd_posterior / sum(unstd_posterior)\n\nplot(p_grid, posterior, type = \"b\",\n     xlab = \"Probabidad de agua\" , \n     ylab = \"Posteriori\")\nmtext(sprintf(\"%i puntos\", grid_size))"
  },
  {
    "objectID": "posts/2023-07-15-pernoctaciones/index.html",
    "href": "posts/2023-07-15-pernoctaciones/index.html",
    "title": "Pernoctaciones",
    "section": "",
    "text": "Me preocupa el turismo: viajamos demasiado.\n\n\n\n\n\n\n\n\n\nLa pandemia parecía haber frenado el ritmo, pero la tendencia vuelve a ser creciente en sitios como Madrid, Canarias, el País Vasco…"
  },
  {
    "objectID": "posts/2022-10-01-asalariado-taleb-habla-de-ti/index.html",
    "href": "posts/2022-10-01-asalariado-taleb-habla-de-ti/index.html",
    "title": "Asalariado, Taleb habla de ti",
    "section": "",
    "text": "Un perro alardea ante un lobo de todos los lujos y comodidades a su disposición.\nEste está casi convencido de unirse a él.\nHasta que le pregunta por su collar y se queda aterrado al saber para qué se usa.\n—No quiero ninguna de tus comidas— dijo el lobo.\nActo seguido echó a correr y correr y aún sigue corriendo.\nTú qué quieres ser: ¿lobo o perro?\nYo también soy perro. Todavía no tengo claro si estoy dispuesto a ser lobo.\n\nEl texto está extraído de Jugarse la piel, de Taleb, y es una fábula atribuida no sé si a Ahiqar, Esopo o La Fontaine.\nLinks de afiliado:\n\nComprar libro electrónico.\nComprar libro de papel."
  },
  {
    "objectID": "posts/2024-04-24-por-que-nse/index.html",
    "href": "posts/2024-04-24-por-que-nse/index.html",
    "title": "Por qué puedes necesitar non standard evaluation en dplyr",
    "section": "",
    "text": "Tienes los datos de penguins y quieres hacer una función con el código de una exploración sobre este conjunto de datos.\ndf_penguins &lt;- palmerpenguins::penguins\ndf_penguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\nLa situación que te planteo es que tienes que crear una función cuyo argumento sea el nombre de una de las columnas, y calcule la media de la longitud del pico (bill_length_mm) en función de esa columna. Con dplyr."
  },
  {
    "objectID": "posts/2024-04-24-por-que-nse/index.html#planteamiento-tidyselect",
    "href": "posts/2024-04-24-por-que-nse/index.html#planteamiento-tidyselect",
    "title": "Por qué puedes necesitar non standard evaluation en dplyr",
    "section": "Planteamiento tidyselect",
    "text": "Planteamiento tidyselect\nUna forma de hacerlo, aceptando que el nombre de la columna venga como character, es esta:\n\nlibrary(dplyr)\n\ncalcula_mediana_pico &lt;- function(data, columna_grupos) {\n  data |&gt; \n    group_by(pick(all_of(columna_grupos))) |&gt; \n    summarise(mediana_long_pico = median(bill_length_mm, na.rm = TRUE))\n}\n\ndf_penguins |&gt; \n  calcula_mediana_pico(\"island\")\n\n# A tibble: 3 × 2\n  island    mediana_long_pico\n  &lt;fct&gt;                 &lt;dbl&gt;\n1 Biscoe                 45.8\n2 Dream                  44.7\n3 Torgersen              38.9\n\n\nLe pongo una pega. Puede haber más pero yo le pongo solo una: que tengo que entrecomillar la columna con la que agrupo. O sea, no puedo hacer esto:\n\ndf_penguins |&gt; \n  calcula_mediana_pico(island)\n# Error: object 'island' not found"
  },
  {
    "objectID": "posts/2024-04-24-por-que-nse/index.html#planteamiento-rlang",
    "href": "posts/2024-04-24-por-que-nse/index.html#planteamiento-rlang",
    "title": "Por qué puedes necesitar non standard evaluation en dplyr",
    "section": "Planteamiento rlang",
    "text": "Planteamiento rlang\nMe gustaría que ese código funcionara, porque con dplyr es normal escribir los nombres de columnas sin comillas:\n\nPuedes usar el autocompletado (con comillas no)\nVa a ser consistente con otras funciones de dplyr\n\n\ndf_penguins |&gt; \n  select(island, bill_length_mm, year) |&gt; \n  filter(year &gt; 2007) |&gt; \n  calcula_mediana_pico(island)\n\nEn un código como ese, no quiero tener que entrecomillar island porque no viene a cuento.\nEso lo puedo solucionar con rlang. La función quedará algo así:\n\nlibrary(rlang)\n\ncalcula_mediana_pico &lt;- function(data, columna_grupos) {\n  data |&gt; \n    group_by(!!enquo(columna_grupos)) |&gt; \n    summarise(mediana_long_pico = median(bill_length_mm, na.rm = TRUE))\n}\n\nTe destripo ese ingrediente que acabo de poner.\n\nenquo\n\nf &lt;- function(argumento) {\n  enquo(argumento)\n}\n\nf(island)\n\n&lt;quosure&gt;\nexpr: ^island\nenv:  global\n\n\nenquo() me devuelve lo que se llama una quosure (ni idea de su traducción): lo puedes ver como una expresión con información sobre el entorno.\nAl evaluar enquo() dentro de una función con un argumento, obtenemos el valor que se le ha dado al argumento sin llegar a evaluarlo:\n\nf(1 + 1)\n\n&lt;quosure&gt;\nexpr: ^1 + 1\nenv:  global\n\n\nPara evaluarlo tenemos que:\n\nAñadir el operador !! (bang bang).\nOperar dentro de unas funciones determinadas, como muchas de dplyr.\n\n\nselecciona &lt;- function(columna) {\n  df_penguins |&gt; \n    select(!!enquo(columna))\n}\n\nselecciona(island) |&gt; \n  slice_head(n = 6)\n\n# A tibble: 6 × 1\n  island   \n  &lt;fct&gt;    \n1 Torgersen\n2 Torgersen\n3 Torgersen\n4 Torgersen\n5 Torgersen\n6 Torgersen\n\n\nQue es justo lo que hacía en la función que te planteé más arriba:\n\ncalcula_mediana_pico &lt;- function(data, columna_grupos) {\n  data |&gt; \n    group_by(!!enquo(columna_grupos)) |&gt; \n    summarise(mediana_long_pico = median(bill_length_mm, na.rm = TRUE))\n}\n\ndf_penguins |&gt; \n  calcula_mediana_pico(species)\n\n# A tibble: 3 × 2\n  species   mediana_long_pico\n  &lt;fct&gt;                 &lt;dbl&gt;\n1 Adelie                 38.8\n2 Chinstrap              49.6\n3 Gentoo                 47.3\n\n\nNi que decir tiene que es compatible con otras operaciones de dplyr:\n\ndf_penguins |&gt; \n  select(species, bill_length_mm, year) |&gt; \n  filter(year &gt; 2007) |&gt; \n  calcula_mediana_pico(species) |&gt; \n  arrange(desc(mediana_long_pico))\n\n# A tibble: 3 × 2\n  species   mediana_long_pico\n  &lt;fct&gt;                 &lt;dbl&gt;\n1 Chinstrap              49.7\n2 Gentoo                 47.5\n3 Adelie                 38.7\n\n\n\n!!enquo() puede reemplazarse por el operador {{}} (curly curly), con el que tus códigos quedarán más limpios, aunque a mí no termina de gustarme porque creo que se enienden peor los elemenos de la librería rlang.\n\n\n\nMás columnas\nVale, ahora no solo quieres dar al usuario la posibilidad de que dé la columna de agrupación, sino también la métrica que agregamos.\nSi solo quieres eso, no necesitas más ingredientes, pero ya verás cómo sí quieres tener otra cosa:\n\ncalcula_mediana &lt;- function(data, columna_grupos, metrica) {\n  data |&gt; \n    group_by(!!enquo(columna_grupos)) |&gt; \n    # cambio nombre de la mediana\n    summarise(mediana = median(!!enquo(metrica), na.rm = TRUE))\n}\n\ndf_penguins |&gt; \n  calcula_mediana(species, bill_depth_mm)\n\n# A tibble: 3 × 2\n  species   mediana\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Adelie       18.4\n2 Chinstrap    18.4\n3 Gentoo       15  \n\n\nGuay, ha funcionado. Puedo usar cualquier métrica y columna:\n\ndf_penguins |&gt; \n  calcula_mediana(year, flipper_length_mm)\n\n# A tibble: 3 × 2\n   year mediana\n  &lt;int&gt;   &lt;dbl&gt;\n1  2007     195\n2  2008     200\n3  2009     199\n\n\nLo que me falta ahora es poder cambiar el nombre también de la columna mediana en función de la métrica elegida.\n\n\n:= y as_name\nLa intuición podría decirte que hicieras algo parecido a la izquierda del igual, con !!enquo(). No te va a funcionar, salvo que añadas el operador. :=:\n\ncalcula_mediana &lt;- function(data, columna_grupos, metrica) {\n  data |&gt; \n    group_by(!!enquo(columna_grupos)) |&gt; \n    # cambio nombre de la mediana\n    summarise(!!enquo(metrica) := median(!!enquo(metrica), na.rm = TRUE))\n}\n\ndf_penguins |&gt; \n  calcula_mediana(island, bill_depth_mm)\n\nUn pequeño cambio y lo solucionas todo.\nPero vamos incluso un paso más allá. Quieres que la métrica nueva no se llame igual que la columna que agregas sino \"mediana_\" pegado al nombre de la columna. Para eso necesitas la combinación de as_name(), que te va a permitir tratar como character el argumento que pasa el usuario:\n\nas_name(quote(bill_length_mm))\n\n[1] \"bill_length_mm\"\n\n\nVamos a aplicar esto:\n\ncalcula_mediana &lt;- function(data, columna_grupos, metrica) {\n  nuevo_nombre &lt;- paste0(\n    \"mediana_\", \n    as_name(enquo(metrica))\n  )\n  data |&gt; \n    group_by(!!enquo(columna_grupos)) |&gt; \n    summarise(!!nuevo_nombre := median(!!enquo(metrica), na.rm = TRUE))\n}\n\ndf_penguins |&gt; \n  calcula_mediana(island, bill_depth_mm)\n\nO:\n\ndf_penguins |&gt; \n  calcula_mediana(year, body_mass_g)\n\n# A tibble: 3 × 2\n   year mediana\n  &lt;int&gt;   &lt;dbl&gt;\n1  2007    3900\n2  2008    4200\n3  2009    4000\n\n\n\nComo decía más arriba, !!enquo() se puede reemplazar por {{}}, pero creo que dificulta ver el papel de enquo() en todo esto.\n\nAtención:\n\nnuevo_entorno &lt;- new.env()\nnuevo_entorno$`+` &lt;- `-`\n\nsuma_expr &lt;- expr(1 + 1)\nsuma_expr\n\n1 + 1\n\n\nY resulta que:\n\neval_tidy(suma_expr, env = nuevo_entorno)\n\n[1] 0\n\n\n¿O no?\n\neval_tidy(suma_expr)\n\n[1] 2"
  },
  {
    "objectID": "posts/2023-11-26-comparativa-tiempos-formatos-ficheros/index.html",
    "href": "posts/2023-11-26-comparativa-tiempos-formatos-ficheros/index.html",
    "title": "¿Qué formato de datos es más rápido de leer y escribir?",
    "section": "",
    "text": "He leído aquí una comparativa entre tiempos de ejecución de lectura y escritura de varios tipos de ficheros de datos. Con Python.\nY he pensado:\nHe creado dos data frames, a los que he llamado df_numeric y df_mixed. Ambos tienen un millón de filas y 20 columnas.\nY lo que quiero es ver cuánto tardo en guardar cada uno en disco, en función del tipo de fichero y la función que elija."
  },
  {
    "objectID": "posts/2023-11-26-comparativa-tiempos-formatos-ficheros/index.html#ficheros-con-columnas-numéricas",
    "href": "posts/2023-11-26-comparativa-tiempos-formatos-ficheros/index.html#ficheros-con-columnas-numéricas",
    "title": "¿Qué formato de datos es más rápido de leer y escribir?",
    "section": "Ficheros con columnas numéricas",
    "text": "Ficheros con columnas numéricas\nEmpiezo con el de todo numérico. Uso mi librería favorita, microbenchmark.\n\nruntimes_numeric &lt;- microbenchmark::microbenchmark(\n  csv_utils = write.csv(df_numeric, \"numeric_utils.csv\", row.names = FALSE),\n  csv_readr = write_csv(df_numeric, \"numeric_readr.csv\"),\n  fwrite = data.table::fwrite(df_numeric, \"numeric_fwrite.csv\"),\n  rds_base = base::saveRDS(df_numeric, \"numeric_base.rds\"),\n  rds_readr = readr::write_rds(df_numeric, \"numeric_readr.rds\"),\n  parquet = arrow::write_parquet(df_numeric, \"numeric.parquet\"),\n  feather = arrow::write_feather(df_numeric, \"numeric.feather\"),\n  times = 10L\n)\n\n\n\nUnit: milliseconds\n      expr         min         lq       mean     median         uq        max\n csv_utils 202203.9288 266623.082 282231.496 281121.287 302487.454 331668.044\n csv_readr   9055.3445   9846.454  20284.347  10764.766  12634.343 104354.622\n    fwrite   1610.8502   8701.182   8911.451   9409.666  10717.019  11205.837\n  rds_base  25460.8486  26287.225  29357.257  28765.523  32329.897  35358.147\n rds_readr    330.9374   3041.022   3080.719   3132.418   3334.561   5672.141\n   parquet   3234.5500   8282.600   9649.330   9843.929  12261.475  12863.989\n   feather    410.0282   4370.313   4700.664   4539.408   5586.161   7597.147\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nVoy a visualizar esto (se puede hacer directamente sobre la salida de la función microbenchmark() pero lo paso a data frame porque he querido hacer algunas pruebas con las que necesitaba más control, aunque al final no las muestro aquí porque no aportan nada).\nCreo dos data frames para pintar los tiempos de cómputo y el tamaño de los ficheros. Los tiempos (milisegundos) vienen dados para ejecución: aquí están los primeros casos.\n\n\n       case       time\n1   feather   410.0282\n2    fwrite  1610.8502\n3 rds_readr   330.9374\n4   parquet  3234.5500\n5  rds_base 27254.9053\n6   feather  4436.2614\n7   feather  7597.1466\n\n\nTomo también el tamaño de los ficheros generados:\n\n\n       size      case\n1 106522192  rds_base\n2 359999269    fwrite\n3 385418847 csv_readr\n4 160000514 rds_readr\n5 359998865 csv_utils\n6 133286834   feather\n7 143632169   parquet\n\n\nCreo los gráficos por separado aunque se puede visualizar todo junto (pero me parece ganas de complicar).\nEstos son los tiempos de cómputo (quito utils::write.csv() porque ya sabemos que tarda mucho)."
  },
  {
    "objectID": "posts/2023-12-07-tidyverse-rbase/index.html",
    "href": "posts/2023-12-07-tidyverse-rbase/index.html",
    "title": "Comparativa pequeña entre tidyverse y R base",
    "section": "",
    "text": "Me gusta leer críticas contra tidyverse. ¿Porque lo odio?\nNo, lo uso todo el rato. Todos los días. Mi trabajo se basa en el tidyverse.\n¿Por qué leo críticas entonces?\nPorque no entiendo que las haya. Así que me obligo a leer otros puntos de vista para ver en qué estoy equivocado.\nPor ahora, gano yo. El link que publico al principio no es convincente. Un día le responderé al autor. Mientras tanto, estudio R base.\nMe parece entretenido. Incluso bonito.\nY bromas aparte, le reconozco una cosa: me quita toda la gestión de dependencias. Cuando ejecuto este blog en GitHub Actions o cuando creo una librería, quitar dependencias simplifica mucho el trabajo y el proceso.\nEso sí me gusta de R base.\nPero para el día a día es un infierno.\nAsí que se me ha ocurrido hacerme un diccionario de R base y tidyverse, de cosas que consulto a veces.\nEs un mini diccionario: faltarán muchas cosas aquí que te harán falta. Pero aquí encontrarás algunas que quizá te hagan falta. Y oye, si es así, tu búsqueda ha terminado."
  },
  {
    "objectID": "posts/2023-12-07-tidyverse-rbase/index.html#conversión-r-base---tidyverse",
    "href": "posts/2023-12-07-tidyverse-rbase/index.html#conversión-r-base---tidyverse",
    "title": "Comparativa pequeña entre tidyverse y R base",
    "section": "Conversión R base - Tidyverse",
    "text": "Conversión R base - Tidyverse\nPara simplificar dependencias trabajaré, por supuesto, con iris.\n\nComparativa R base - Tidyverse\n\n\n\n\n\n\n\nOperación\nTidyverse\nR base\n\n\nCalcula la media de una métrica por grupo.\niris |&gt;\ngroup_by(Species) |&gt;\nsummarise(mean(Sepal.Length))\naggregate(\nSepal.Length ~ Species,\niris,\nmean)\n\n\nExtrae los números de un texto (aplica a cualquer patrón, como DNI, razón social, etc).\nstr_extract(\n\"cosa123cosa\",\n\"\\\\d+\")\nregmatches(\n\"cosa123cosa\",\nm = regexpr(\"\\\\d+\", \"cosa123cosa\", perl = TRUE))\n\n\nQuédate con las observaciones cuyo valor (en una métrica) sea mayor que la media de su grupo.\niris |&gt;\ngroup_by(Species) |&gt;\nfilter(Sepal.Lengh &gt;= mean(Sepal.Length))\ndo.call(\"rbind\",\nlapply(split(iris, iris$Species),\nfunction(df) {               subset(df, Sepal.Length &gt;= mean(df$Sepal.Length)) }))\n\n\n(*) Pasa de formato ancho a formato largo - Transformación melt - Pasa métricas a filas\niris |&gt;\npivot_longer(-Species)\niris$id &lt;- 1:nrow(iris)\nreshape(iris,\nidvar=\"id\",\ndirection=\"long\",          varying=list(1:4),         v.names = \"value\",         timevar=\"metric\")\n\n\n\nEn la 4ª he puesto un asterisco porque está mal. No soy capaz de hacer eso en R base de una manera razonablemente cómoda para mí.\nPero yo lo sigo intentando."
  },
  {
    "objectID": "posts/2023-10-12-bicimad/index.html",
    "href": "posts/2023-10-12-bicimad/index.html",
    "title": "Paseo en bicicleta eléctrica por Madrid",
    "section": "",
    "text": "Últimamente tengo la sensación de que las estaciones de Bicimad, el servicio de bicis públicas de Madrid, están vacías.\nCuando abro la app, veo que las estaciones de la mitad sur de Madrid están llenas, y las de la mitad norte están vacías.\nQuería ver si los datos públicos me decían algo pero:\n\nHe tardado casi dos horas en leer el json de datos, porque está escrito como si… no sé. O sea, se suele decir que los datos en json están semiestructurados. Pues mira, los datos de una fotografía tienen más estructura que el json de estos datos públicos.\nLos datos que me he puesto a ver (los de estaciones) solo están hasta diciembre de 2022. Como me di cuenta de esto en verano, he cogido los datos de julio de 2022.\n\nLos datos de junio los he agregado a nivel horario y calculo la proporción media horaria de bases disponibles.\nEso es lo que puedes ver aquí:"
  },
  {
    "objectID": "posts/2024-10-04-ergodicidad/index.html",
    "href": "posts/2024-10-04-ergodicidad/index.html",
    "title": "Procesos ergódicos",
    "section": "",
    "text": "Leo a Gil Bellosta sobre ergocidad y he querido plantear lo que comenta.\nEl define la ergodicidad mucho mejor de lo que lo podría hacer yo, salvo que lo copiara. Pero para copiarlo, mejor ve a su web.\nPlanteo aquí el código de su ejemplo.\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2024-10-04-ergodicidad/index.html#lanzamiento-de-una-moneda",
    "href": "posts/2024-10-04-ergodicidad/index.html#lanzamiento-de-una-moneda",
    "title": "Procesos ergódicos",
    "section": "Lanzamiento de una moneda",
    "text": "Lanzamiento de una moneda\n\nn_individuos = 100\nn_veces = 1000\n\nTienes a python n_individuos que lanzan una moneda al aire python n_veces cada uno. Eso hace un total de python n_individuos * n_veces lanzamientos, en los que la probablidad teórica de obtener cara (resultado igual a 1 en este caso) es \\(0,5\\).\n\nnp.random.seed(10)\nlanzamientos = np.random.randint(0, 2, size=n_individuos * n_veces)\nlanzamientos = lanzamientos.reshape(n_individuos, n_veces)\nlanzamientos.shape\n\n(100, 1000)\n\n\nEste es un proceso estocástico ergódico porque:\n\nPara cualquier individuo \\(i\\), la probabilidad de obtener cara en todos sus lanzamientos es 0,5, y\npara cualquier lanzamiento \\(k\\), la probabilidad de obtener cara entre los \\(k\\)-ésimos lanzamientos de todos los individuos es \\(0,5\\).\n\nPor ejemplo, la probabilidad de obtener cara para el individuo 2 (el tercero, porque esto es Python y piensan raro) es:\n\nnp.mean(lanzamientos[2, :])\n\n0.494\n\n\nA su vez, la probabilidad de que obtengan cara entre todos los individuos en el lanzamiento 50 (el 51 xD) es:\n\nnp.mean(lanzamientos[:, 50])\n\n0.53"
  },
  {
    "objectID": "posts/2024-10-04-ergodicidad/index.html#ejemplo-de-proceso-no-ergódico",
    "href": "posts/2024-10-04-ergodicidad/index.html#ejemplo-de-proceso-no-ergódico",
    "title": "Procesos ergódicos",
    "section": "Ejemplo de proceso no ergódico",
    "text": "Ejemplo de proceso no ergódico\nSupón ahora este proceso:\n\nPartes de un capital de 1.\nTiras una moneda al aire.\nSi sale cara, multiplicas el capital por 1,5; si sale cruz, por 0,6\nVe al paso 2.\n\nLa simulación de lanzamientos de monedas con sus resultados \\(1,5\\) y \\(0,6\\) se puede hacer así:\n\nmultiplicadores_capital = np.random.choice([0.6, 1.5], size=n_individuos * n_veces)\nmultiplicadores_capital = multiplicadores_capital.reshape(n_individuos, n_veces)\n\nY la evolución del capital se calcula así:\n\ncapitales = multiplicadores_capital.cumprod(axis=1)\n\nEl capital de cada individuo (python n_individuos) empezaba en 1, por lo que el capital medio por individuo era de 1. Tras el primer lanzamiento, el capital medio queda en:\n\ncapitales[:, 0].mean()\n\n1.0499999999999998\n\n\nTen en cuenta que la media teórica sería \\(\\frac{1,50 + 0,60}{2} = 1,05\\).\nLa evolución de capitales medios:\n\n# compute mean per column on numpy array capitales\nevolucion_capital_medio = capitales.mean(axis=0)\n\nplt.plot(evolucion_capital_medio[0:150, ], color='#800080')\nplt.xlabel('Tiempo')\nplt.ylabel('Capital medio')\nplt.title('Evolución del capital medio')\nplt.show()\n\n\n\n\n\n\n\n\nSi pongo de límite hasta la tirada 100, el capital medio ha crecido:\n\nn_tirada = 100\nevolucion_capital_medio[n_tirada, ]\n\n16.859029246380643\n\n\nSin embargo, solo un 11% de los individuos ha visto su capital crecer:\n\n(capitales[:, n_tirada] &gt; 1).sum()\n\n11\n\n\nEn un proceso no ergódico (¿cuántos hay así en una sociedad occidental?), puedes ver crecimiento global, pero solo de unos pocos."
  },
  {
    "objectID": "posts/2024-08-17-evaluacion-expresiones/index.html",
    "href": "posts/2024-08-17-evaluacion-expresiones/index.html",
    "title": "Evaluación de expresiones en R",
    "section": "",
    "text": "Levanta la mano si a ti también te gusta concatenar caracteres con + como en Python.\nMe refiero a que en Python puedes hacer 'Hola ' + ' Leo' pero en R no.\n¿O sí?\n\"hola, puedo \" + \"concatenar\"\n\n[1] \"hola, puedo concatenar\"\nPero:\n1 + 3\n\n[1] 4"
  },
  {
    "objectID": "posts/2024-08-17-evaluacion-expresiones/index.html#justificación-de-quosures",
    "href": "posts/2024-08-17-evaluacion-expresiones/index.html#justificación-de-quosures",
    "title": "Evaluación de expresiones en R",
    "section": "Justificación de quosures",
    "text": "Justificación de quosures\nLa idea detrás de funciones como base::subset() o dplyr::mutate() es que puedes escribir operaciones dentro de data frames haciendo referencia directamente a las columnas, sin usar la sintaxis df$columna.\n\nlibrary(rlang)\nopera &lt;- function(data, entrada) {\n  eval_tidy(enexpr(entrada), data)\n}\n\nnuevo_calculo &lt;- opera(iris, Sepal.Length + Sepal.Width)\nhead(nuevo_calculo)\n\n[1] 8.6 7.9 7.9 7.7 8.6 9.3\n\n\nEl inconveniente es que enexpr() puede no ser suficiente porque no tiene en cuenta información del entorno.\nPor ejemplo:\n\ndf &lt;- data.frame(col1 = c(5, 6, 7))\nfactor &lt;- 2\n\nopera2 &lt;- function(data, entrada) {\n  factor &lt;- 10\n  eval_tidy(enexpr(entrada), data)\n}\n\n# Con enquo no sabes cuál es el entorno original de la variable\nopera2(df, col1 * factor)\n\n[1] 50 60 70\n\n\nUna solución que se te puede ocurrir es: no pongas en la expresión variables que estén definidas en la función. Ya, ¿y si no sabes lo que hay en la función? ¿Y si el código es tan largo que no te da para revisarlo?\nPues lo arreglas con enquo().\n\ndf &lt;- data.frame(col1 = c(5, 6, 7))\nfactor &lt;- 2\n\nopera3 &lt;- function(data, entrada) {\n  factor &lt;- 10\n  eval_tidy(enquo(entrada), data)\n}\n\n# Con enquo usas el entorno donde se definió la variable original\nopera3(df, col1 * factor)\n\n[1] 10 12 14"
  },
  {
    "objectID": "posts/2024-02-24-normality/index.html",
    "href": "posts/2024-02-24-normality/index.html",
    "title": "¿Cómo surge la distribución normal?",
    "section": "",
    "text": "La distribución normal siempre me ha parecido demasiado perfecta para esperar verla en unos datos reales.\nPero claro, eso me pasa por no saber de dónde viene.\nTe cuento.\nImagina un grupo de personas que parten de una línea. Cada uno lanza una moneda al aire; si sale cara, da un paso a la izquierda, de máximo 1 metro; si no, lo da a la derecha. Los pasos estarán entre 0 y 1 metro. Al cabo de 16 lanzamientos, ¿a qué distancia estará del origen?\nSerá la suma de las distancias de esos pasos.\nPodemos calcular esto para 1000 personas.\nPrimero si dan 4 pasos:\npos &lt;- replicate(1000, sum(runif(4, -1, 1)))\nhist(pos)\nAhora si dan 8 pasos:\npos &lt;- replicate(1000, sum(runif(8, -1, 1)))\nhist(pos)\nAhora si dan 16:\npos &lt;- replicate(1000, sum(runif(16, -1, 1)))\nhist(pos)\nRealmente, casi con cualquier distribución puedes hacer esto. La suma de valores de cualquier distribución seguirá una distribución normal (con algunas necesitarás una muestra más grande que con otras).\nhist(replicate(1000, sum(rpois(100, 2))))"
  },
  {
    "objectID": "posts/2024-02-24-normality/index.html#ejemplo-de-modelo-basado-en-la-normalidad",
    "href": "posts/2024-02-24-normality/index.html#ejemplo-de-modelo-basado-en-la-normalidad",
    "title": "¿Cómo surge la distribución normal?",
    "section": "Ejemplo de modelo basado en la normalidad",
    "text": "Ejemplo de modelo basado en la normalidad\nLa altura de varias personas adultas es esperable que se distribuya como una distribución normal.\nLa idea intuitiva es que la altura de una persona adulta es el resultado de muchos pequeños aumentos que se suman. Para distintos individuos, estas sumas darán resultados distintos, porque los aumentos progresivos entre ellos serán distintos al cabo de los años. Pero el conjunto de todas ellas será normal, si has esperado suficientes años para que los casos más altos (aumentos más grandes acumulados) se compensen con los más bajos (aumentos más pequeños).\nVamos a trabajar con una muestra de personas, de las que conocemos su altura.\nQueremos modelizar la altura como una distribución normal, o sea, estimaremos la media y desviación típica.\nEl método bayesiano considerará todas las medias y desviaciones típicas posibles (realmente haremos una aproximación), y evaluaremos cada una en función de cómo de plausibles son, dados los datos que tenemos.\n\nlibrary(rethinking)\n\ndata(Howell1)\nhead(Howell1)\n\n   height   weight age male\n1 151.765 47.82561  63    1\n2 139.700 36.48581  63    0\n3 136.525 31.86484  65    0\n4 156.845 53.04191  41    1\n5 145.415 41.27687  51    0\n6 163.830 62.99259  35    1\n\n\nPor ahora, vamos a centrarnos solo en los casos adultos, porque la altura de los niños no se distribuye normalmente, sino que está correlada con la edad.\n\nadultos &lt;- Howell1[Howell1$age &gt; 18, ]\n\nhist(adultos$height, col = \"#800080\")\n\n\n\n\n\n\n\n\nQueremos modelizar la altura \\(h_i\\) de cada individuo \\(i\\) como \\(h_i \\sim \\cal{N}(\\mu, \\sigma)\\). El objetivo es estimar \\(\\mu\\) y \\(\\sigma\\).\nComo estamos en un modelo bayesiano, necesitamos unos prioris para ambos parámetros. Los prioris son distribuciones de estos parámetros que consideramos razonables.\n\\[\nh_i \\sim \\cal{N}(\\mu, \\sigma)\n\\\\\n\\mu \\sim \\cal{N}(178, 20)\n\\\\\n\\sigma \\sim \\cal{U}(0, 50)\n\\]\nEstamos dando como distribución a priori de la media una normal centrada en \\(\\cal{178}\\) y con un rango de \\(\\cal{40}\\)cm con una probablidad de 95%.\n\ncurve(dnorm(x, 178, 20), from = 100, to = 250, col = \"#800080\", lwd = 2)\n\n\n\n\n\n\n\n\nLa priori sobre la desviación típica es plana, en el sentido de que no sabemos dar más importancia a unos valores frente a otros, dentro los propuestos en el intervalo de la uniforme.\nPodemos simular cómo serían unos datos a partir de estas prioris.\n\nsample_mu &lt;- rnorm(1e4, 178, 20)\nsample_sigma &lt;- runif(1e4, 0, 50)\n\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\nhist(prior_h, col = \"#800080\")\n\n\n\n\n\n\n\n\nLo que hemos hecho es simular 10.000 pares de \\(\\mu\\) y \\(\\sigma\\) y para cada par hemos simulado una altura. Así tenemos 10.000 alturas, cada una originada a partir de una población normal distinta (pero todas las poblaciones normales son tan plausibles como indica nuestra priori).\nSi cambiáramos la priori, la simulación a priori de la altura se vería afectada:\n\nsample_mu &lt;- rnorm(1e4, 178, 100)\n\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\nhist(prior_h, col = \"#800080\")\n\n\n\n\n\n\n\n\nEsa es una priori menos informativa, con una media mucho más dispersa. Pero puedes ver que las alturas que derivarían de unas prioris así no son realistas, dado que hay casos de altura negativa o también demasiado altos.\n\nfit_altura &lt;- quap( \n  alist(\n    height ~ dnorm( mu , sigma ) ,\n    mu ~ dnorm( 178 , 20 ) ,\n    sigma ~ dunif( 0 , 50 )\n  ), \n  data = adultos\n)\n\nprecis(fit_altura)\n\n            mean        sd       5.5%      94.5%\nmu    154.654539 0.4172190 153.987743 155.321336\nsigma   7.762397 0.2950864   7.290792   8.234002\n\n\nObtenemos un modelo normal con una media cuya media es 154 y su desviación típica es 0.42. Sí, el resultado de para la media de de la población es una distribución: no conocemos el valor exacto de la media sino que hemos estimado la distribución a la que pertenece.\nLo mismo con la desviación típica.\nConcretamente, esta distribución de la media es la que muestra este histograma.\n\nposterior_h &lt;- extract.samples(fit_altura, n = 1e4)\nhist(posterior_h$mu, col = \"#800080\")\n\n\n\n\n\n\n\n\n\nAjuste de prioris\nLa priori no informativa que sugerimos antes no tenía sentido porque daba lugar a valores de alturas inhumanas, como negativos o demasiados altos.\nPodemos proponer también una priori más restrictica.\n\nsample_mu &lt;- rnorm(1e4, 178, 0.1)\n\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\nhist(prior_h, col = \"#800080\")\n\n\n\n\n\n\n\n\nEsta priori fuerza a la media a quedarse mucho más centrada en la media propuesta. Y esto cambia el modelo.\n\nfit_altura2 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma), \n    mu ~ dnorm(178, 0.1), \n    sigma ~ dunif(0, 50)\n  ), \n  data = adultos\n)\n\nprecis(fit_altura2)\n\n           mean        sd      5.5%     94.5%\nmu    177.86598 0.1002314 177.70579 178.02617\nsigma  24.48463 0.9356079  22.98935  25.97991\n\n\nLa media \\(\\mu\\) apenas se ha movido del prior ahora. Sin embargo, el valor de \\(\\sigma\\) ha cambiado mucho porque el modelo lo calcula condicionada a la restricción del otro parámetro."
  },
  {
    "objectID": "posts/2021-12-26-burbuja-tur-stica/index.html",
    "href": "posts/2021-12-26-burbuja-tur-stica/index.html",
    "title": "Burbuja turística",
    "section": "",
    "text": "Te gusta viajar. Conoces otras culturas, comidas diferentes, desconectas.\nPreparar la maleta, a veces, te estresa un poco. Pero también te hace ilusión. ¿Qué me llevo? Gran pregunta.\nEn el aeropuerto sacas tarjeta de embarque y documentación haga falta o no. A veces te la piden en un sitio, a veces en otro. Tú la sacas en todos y arreglado.\nTe tienes que quitar las botas en el control. Y el cinturón. Sacas los líquidos, los pones en una bandeja. Y el móvil, a mano, que se vea.\nLuego a hacer cola para embarcar. Hay gente que llega muy justa. Otros respetan a rajatabla lo de las dos horas de antelación. ¿Tú qué prefieres?\nMira. Da igual.\n¿Qué tal si mejor no te vas de viaje?\nSí, no viajes. O a ver, viaja menos.\n¿Sabes que lo de viajar se nos está yendo de las manos? Se viaja mucho. Será que se ha abaratado y ahora está al alcance de más personas.\nEso es bueno, ¿no?\nNo sé. No lo tengo tan claro. Así, sacado de contexto, que cada vez más gente pueda viajar sí parece bueno. ¿Pero es sostenible?\nVas de viaje a un sitio paradisíaco. Como es paradisíaco, se pone de moda. Como se pone de moda, edifican de todo. Y deja de ser paradisíaco. Encuentras otro sitio paradisíaco. Pero se pone de moda.\nYa ves por dónde voy.\n\n\n\n\n\n\n\n\n\nLa selección de países es random. Los datos son de la ONU.\nNo tengo ni idea de si hay una burbuja turística. Pero sí parece que el turismo crece. El doble o el triple de turistas en 20 años en esos países. Si el turismo crece, se invertirá más en ese sector. Esto hará que el turismo tenga más relevancia en el PIB del país.\nAnte una crisis, que se dependa principalmente de un sector, no es un buen plan.\nPero el camino es ese."
  },
  {
    "objectID": "posts/2023-06-12-regresion-a-la-media/index.html",
    "href": "posts/2023-06-12-regresion-a-la-media/index.html",
    "title": "Los hijos de padres altos son bajitos",
    "section": "",
    "text": "# A tibble: 6 × 2\n  father   son\n   &lt;dbl&gt; &lt;dbl&gt;\n1   65    59.8\n2   63.3  63.2\n3   65    63.3\n4   65.8  62.8\n5   61.1  64.3\n6   63    64.2\n\n\n\n\n\n\n\n\n\n\n\nEl siguiente gráfico muestra qué es la regresión a la media.\nLa línea verdosa cae sobre los puntos en los que la altura del padre y del hijo son iguales. O sea:\n\nlos puntos por encima de la línea son de hijos más altos que sus padres\nlos puntos por debajo, de hijos más bajitos que sus padres.\n\n¿Qué muestra la línea azul?\n\n\n\n\n\n\n\n\n\nLa línea azul muestra lo esperable según la recta de regresión. Podríamos decir que, en media, los padres de cierta altura tienen hijos de altura según indique la línea azul.\nPero la línea azul queda por encima de la línea verdosa en unos puntos y por debajo en otro. ¿Cómo interpretamos eso?\nPues que, en media:\n\nlos padres bajitos tienen hijos más altos que ellos (la línea azul queda por encima de la verdosa en el lado izquierdo del gráfico).\nlos padres altos tienen hijos más bajos que ellos (la línea azul queda por debajo de la verdosa en el lado derecho del gráfico)."
  },
  {
    "objectID": "posts/2024-01-24-rethinking-2/index.html",
    "href": "posts/2024-01-24-rethinking-2/index.html",
    "title": "Statistical Rethinking (2)",
    "section": "",
    "text": "Hay un experimento en el que pones a unas personas en línea con una moneda. Cada persona tiene una moneda que lanza 16 veces.\nPor cada vez que sale cara, da un paso a la derecha; por cada vez que sale cruz, a la izquierda.\nCada paso será de una distancia aleatoria entre 0 y 1 metro. ¿Cuántos metros se ha desplazado cada persona con respecto al origen?\nEl experimento con una sola persona sería algo así:\n\ndistancia_por_paso &lt;- runif(16, -1, 1)\nsum(distancia_por_paso)\n\n[1] 2.171815\n\n\nNuestro sujeto ha recorrido 2.1718148 desde la línea.\nLo podemos extrapolar a 10000 sujetos:\n\npos &lt;- replicate(10000, sum(runif(16, -1, 1)))\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\nggplot() + \n  geom_histogram(aes(x = pos), fill = \"#800080\")"
  },
  {
    "objectID": "posts/2024-01-24-rethinking-2/index.html#por-qué-lo-normal-es-normal",
    "href": "posts/2024-01-24-rethinking-2/index.html#por-qué-lo-normal-es-normal",
    "title": "Statistical Rethinking (2)",
    "section": "",
    "text": "Hay un experimento en el que pones a unas personas en línea con una moneda. Cada persona tiene una moneda que lanza 16 veces.\nPor cada vez que sale cara, da un paso a la derecha; por cada vez que sale cruz, a la izquierda.\nCada paso será de una distancia aleatoria entre 0 y 1 metro. ¿Cuántos metros se ha desplazado cada persona con respecto al origen?\nEl experimento con una sola persona sería algo así:\n\ndistancia_por_paso &lt;- runif(16, -1, 1)\nsum(distancia_por_paso)\n\n[1] 2.171815\n\n\nNuestro sujeto ha recorrido 2.1718148 desde la línea.\nLo podemos extrapolar a 10000 sujetos:\n\npos &lt;- replicate(10000, sum(runif(16, -1, 1)))\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\nggplot() + \n  geom_histogram(aes(x = pos), fill = \"#800080\")"
  },
  {
    "objectID": "posts/2023-09-30-tsne/index.html",
    "href": "posts/2023-09-30-tsne/index.html",
    "title": "Cómo visualizar cualquier conjunto de datos en 2 dimensiones",
    "section": "",
    "text": "En este post te muestro cómo puedes usar el algoritmo t-SNE con R para visualizar datos de grandes dimensiones en solo 2.\nUtilizo el conjunto de datos MNIST, que tiene 60.000 observaciones de cifras escritas a mano, determinadas por una malla de 28x28 con un indicador del nivel de gris que hay en cada píxel del mallado.\nCada una de las observaciones viene dada por el nivel de gris que hay en cada píxel, o sea, por 784 variables.\nSi quisieras hacer una visualización de cómo se distribuyen tus observaciones en todas esas variables… pues simplemente no podrías.\nt-SNE te lo proyecta todo en 2 dimensiones, para que puedas luego visualizarlo."
  },
  {
    "objectID": "posts/2023-09-30-tsne/index.html#preparación-de-datos",
    "href": "posts/2023-09-30-tsne/index.html#preparación-de-datos",
    "title": "Cómo visualizar cualquier conjunto de datos en 2 dimensiones",
    "section": "Preparación de datos",
    "text": "Preparación de datos\nHe hecho un poco de trampa y los datos ya los tengo preparados. Pero vamos, que voy a usar los datos que tienes si ejecutas keras::dataset_mnist(). Concretamente la parte de entrenamiento.\nY bueno, tendrás que ejecutar eso y configurarlo bien, porque menuda aventura: Python y sus entornos virtuales.\nAparte, usaré librerías de tidyverse para dar forma a los datos.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(Rtsne)\n\nggplot2::theme_set(ggplot2::theme_light())\n\nPara lo que quiero hacer no necesito las 60.000 observaciones que hay, así que me quedo con las 1000 primeras.\n\nmax_number &lt;- 1000\n# map_dfr(seq_len(dim(mnist)[1]), function(i) {\ndata &lt;- map_dfr(seq_len(max_number), function(i) {\n  data_aux &lt;- mnist[i, 1:28, 1:28] \n  colnames(data_aux) &lt;- paste0(\"Y\", 1:28)\n  \n  data_aux |&gt; \n    as_tibble() |&gt; \n    mutate(x = paste0(\"X\", 1:28)) |&gt; \n    pivot_longer(-x, names_to = \"y\") |&gt; \n    unite(\"variable\", x, y) |&gt; \n    mutate(case = i)  \n})\n\ndata &lt;- data |&gt; \n  pivot_wider(names_from = variable, values_from = value)\n\ndim(data)\n\n[1] 1000  785\n\n\nLa columna extra es la que me indica que son observaciones diferentes."
  },
  {
    "objectID": "posts/2023-09-30-tsne/index.html#cómo-aplicas-t-sne",
    "href": "posts/2023-09-30-tsne/index.html#cómo-aplicas-t-sne",
    "title": "Cómo visualizar cualquier conjunto de datos en 2 dimensiones",
    "section": "Cómo aplicas t-SNE",
    "text": "Cómo aplicas t-SNE\nMira, sinceramente ni idea.\nO sea, tú ejecuta Rtsne() sobre tu data frame y surge la magia.\n\ntsne_fit &lt;- Rtsne(data)\ndim(tsne_fit$Y)\n\n[1] 1000    2"
  },
  {
    "objectID": "posts/2023-09-30-tsne/index.html#cómo-puedes-visualizar-t-sne",
    "href": "posts/2023-09-30-tsne/index.html#cómo-puedes-visualizar-t-sne",
    "title": "Cómo visualizar cualquier conjunto de datos en 2 dimensiones",
    "section": "Cómo puedes visualizar t-SNE",
    "text": "Cómo puedes visualizar t-SNE\nRecupero de los datos originales la etiqueta del número que se supone que representa cada mallado. Como estoy trabajando solo con los 1.000 primeros, me quedo solo con esos.\nEn el gráfico muestro cómo se distribuyen los puntos y los coloreo en función de si son un 0, 1, 2, 3, etc.\n\ndf_to_plot &lt;- as_tibble(tsne_fit$Y)\ndf_to_plot$label &lt;- as.factor(number_labels[1:max_number])\n\nggplot(df_to_plot) + \n  geom_point(aes(x = V1, y = V2, col = label)) + \n  labs(\n    title = \"t-SNE applied to MNIST dataset\",\n    subtitle = sprintf(\"Sample of first %i elements\", max_number), \n    caption = \"Source: keras::dataset_mnist()\"\n  )"
  },
  {
    "objectID": "posts/2024-12-06-bootstrap-and-bayes-regression/index.html",
    "href": "posts/2024-12-06-bootstrap-and-bayes-regression/index.html",
    "title": "Regresión lineal bootstrap y bayesiana",
    "section": "",
    "text": "Del libro Statistical Rethinking tomo los datos de divorcios.\nlibrary(rethinking)\nlibrary(ggplot2)\nggplot2::theme_set(ggplot2::theme_bw())\n\ndata(WaffleDivorce)\ndf_divorce &lt;- WaffleDivorce\nhead(df_divorce)\n\n    Location Loc Population MedianAgeMarriage Marriage Marriage.SE Divorce\n1    Alabama  AL       4.78              25.3     20.2        1.27    12.7\n2     Alaska  AK       0.71              25.2     26.0        2.93    12.5\n3    Arizona  AZ       6.33              25.8     20.3        0.98    10.8\n4   Arkansas  AR       2.92              24.3     26.4        1.70    13.5\n5 California  CA      37.25              26.8     19.1        0.39     8.0\n6   Colorado  CO       5.03              25.7     23.5        1.24    11.6\n  Divorce.SE WaffleHouses South Slaves1860 Population1860 PropSlaves1860\n1       0.79          128     1     435080         964201           0.45\n2       2.05            0     0          0              0           0.00\n3       0.74           18     0          0              0           0.00\n4       1.22           41     1     111115         435450           0.26\n5       0.24            0     0          0         379994           0.00\n6       0.94           11     0          0          34277           0.00\nVoy a plantear una regresión del divorcio frente a la edad media de matrimonio.\nComo primer paso, procedo de la misma forma que el autor, esclando las variables.\ndf_divorce$s_age &lt;- scale(df_divorce$MedianAgeMarriage)\ndf_divorce$s_div &lt;- scale(df_divorce$Divorce)"
  },
  {
    "objectID": "posts/2024-12-06-bootstrap-and-bayes-regression/index.html#regresión-lineal-bootstrap",
    "href": "posts/2024-12-06-bootstrap-and-bayes-regression/index.html#regresión-lineal-bootstrap",
    "title": "Regresión lineal bootstrap y bayesiana",
    "section": "Regresión lineal bootstrap",
    "text": "Regresión lineal bootstrap\n\nbootstrap_size &lt;- 1000\ncoef_df &lt;- purrr::map_dfr(seq_len(bootstrap_size), function(i) {\n    filas = sample(\n        seq_len(nrow(df_divorce)), \n        size = nrow(df_divorce), \n        replace = TRUE\n    )\n\n    df_boot &lt;- df_divorce[filas, ]\n    fit &lt;- lm(s_div ~ s_age, data = df_boot)\n    coefs = coef(fit)\n    tibble::tibble(\n        intercept = coefs[1],\n        age = coefs[2]\n    )\n})\n\ncoef_df$id = seq_len(nrow(coef_df))\nhead(coef_df)\n\n# A tibble: 6 × 3\n  intercept    age    id\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1    0.0712 -0.645     1\n2    0.0440 -0.773     2\n3   -0.123  -0.413     3\n4   -0.215  -0.564     4\n5   -0.0332 -0.665     5\n6   -0.0946 -0.670     6\n\n\n\nggplot() + \n    geom_point(\n        data = df_divorce, \n        aes(s_age, s_div), \n        size = 2\n    ) +\n    geom_abline(\n        data = coef_df, \n        aes(intercept = intercept, slope = age), \n        color = \"#800080\", alpha = 0.1\n    )"
  },
  {
    "objectID": "posts/2024-12-06-bootstrap-and-bayes-regression/index.html#regresión-lineal-bayesiana",
    "href": "posts/2024-12-06-bootstrap-and-bayes-regression/index.html#regresión-lineal-bayesiana",
    "title": "Regresión lineal bootstrap y bayesiana",
    "section": "Regresión lineal bayesiana",
    "text": "Regresión lineal bayesiana\n\nfit &lt;- quap(\n    alist(\n        s_div ~ dnorm(mu, sigma), \n        mu &lt;- a + b * s_age, \n        a ~ dnorm(0, 0.2),\n        b ~ dnorm(0, 0.5),\n        sigma ~ dexp(1)\n    ), \n    data = df_divorce\n)\n\nAhora muestro los datos en un gráfico de dispersión y superpongo las rectas de regresión a posteriori.\n\npost &lt;- extract.samples(fit)\npost_mean &lt;- apply(post, 2, mean)\npost_hpd &lt;- apply(post, 2, HPDI, prob = 0.89)\n\nggplot() + \n    geom_point(\n        data = df_divorce, \n        aes(s_age, s_div), \n        size = 2\n    ) +\n    geom_abline(\n        intercept = post_mean[1], \n        slope = post_mean[2], \n        color = \"#800080\"\n    ) +\n    geom_abline(\n        intercept = post_hpd[1, 1], \n        slope = post_hpd[2, 1], \n        linetype = \"dashed\", \n        color = \"#800080\"\n    ) +\n    geom_abline(\n        intercept = post_hpd[1, 2], \n        slope = post_hpd[2, 2], \n        linetype = \"dashed\", \n        color = \"#800080\"\n    )"
  },
  {
    "objectID": "posts/2024-10-23-random-numbers/index.html",
    "href": "posts/2024-10-23-random-numbers/index.html",
    "title": "Juego probabilístico con números aleatorios",
    "section": "",
    "text": "Para trabajar con números aleatorios en Python parece que la referencia es usar numpy.\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2024-10-23-random-numbers/index.html#numpy-básico",
    "href": "posts/2024-10-23-random-numbers/index.html#numpy-básico",
    "title": "Juego probabilístico con números aleatorios",
    "section": "numpy básico",
    "text": "numpy básico\nVamos a simular muchos lanzamientos de 4 monedas y vamos a ver en qué proporción ocurre que las cuatro monedas muestran cara. O sea, lanzo 4 monedas y apunto si todas han salido cara; las lanzo otra vez y vuelvo a apuntar; las lanzo otra vez y vuelvo a apuntar; y así 10.000 veces.\n\ntotal_runs = 10000\nn_all_heads = 0\n\nfor _ in range(total_runs):\n    heads = np.random.random(size=4)\n    # Si &lt;0.5, cara; si no, cruz\n    heads = heads &lt; 0.5\n    n_heads = heads.sum()\n    if n_heads == 4:\n        n_all_heads += 1\n\nn_all_heads / total_runs\n\n0.0629"
  },
  {
    "objectID": "posts/2024-10-23-random-numbers/index.html#algo-de-visualización",
    "href": "posts/2024-10-23-random-numbers/index.html#algo-de-visualización",
    "title": "Juego probabilístico con números aleatorios",
    "section": "Algo de visualización",
    "text": "Algo de visualización\nEn una moneda la probabilidad de obtener cara es 0.5, pero puedes tener una moneda trucada con una probabilidad diferente (¿puedes?).\nVamos a ver cómo se distribuye el obtener cara en distintos casos. El ejemplo ahora consiste en lanzar una moneda 100 veces, y en esas 100 veces veo cuántas caras he obtenido. Así veré cómo de esperable es obtener muy pocas caras o muchas caras, en función de cuán trucada esté la moneda.\n\ndef visualizacion_experimentos(probabilidad, lanzamientos, total_experimentos):\n    caras_obtenidas = np.empty(total_experimentos)\n\n    for i in range(total_experimentos):\n        caras_obtenidas[i] = np.sum(np.random.random(lanzamientos) &lt; probabilidad)\n    \n    plt.hist(caras_obtenidas)\n    plt.xlabel('Caras obtenidas')\n    plt.ylabel('Conteo')\n    plt.title(f'Lanzamientos por experimento: {lanzamientos}. Probabilidad: {probabilidad}')\n    plt.show()\n\nEn caras_obtenidas tenemos guardado el número de caras que hemos obtenido en cada experimento (cada uno consiste en 100 lanzamientos de una moneda cuya probabilidad de cara es 0.5).\n\nvisualizacion_experimentos(0.5, 100, 10000)\n\n\n\n\n\n\n\n\nLo que más peso tiene está en torno a 50 caras (frente a 100 totales). Es raro bajar de 40 o superar 60. Y parece imposible bajar de 30 o superar 70.\nVamos a cambiar la probabilidad:\n\nvisualizacion_experimentos(0.1, 100, 10000)"
  },
  {
    "objectID": "posts/2023-05-27-como-ser-un-estoico/index.html",
    "href": "posts/2023-05-27-como-ser-un-estoico/index.html",
    "title": "Cómo ser un estoico",
    "section": "",
    "text": "Morimos cada día (Séneca)\nUna filosofía de vida la necesitamos todos y todos la desarrollamos, de manera consciente o no.\nAtaraxia : tranquilidad de la mente\nLos hombres que realizaron estos descubrimientos antes que nosotros no son nuestros amos, sino nuestros guías. La verdad está abierta a todos, aún no ha sido monopolizada. Y quieres muchas para que la descubra la posteridad. (Séneca)\nNo te sientas avergonzado por aquellas cosas de las que no hay que avergonzarse (Crates, cínico)\nCicerón tradujo ethos (ética) por moralis (moralidad)"
  },
  {
    "objectID": "posts/2023-05-27-como-ser-un-estoico/index.html#la-disciplina-del-deseo",
    "href": "posts/2023-05-27-como-ser-un-estoico/index.html#la-disciplina-del-deseo",
    "title": "Cómo ser un estoico",
    "section": "La disciplina del deseo",
    "text": "La disciplina del deseo\n\nAlgunas cosas se pueden cambiar; otras, no\nSeñor, concédeme serenidad para aceptar todo aquello que no puedo cambiar, fortaleza para cambiar lo que soy capaz de cambiar, y sabiduría para entender la diferencia.\nComo se trata de ideas que en realidad han superado las pruebas del tiempo, seremos sabios si las adoptamos en nuestra vida.\nAcertar en el blanco se puede escoger, pero no se puede desear (Cicerón)\nSi sientes añoranza por un hijo o amigo, cuando no te es dado tenerlo, debes saber que añoras un higo en invierno\nPodemos y debemos aprender del lado pero solo podemos actuar sobre el hic et nunc. La actitud correcta es obtener consuelo del conocimiento de que hiciste todo lo mejor para afrontar el pasado\nVive de acuerdo con la naturaleza.\nAristóteles: los seres humanos son animales que pueden racionalizar y políticos (viven en polis, comunidad). Para los estoicos, la vida humana consiste en la aplicación de la razón a la vida social.\nel dilema del omnívoro.\nSócrates. Coherente con sus decisiones y sus puntos de vista, aceptó las consecuencias. Snowden huyó.\nEpicteto: son unos locos los que creen que el mundo es blanco y negro, el bien frente al mal, donde siempre es posible diferenciar con claridad a los buenos de los malos. Ese no es el mundo en el que vivimos y suponer lo contrario es bastante peligroso y demuestra muy poca sabiduría.\nAristóteles: la eudaimonía la pueden alcanzar solo unos pocos, mediante pocos méritos propios. Los cínicos rechazan los prerrequisitos de Aristóteles y dicen que lo que hace falta es tener menos. Cínico= perruno\npara los estoicos, la salud, la riqueza, la educación y la buena apariencia son indiferentes preferidos y lo opuesto son indiferentes dispreferidos.\nSeneca: evita el dolor y busca la alegría. Salvo que ponga en peligro tu integridad\nTodas las virtudes dependen de un rasgo subyacente: la sabiduría, porque siempre es una cualidad buena\nlos estoicos añadieron cuatro virtudes:\n\nSabiduría práctica: tomar decisiones que mejoran nuestra eudaimonía ( buena vida)\nvalor: capacidad para actuar bien bajo circunstancias adversas\ntemplanza: controlar nuestros deseos, para no dejarnos llevar por excesos\nJusticia: tratar a los demás con dignidad y ecuanimidad\n\nTomás de Aquino añadió tres, por encima de las cuatro: fe, esperanza y caridad.\nseis virtudes centrales entre cristianismo, confucionismo, hinduismo, judaísmo, taoísmo, Sócrates, Platón y Aristóteles: valor, justicia, humanidad, templanza, sabiduría, trascendencia\nes racional ser negacionista de la evolución si partes como católico de que lo que dice la Biblia es la verdad\nla disonancia cognitiva es una situación en la que eres consciente de que crees dos cosas que se contradicen. Eso nos desagrada, Epicteto decía que a nadie le gusta estar conscientemente equivocado\naumentar la disonancia en personas que están equivocadas es yn método para que busquen más información por sí mismas y solucionen si conflicto\nestoicos como Catón o Stockdale tuvieron una vida muy difícil. Aplicar sus métodos a la tuya debería ser mucho más fácil.\nsé consciente de tu capacidad de acción. Sé un agente, no un paciente\nmantén la atención en tus capacidades, no en tus discapacidades\nRaíces de la moralidad: escéptico, racionalista, empirista, intuicionista"
  },
  {
    "objectID": "posts/2023-05-27-como-ser-un-estoico/index.html#la-disciplina-del-consentimiento-cómo-reaccionar-ante-las-situaciones",
    "href": "posts/2023-05-27-como-ser-un-estoico/index.html#la-disciplina-del-consentimiento-cómo-reaccionar-ante-las-situaciones",
    "title": "Cómo ser un estoico",
    "section": "La disciplina del consentimiento: Cómo reaccionar ante las situaciones",
    "text": "La disciplina del consentimiento: Cómo reaccionar ante las situaciones\n\ntengo que morir, ¿de verdad? Si es así entonces estoy muriendo: si es pronto Aires cenaré porque es la hora de cenar, y después, cuando llegue el momento, moriré. (Epicteto)\nsi nos asusta la muerte es por ignorancia. Cualquiera que estudia a otros seres es consciente de la muerte de estos\nequirindón\nsegún Aristóteles, tres tipos de amistad (filia): de utilidad, de placer, de los buenos."
  },
  {
    "objectID": "posts/2023-05-27-como-ser-un-estoico/index.html#ejercicios-espirituales-prácticos",
    "href": "posts/2023-05-27-como-ser-un-estoico/index.html#ejercicios-espirituales-prácticos",
    "title": "Cómo ser un estoico",
    "section": "Ejercicios espirituales prácticos",
    "text": "Ejercicios espirituales prácticos\n\nExamina tus impresiones\nRecuerda las fugacidad d las cosas\nLa cláusula de reserva\nCómo puedes usar la virtud aquí y ahora?\nHaz una pausa y respira hondo\nAlteriza\nHabla poco y bien\nElige bien la compañía\nResponde a los insultos con humor\nNo hables demasiado de ti mismo\nHabla sin juzgar\nReflexiona sobre tu día"
  },
  {
    "objectID": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html",
    "href": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html",
    "title": "Quita caracteres raros en tus variables",
    "section": "",
    "text": "La programación está pensada con el alfabeto anglosajón. Cuando tienes datos en español, francés, turco, etc., aunque el alfabeto tiene casi todo en común con el anglosajón, tiene demasiados elementos molestos en algunos casos.\nSin ir más lejos, estas líneas están llenas de tildes (y alguna eñe caerá). En las filas de una columna pueden no molestarte (dependerá del análisis que vayas hacer), pero si tienes que pasar esos valores a nombres de columnas, puede ser muy mala idea que tengas alguna llamada año.\nVeamos formas de solucionarlo."
  },
  {
    "objectID": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html#a-mano",
    "href": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html#a-mano",
    "title": "Quita caracteres raros en tus variables",
    "section": "A mano",
    "text": "A mano\nForma horrible.\n\nnombres &lt;- c(\"Campaña veintitrés\", \"C'est très petite\", \"Alışveriş Arabası Önü Giydirme\")\nnombres &lt;- rep(nombres, 2)\nnombres\n\n[1] \"Campaña veintitrés\"             \"C'est très petite\"             \n[3] \"Alışveriş Arabası Önü Giydirme\" \"Campaña veintitrés\"            \n[5] \"C'est très petite\"              \"Alışveriş Arabası Önü Giydirme\"\n\n\nTienes una columna con esos valores. Tienen textos en español, francés y turco. Y repetidos. Quieres adaptarlo a alfabeto anglosajón. Por ejemplo, \"campaña\" pasará a ser \"campana\" (habrá niños en el cuerpo de un adulto que se pondrían nerviosos si estuviera la palabra \"año\" por allí).\nPuedes plantear algún replace, pero como intentes hacerlo carácter a carácter, vas a querer dejar tu trabajo.\n\nstringr::str_replace(nombres, \"ñ\", \"n\")\n\n[1] \"Campana veintitrés\"             \"C'est très petite\"             \n[3] \"Alışveriş Arabası Önü Giydirme\" \"Campana veintitrés\"            \n[5] \"C'est très petite\"              \"Alışveriş Arabası Önü Giydirme\"\n\n\nEso puede serte útil si tienes algún maestro que te dijera como cambiar carácter. Pero no tiene sentido que tengas que hacerlo letra a letra (aunque es muy posible que la mejor forma de construir ese maestro es hacerlo letra a letra la primera vez)."
  },
  {
    "objectID": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html#conversiones",
    "href": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html#conversiones",
    "title": "Quita caracteres raros en tus variables",
    "section": "Conversiones",
    "text": "Conversiones\nNo sé cómo llamar a esto, pero hay un par de funciones que pueden ayudarte… en cierto modo.\nMatizo: te pueden quitar los caracteres raros, pero no va a ser suficiente para que los textos te sirvan como nombres de columnas.\nTen en cuenta que los nombres de columnas deberían ser en minúsculas y con barras bajas (o quizá con mayúscula la primera letra de cada palabra, si barras), no solo sin tildes ni caracteres raros.\nPero vamos a ver cómo queda.\nEn R base tienes la función iconv().\n\niconv(nombres, to = \"ASCII//TRANSLIT\")\n\n[1] \"Campana veintitres\"             \"C'est tres petite\"             \n[3] \"Alisveris Arabasi Onu Giydirme\" \"Campana veintitres\"            \n[5] \"C'est tres petite\"              \"Alisveris Arabasi Onu Giydirme\"\n\n\nNo está mal, que me ha incluso simplificado las íes turcas. Pero esto sigue sin valerme a mí.\nSi te vale a ti, una alternativa es:\n\nstringi::stri_trans_general(nombres, \"latin-ascii\")\n\n[1] \"Campana veintitres\"             \"C'est tres petite\"             \n[3] \"Alisveris Arabasi Onu Giydirme\" \"Campana veintitres\"            \n[5] \"C'est tres petite\"              \"Alisveris Arabasi Onu Giydirme\"\n\n\nPero:\n\nmicrobenchmark::microbenchmark(\n    base = iconv(nombres, to = \"ASCII//TRANSLIT\"), \n    stringi = stringi::stri_trans_general(nombres, \"latin-ascii\")\n)\n\nUnit: microseconds\n    expr   min     lq    mean median     uq    max neval\n    base  48.8  92.40  93.886   97.7 107.70  138.3   100\n stringi 340.3 422.45 708.301  868.9 880.25 1161.3   100"
  },
  {
    "objectID": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html#mi-solución",
    "href": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html#mi-solución",
    "title": "Quita caracteres raros en tus variables",
    "section": "Mi solución",
    "text": "Mi solución\nInsisto que lo que quiero no es solo unificar caracteres, cosa que podría ser útil en algún proyecto de procesamiento de lenguaje natural, sino convertir esos textos a algo útil como nombres de columnas de una tabla (con sus minúsculas, sus barras bajas en lugar de espacios, etc).\nPara esto me gusta la librería janitor. La función con la que la conocí es clean_names().\nRecuerda que los nombres de columnas del data frame iris son Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species. Pero:\n\nlibrary(janitor)\niris |&gt; \n  clean_names() |&gt; \n  names()\n\n[1] \"sepal_length\" \"sepal_width\"  \"petal_length\" \"petal_width\"  \"species\"     \n\n\nEsta librería tiene la función make_clean_names(), que actúa sobre vectores.\nPero ten en cuenta una cosa: que su utilidad principal es para nombres de columnas, y en una tabla nunca debería haber dos columnas con el mismo nombre, así que evita duplicados:\n\nmake_clean_names(nombres)\n\n[1] \"campana_veintitres\"               \"cest_tres_petite\"                \n[3] \"alisveris_arabasi_onu_giydirme\"   \"campana_veintitres_2\"            \n[5] \"cest_tres_petite_2\"               \"alisveris_arabasi_onu_giydirme_2\"\n\n\nTú puedes no querer eso. Pero hay un argumento que te soluciona el problema:\n\nmake_clean_names(nombres, allow_dupes = TRUE)\n\n[1] \"campana_veintitres\"             \"cest_tres_petite\"              \n[3] \"alisveris_arabasi_onu_giydirme\" \"campana_veintitres\"            \n[5] \"cest_tres_petite\"               \"alisveris_arabasi_onu_giydirme\""
  },
  {
    "objectID": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html#python",
    "href": "posts/2024-10-18-quita-caracteres-raros-en-tus-variables/index.html#python",
    "title": "Quita caracteres raros en tus variables",
    "section": "Python",
    "text": "Python\nEn algún momento, haré esto en Python."
  },
  {
    "objectID": "posts/2023-06-24-bayes-y-monedas/index.html",
    "href": "posts/2023-06-24-bayes-y-monedas/index.html",
    "title": "Cara o cruz, modo Bayes (I)",
    "section": "",
    "text": "Tienes una moneda, aparentemente no trucada, y la lanzas 100 veces.\nEsperarías que 50 veces saliera cara, ¿no? Más o menos.\nPues sale 60 veces.\n¿Sigues pensando que no está trucada?"
  },
  {
    "objectID": "posts/2023-06-24-bayes-y-monedas/index.html#el-modo-frecuentista",
    "href": "posts/2023-06-24-bayes-y-monedas/index.html#el-modo-frecuentista",
    "title": "Cara o cruz, modo Bayes (I)",
    "section": "El modo frecuentista",
    "text": "El modo frecuentista\nPuedes plantear el problema así:\n\nRealizas el experimento 10.000 veces.\nO sea, lanzas la moneda 100 veces y cuentas, las caras; lo haces otra vez; y otra… y así 10.000 veces.\n\n¿Cuántas de esas veces has obtenido por lo menos 60 caras?\nLas que marco en el gráfico:\n\n\n\n\n\n\n\n\n\nEsa área es el p-valor de este test:\n\nbinom.test(heads, flips, alternative = \"greater\")\n\n\n    Exact binomial test\n\ndata:  heads and flips\nnumber of successes = 60, number of trials = 100, p-value = 0.02844\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.5129758 1.0000000\nsample estimates:\nprobability of success \n                   0.6"
  },
  {
    "objectID": "posts/2023-06-24-bayes-y-monedas/index.html#qué-hay-mal-aquí",
    "href": "posts/2023-06-24-bayes-y-monedas/index.html#qué-hay-mal-aquí",
    "title": "Cara o cruz, modo Bayes (I)",
    "section": "¿Qué hay mal aquí?",
    "text": "¿Qué hay mal aquí?\nLo malo de esto es que ese planteamiento te da la probabilidad de tus datos, en el supuesto de que la moneda no esté trucada (muy baja, por cierto).\nPero es que tú no quieres eso.\nTú ya lanzaste la moneda 100 veces, y salió cara 60. No quieres la probabilidad de que eso ocurra… ¡porque eso ya ha ocurrido!\nLo que quieres saber es si es probable o no que la moneda esté trucada.\nSi no lo está, la probabilidad de obtener cara es 0,5.\nAhora bien, dado que has obtenido cara 60 veces de 100, ¿cuál es probabilidad real de obtener cara en esa moneda?"
  },
  {
    "objectID": "posts/2023-06-24-bayes-y-monedas/index.html#el-modo-bayesiano",
    "href": "posts/2023-06-24-bayes-y-monedas/index.html#el-modo-bayesiano",
    "title": "Cara o cruz, modo Bayes (I)",
    "section": "El modo bayesiano",
    "text": "El modo bayesiano\n\nPriori\nSupón que no tienes ni idea de si la moneda está trucada o no. O sea, la probabilida de obtener cara va entre 0 y 1.\nSí, normalmente es 0,5, pero como ahora no sabes nada de la moneda, vas a suponer que puedes obtener cara siempre, o nunca, o pocas veces, o muchas… pues eso, cualquier número entre 0 y 1.\nEl siguiente gráfico es una aproximación a cómo expresamos eso visualmente. La idea es que la probabilidad de obtener cara sigue una distribución uniforme entre 0 y 1.\nEso es tu información a priori: no tienes ni idea.\n\n\n\n\n\n\n\n\n\n\n\nModelo generativo\nAhora vas a simular varias situaciones de lanzamientos de la moneda 100 veces. Pero en cada experimento, la probabilidad de obtener cara varía, según la distribución uniforme que has asumido antes.\nEsta es la distribución del número de caras que obtienes en este caso:\n\n\n\n\n\n\n\n\n\nVisto así, parece poca cosa, pero ya tienes la información suficiente para tener mejor idea de la probabilidad de obtener cara.\n\n\nPosteriori\nPrimero, construyes una tabla en la que tienes el número de caras obtenido en función de la probabilidad de obtener cara.\n\n\n    p_priori number_of_heads\n1 0.13062305               9\n2 0.60850103              64\n3 0.39369935              38\n4 0.60221118              54\n5 0.92457859              93\n6 0.03955949               3\n\n\nEn esa tabla podemos quedarnos solo con los casos en los que obtienes 60 caras. Y en esos casos observas la probabilidad de obtener cara.\nEsa probabilidad sigue esta distribución:\n\n\n\n\n\n\n\n\n\nSi prefieres números:\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4877  0.5715  0.6081  0.6055  0.6368  0.7400 \n\n\nEsos números ya no siguen la distribución uniforme a priori de antes, en la que indicabas que no sabías nada de la moneda.\nAhora ya sabes entre qué números puede variar esa probabilidad.\n\n\nDatos\nEsos valores a posteriori los hemos calculado porque en tu experimento inicial conseguiste 60 caras.\nPero si tus datos fueran otros, tu modelo generativo te permite obtener el posteriori de nuevo.\nSolo tienes que moverte entre los resultados.\nAquí tienes, visualmente, en qué consiste la creación de los posteriori, dados los datos:"
  },
  {
    "objectID": "posts/2023-08-14-muertes-humanas/index.html",
    "href": "posts/2023-08-14-muertes-humanas/index.html",
    "title": "Muertes de humanos causadas por otros animales",
    "section": "",
    "text": "Siempre me ha fascinado el dato que te voy a enseñar ahora.\n¿Cuál es el animal que más humanos mata al año?\nHay gente que se come el coco sin llegar a nada. Otros dicen que el humano.\nPero no.\nEl gráfico siguiente además ayudar a desmitificar peligros, como los tiburones.\nObservación. No consigo encontrar la fuente original de los datos. No es el único sitio donde lo he visto y me gusta la idea general, pero los números quizá no sean fiables (y la traducciones menos todavía, porque esas las he hecho yo, así que eso sí que no xD)\n\n\n\n\n\n\n\n\n\nHay 5 ideas que me encantan de esto:\n\nLos insectos matan a mazo, mazo de humanos.\nLos tiburones, mal vistos por películas, no suponen mucho problema.\nLos humanos matamos mucho, pero no somos lo peor (casi sí).\nLos perros, ellos tan amigos, pero cuidado con ellos.\n¿Algo parecido a regla de Pareto?\n\nLa fuente es esta."
  }
]